{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ea92989-1dfb-4eaa-a445-359ee69428b8",
   "metadata": {},
   "source": [
    "# A realistic approach to finetuning in an enterprise environment\n",
    "You’d have to be living under a rock not to notice the massive advancements in the LLM (Large Language Model) space since the release of ChatGPT in 2022. These breakthroughs have driven many companies and organizations to invest heavily in pushing the space forward, resulting in a wealth of open-source models that can be leveraged to build data products.\n",
    "\n",
    "This is an excellent starting point, but many companies possess a significant amount of internal documents and other proprietary data that could greatly enhance their data products. Ideally, they would want to incorporate this data into their tools. To address this need, many providers have introduced fine-tuning services. For example, OpenAI offers fine-tuning for their GPT-4 model. This is a fantastic service that undoubtedly provides value to many clients. However, for data engineers like me who live and work in the EU, this is often not a viable option.\n",
    "\n",
    "The EU is governed by the GDPR (General Data Protection Regulation), which imposes strict rules on how data can be used. Even before GDPR, many large enterprises were reluctant to share their data with external partners, making it challenging to use external fine-tuning services without undergoing resource-intensive compliance reviews. Consequently, many organizations opt to focus on other areas instead.\n",
    "\n",
    "The goal of this blog post is to demonstrate how you can fine-tune some of these models in a restricted enterprise environment. To illustrate this, I will run everything on my personal laptop, a 2021 MacBook Pro with an M1 Max chip and 32 GB of RAM. While this is a fairly powerful machine, I believe it is realistic to expect access to equivalent resources within an enterprise setting. I’ll show you how to achieve this step-by-step.\n",
    "\n",
    "## Me\n",
    "First a bit about who I am. My name is Dilovan Celik, you are probably reading this on my GitHub. If you want to read more information from me, you can visit my substack [Thoughts on Data](https://dilovan.substack.com) or connect with me on [LinkedIn](https://www.linkedin.com/in/dilovancelik/). Now on to the main event. \n",
    "\n",
    "## The Project\n",
    "To make this post more relatable, I’ll guide you through the process of building a Retrieval-Augmented Generation (RAG) system from scratch, where we will fine-tune the embedding algorithm. While building a RAG is an interesting project in itself, my primary hope is not only to show you how to build a RAG but also to empower you to experiment with creating your own products, even when you don’t have access to unlimited resources.\n",
    "\n",
    "Before diving in, let’s take a look at what exactly we’ll be building.\n",
    "\n",
    "### Retrieval Agumented Generation (RAG)\n",
    "A RAG is a method to enhance an existing chatbot with your own data. Essentially, it involves building a layer on top of the chatbot that takes user input and enriches it with relevant context from your internal documents. (This, by the way, is a different approach to integrating your own data into a chatbot.)\n",
    "\n",
    "The RAG enriches the original user query by embedding it and searching through a database of your internal documents, finding documents that are similar to the user's query.\n",
    "\n",
    "A simplified illustration of the RAG architecture might look like this:\n",
    "\n",
    "![Illustration of RAG architecture](./images/rag_illustration.png)\n",
    "\n",
    "### Mette (Frederiksen, Danish PM) Bot\n",
    "Our RAG will be an application where you can input a topic and retrieve the Danish Prime Minister Mette Frederiksen's opinion on the subject. The RAG will be enriched with speeches given by Mette Frederiksen.\n",
    "\n",
    "The challenge is that these speeches are mostly in Danish. Since Danish is a relatively small language (approximately 6 million native speakers), it can be difficult to find a highly effective open-source embedding model. Even when choosing a multilingual model, danish typically represents only a small fraction of the training data.\n",
    "\n",
    "Therefore, our approach will be to download a small multilingual embedding model and improve it for our specific data through fine-tuning.\n",
    "\n",
    "\n",
    "## Fintunning Pipeline\n",
    "Now that we have some background on what I'm trying to achieve lets look at the steps we need to take. \n",
    "\n",
    "1. Data Retrieval (We will not go through this, because every case is different)\n",
    "2. Data Preparation\n",
    "3. Establish Baseline for non Finetuned Model\n",
    "4. Finetuning\n",
    "5. Test to see if the model performance has improved\n",
    "6. Sample code for simple RAG\n",
    "\n",
    "As shown I will not go through the data retrieval part. I will skip this part, because the recommended approach varies depending on your data source. You can find my data in the folder data/taler\n",
    "\n",
    "To avoid having to import libraries all over the place lets import the needed dependencies here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125960c4-4081-4bb7-bcfe-006ea70e7b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "import ollama\n",
    "import uuid\n",
    "import json\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import psycopg as pg\n",
    "from ollama import chat, ChatResponse\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from peft import LoraConfig, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e19dd5-092a-4dc0-b1ec-a203415fdf0d",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "One of the most critical steps when fine-tuning a model is preparing your data properly. Before diving in, it's important to consider the specific goal of your application.\n",
    "\n",
    "For example, imagine you are building an application where users can input a list of symptoms and receive the 10 most likely diagnoses. In this scenario, you need a model that can match the embedding of the symptom list with the embeddings of the diagnoses. Conversely, if your goal were to retrieve applicable laws based on a user's question, you would need to train the model to match questions to relevant legal texts.\n",
    "\n",
    "In this example, our objective is to create a model that can take a subject (e.g., Crime, Unemployment, Climate Change) and match it with public speeches made by the Danish Prime Minister, Mette Frederiksen.\n",
    "\n",
    "I have a folder containing 153 speeches by the Prime Minister. However, because these speeches cover a wide range of subjects, I need to break them into smaller, more manageable chunks.\n",
    "\n",
    "A simple approach would be to split each speech into chunks of a fixed number of characters (e.g., 1,000 characters) with some overlap between chunks. This naive method can work quite well in many scenarios.\n",
    "\n",
    "However, I encourage you to analyze your data for natural breaks that could serve as chunking boundaries. For instance, if you were working with legal texts, it might make sense to assign each article to its own chunk.\n",
    "\n",
    "When reviewing my data, I noticed that longer speeches often included lines like **** or ---- as separators between different sections. I can use these as delimiters when chunking the text. For speeches that don't contain these separators, I will fall back on the naive approach, but instead of chunking by characters, I'll chunk by lines to maintain more natural context within each chunk.\n",
    "\n",
    "This approach to data preparation will help ensure that our fine-tuned model produces more relevant and accurate results.\n",
    "The code I wrote to handle chunking looks like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3536928a-ac92-46f2-8e95-d33ad9b37bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 1999.64it/s]\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "files = glob.glob(\"*.txt\", root_dir=\"data/taler\") # find all files in the folder data/taler\n",
    "speech_chunks = {}\n",
    "with tqdm(total=len(files), desc=\"Processing Files\") as fpbar:\n",
    "    for file_name in files:\n",
    "        speech_name = file_name.replace(\".txt\", \"\")\n",
    "        with open(f\"data/taler/{file_name}\", \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            total += len(lines)\n",
    "        context_splits = []\n",
    "\n",
    "        char_split = False\n",
    "        for line in lines:\n",
    "            if re.search(\"[a-zA-Z]\", line) is None and \"\\n\" != line:\n",
    "                char_split = True\n",
    "                break\n",
    "\n",
    "        if char_split:\n",
    "            context = []\n",
    "            for line in lines:\n",
    "                if re.search(\"[a-zA-Z]\", line) is None and \"\\n\" != line:\n",
    "                    context_splits.append(\" \".join(context))\n",
    "                    context = []\n",
    "                else:\n",
    "                    context.append(line)\n",
    "            if context != []:\n",
    "                context_splits.append(\" \".join(context))\n",
    "        else:\n",
    "            chunk_size = 10  # group size\n",
    "            overlap = 2  # overlap size\n",
    "            context_splits = [\n",
    "                \" \".join(lines[i : i + chunk_size])\n",
    "                for i in range(0, len(lines), chunk_size - overlap)\n",
    "            ]\n",
    "\n",
    "        speech_chunks[speech_name] = context_splits\n",
    "        fpbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305f155-488f-4e45-9401-1ee6d0052758",
   "metadata": {},
   "source": [
    "We now have a Python object that looks something like this:\n",
    "\n",
    "```Python\n",
    "{\n",
    "    \"speech-1\": [\"chunk 1\", \"chunk 2\", \"chunk 3\"],\n",
    "    \"speech-2\": [\"chunk 1\", \"chunk 2\", \"chunk 3\"],\n",
    "    ...\n",
    "    \"speech-n\": [\"chunk 1\", \"chunk 2\", \"chunk 3\"],\n",
    "}\n",
    "```\n",
    "\n",
    "It might be tempting to think we can simply feed this data into a \"magic AI model box\" and instantly get a neat embedding algorithm. Unfortunately, it’s not that straightforward.\n",
    "\n",
    "As mentioned earlier, our goal is to create a model where a user can input a subject and retrieve a relevant chunk. To achieve this, we need not only the chunks themselves but also associated user inputs—in this case, the corresponding subjects. Since we don’t have predefined user inputs, we need to generate them.\n",
    "\n",
    "One approach would be to manually annotate each chunk with subjects, which would undoubtedly yield the best results. However, this method would be far too time-consuming. Instead, we’ll generate synthetic data using an LLM (Large Language Model). For this task, I’ll use Microsoft’s phi4 model, running locally on my machine through [ollama](https://ollama.com) (If for some reason you are not allowed to download ollama, you can use the transformer library to create the same functionality in python, read more here: [Transformer Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)). I've chosen this model because it offers a good balance between computational efficiency and output quality. However, I encourage you to experiment with different models to find what works best for your scenario.\n",
    "\n",
    "The method for generating this synthetic data involves feeding each chunk of text into the model and asking it to generate relevant subjects.\n",
    "\n",
    "Below is the prompt I use. The ## part is not part of the actual prompt but rather an English translation for non-Danish speakers:\n",
    "\n",
    "```\n",
    "Kontekst er nedenfor. ## Context below\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Givet den givne kontekst og ingen anden viden. ## Given the context and no other knowledge\n",
    "Genere op til 5 emner som kan beskrive konteksten,. ## Generate up to 5 subjects which can describe the context\n",
    "Hvis der ikke er emner som let kan beskrive konteksten, besvar med <|NAN|> ## If there are no subjects which easily describe the context reply with <|NAN|>\n",
    "\n",
    "Du må kun svarer med emnerne formattet skal være: Emne 1|Emne 2|...|Emne n| ## You are only allowed to reply with subjects in the following format: Subject 1| Subject 2|...|Subject n|\n",
    "```\n",
    "\n",
    "Let's try this approach with a toy example to see how it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "084b4464-f82a-4095-addb-4be2fa3494a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\\\n",
    "Kontekst er nedenfor.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Givet den givne kontekst og ingen anden viden.\n",
    "Genere op til 5 emner som kan beskrive konteksten,. \n",
    "Hvis der ikke er emner som let kan beskrive konteksten, besvar med <|NAN|>\n",
    "\n",
    "Du må kun svarer med emnerne formattet skal være: Emne 1|Emne 2|...|Emne n|\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6143fb02-4ec2-449f-b4bf-db6ae4544382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Chunk:\n",
      "\n",
      " Statsministeren sagde i sin åbningstale, at der er grund til at være stolt af Danmark. At Danmark er et godt samfund. Det er jeg fuldstændig enig i. Men tillad mig at være direkte her ved dagens start:\n",
      " Det Danmark, vi er stolte af, er ikke skabt på meget lave ydelser til børnefamilier. På en kriminel lavalder på 12 år. På topskattelettelser. Det Danmark, vi er stolte af, er skabt på det modsatte. På lige muligheder. På sociale fremskridt. På et progressivt skattesystem.\n",
      " Vi skal fortsætte. Vi skal bygge videre på verdens bedste samfund. Ikke det modsatte.\n",
      " Vi står over for et folketingsår, der bliver afgørende for Danmark. I mine øjne er der tre opgaver, der er vigtigere end alle andre.\n",
      " Vi skal have en finanslov på plads, der sikrer to ting. Styrket kernevelfærd. Flere arbejdspladser. Vi skal have et mere retfærdigt dagpengesystem. Vi skal hjælpe mennesker, der er på flugt og samtidig sikre, at den danske integration kan følge med.\n",
      " Regeringen har et valg. I kan vælge en vej, hvor Dansk Folkepartis manglende ansvarlighed skal forenes med Liberal Alliances ideologiske kamp mod velfærdssamfundet.\n",
      " Eller I kan vælge en vej, hvor udviklingen af vores velfærd styrkes. Hvor vi med økonomisk ansvarlighed fastholder et bredt samarbejde i Folketinget. Det valg skal statsministeren træffe.\n",
      " Jeg er stolt af vores velfærdssamfund. Jeg er dybt imponeret over de mange jordemødre, skolelærere, sosu’er og pædagoger, der med stor faglighed og menneskeligt overskud skaber små mirakler i vores hverdag.\n",
      " Som socialdemokrater vil vi altid forsvare vores fællesskab, når angrebene sættes ind. Fra minusvækst i den offentlige sektor til ideologiske opgør mod vores sundhedsvæsen. Samtidig vil vi også være kritiske, hvor der er brug for det.\n",
      "\n",
      "Response from phi4\n",
      "--------------------------------------------------------------\n",
      "Velfærdspolitik i Danmark|Sociale rettigheder og velfærdsydelser|Skattereform og skattepolitik|Arbejdsmarkedspolitik|Integrationspolitik og flygtningeproblematik\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "context = speech_chunks[\n",
    "    \"mette-frederiksens-aabningstale-ved-folketingets-aabningsdebat\"\n",
    "][1]\n",
    "print(f\"Speech Chunk:\\n{context}\")\n",
    "\n",
    "prompt = PROMPT_TEMPLATE.format(context_str=context)\n",
    "res = ollama.chat(\"phi4\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(f\"Response from phi4\\n--------------------------------------------------------------\\n{res.message.content}\\n--------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e321fd0-760e-4465-a963-6807bd90bb38",
   "metadata": {},
   "source": [
    "As you can see we get subjects back in the way we want, and can now start generating data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88aa85b8-ab7b-4789-95ee-299fbc1d29e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Queries: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1333/1333 [2:13:20<00:00,  6.00s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = {\n",
    "    \"speech\": {},\n",
    "    \"queries\": {},\n",
    "    \"corpus\": {},\n",
    "    \"relevant_docs\": {},\n",
    "    \"related_speech\": {},\n",
    "}\n",
    "\n",
    "## This part is just to be able to track how far we are\n",
    "total = 0\n",
    "for speech, chunks in speech_chunks.items():\n",
    "    total += len(chunks)\n",
    "\n",
    "# TODO Batching \n",
    "with tqdm(total=total, desc=\"Generating Queries\") as pbar:\n",
    "    for speech, chunks in speech_chunks.items():\n",
    "        speech_id = str(uuid.uuid4())\n",
    "        dataset[\"speech\"][speech_id] = speech\n",
    "        for chunk in chunks:\n",
    "            content_id = str(uuid.uuid4())\n",
    "            dataset[\"corpus\"][content_id] = chunk\n",
    "            dataset[\"related_speech\"][content_id] = speech_id\n",
    "            prompt = PROMPT_TEMPLATE.format(context_str=chunk)\n",
    "            res = ollama.chat(\"phi4\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "            reply = res.message.content\n",
    "            if \"<|NAN|>\" in reply:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            for query in reply.split(\"|\"):\n",
    "                query_id = str(uuid.uuid4())\n",
    "                dataset[\"queries\"][query_id] = query\n",
    "                dataset[\"relevant_docs\"][query_id] = [content_id]\n",
    "            pbar.update(1)\n",
    "\n",
    "with open(\"data/base_data.json\", \"w\") as f:\n",
    "    f.write(json.dumps(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12cb6c2-40cf-4f15-88a0-a6ffd3ca1e82",
   "metadata": {},
   "source": [
    "This process will take a while. On my MacBook Pro, it takes approximately 2.5 hours. However, if you have access to a machine with a more powerful GPU, such as an NVIDIA A100, you can significantly speed this up. Alternatively, you can use the pre-generated data, which is available in data/base_data.json.\n",
    "\n",
    "The current code uses a \"relevant document\" setup. In this approach, each query (subject) and chunk is assigned a unique UUID, with a separate structure that maps each query to its relevant subjects.\n",
    "\n",
    "Before moving forward, we need to address a couple of things:\n",
    "\n",
    "1. **Remove Duplicate Subjects:** It’s possible that the same subject appears multiple times, so we need to clean this up.\n",
    "2. **Organize Data for Model Training:** The data needs to be structured appropriately for training our model.\n",
    "\n",
    "Additionally, I’ve added a related speech object to help us look up which speech a particular chunk originates from. This will prove useful later on. However, the data is currently not in the best shape, so let's do some cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a81dd35-3604-42dc-8ab5-9727dad183c3",
   "metadata": {},
   "source": [
    "Once we have cleaned the data, the next step is to determine how we want to train our model. A crucial part of this process involves selecting an appropriate loss function. The loss function is responsible for penalizing the model when it makes incorrect predictions and rewarding it when it makes correct predictions. It essentially guides the model toward better performance during training.\n",
    "\n",
    "For the type of fine-tuning we are aiming for, it is common to organize the data into triplets. The triplet data format consists of:\n",
    "\n",
    "- **Anchor:** The subject or query.\n",
    "- **Positive:** A piece of context related to the anchor.\n",
    "- **Negative:** A piece of context unrelated to the anchor.\n",
    "  \n",
    "An example of this format might look like:\n",
    "\n",
    "- **Anchor:** Defense Spending\n",
    "- **Positive:** \"The new aircraft carrier is over budget but will bring much-needed capabilities.\"\n",
    "- **Negative:** \"It is important to invest education to give all children an equal opportunity.\"\n",
    "  \n",
    "However, a keen observer might notice that our dataset currently contains only positives. Therefore, we need to generate negatives to complete our triplets.\n",
    "\n",
    "There are several ways to achieve this:\n",
    "\n",
    "- **Generate Negatives with an LLM:** Similar to how we generated subjects earlier, we could use a large language model to create negative samples. This is a great approach, but can be very ressource intensive. \n",
    "- **Use an Unrelated Text Corpus:** Another approach is to introduce entirely unrelated text as negatives. For example, if you are working with clinical records, you could download annual reports from Goldman Sachs and use that text as your negative data. Unlike the above approach, this is very ressource efficient, but the results weaker when it comes to finetuning. Because the negative and positive are very far from eachother. \n",
    "- **Use the base model, to look for dissimilar samples in your corpus:** If the base model, has decent performance, this can be a very good compromise between the first and second approach. \n",
    "  \n",
    "For this project, I opted for the second approach. I downloaded a dataset containing customer reviews and randomly selected reviews as negative samples.\n",
    "\n",
    "Here's the code snippet demonstrating this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d188d69-f89c-43e1-b2ea-9b836d8b2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset\n",
    "\n",
    "# Strips away redundant whitespace from the subjects and \n",
    "# removes subjects which are note actual subjects\n",
    "queries = defaultdict(list)\n",
    "for key, value in data[\"queries\"].items():\n",
    "    if value == \"\" or \"--\" in value:\n",
    "        continue\n",
    "    queries[data[\"queries\"][key].strip()].append(key)\n",
    "\n",
    "# We go through the list chunks, and remove parts that does\n",
    "# That does not include any information. This part is very \n",
    "# dependent on your data. So look through and implement the\n",
    "# rules that makes sense for you\n",
    "\n",
    "corpus = defaultdict(list)\n",
    "pop_keys = []\n",
    "for key, value in data[\"corpus\"].items():\n",
    "    value = \" \".join(value.replace(\"Tale\\n \\n \\n \\n \\n \\n \\n \", \"\").split())\n",
    "    if value.isspace() or len(value) <= 60:\n",
    "        pop_keys.append(key)\n",
    "    data[\"corpus\"][key] = value\n",
    "    corpus[value].append(key)\n",
    "\n",
    "for key in pop_keys:\n",
    "    data[\"corpus\"].pop(key)\n",
    "    data[\"related_speech\"].pop(key)\n",
    "\n",
    "\n",
    "# We create a new query structure, which removes duplicate queries\n",
    "# and remap the relevant documents to the new query ids\n",
    "relevant_docs = defaultdict(list)\n",
    "new_queries = {}\n",
    "for query in queries.keys():\n",
    "    id = str(uuid.uuid4())\n",
    "    new_queries[id] = query\n",
    "    query_ids = queries[query]\n",
    "    for query_id in query_ids:\n",
    "        for doc_id in data[\"relevant_docs\"][query_id]:\n",
    "            if doc_id not in pop_keys:\n",
    "                relevant_docs[id].append(doc_id)\n",
    "\n",
    "clean_data = {}\n",
    "clean_data[\"query\"] = new_queries\n",
    "clean_data[\"relevant_docs\"] = relevant_docs\n",
    "clean_data[\"related_speech\"] = data[\"related_speech\"]\n",
    "clean_data[\"corpus\"] = data[\"corpus\"]\n",
    "\n",
    "with open(\"data/cleaned_base_data.json\", \"w\") as out:\n",
    "    json.dump(clean_data, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7551ea49-e26e-489c-858d-e66f2a7ac133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Query Embedding Span:   0%|                                                                                                                                         | 300638/22595801761 [14:37<18317:53:18, 342.64it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"data/cleaned_base_data.json\", \"r\") as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "negatives: pd.DataFrame = pd.read_csv(\"data/negatives.csv\")[\"review_text\"]\n",
    "\n",
    "triplets = []\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Test the model\n",
    "\n",
    "sims = []\n",
    "sim_lkp = {}\n",
    "\"\"\"\n",
    "encoded_queries = {}\n",
    "with tqdm(total=len(data[\"query\"]), desc=\"embedding_queries\") as pbar:\n",
    "    for key, value in data[\"query\"].items():\n",
    "        emb = model.encode(value)\n",
    "        encoded_queries[key] = emb\n",
    "        pbar.update(1)\n",
    "\"\"\"\n",
    "with tqdm(total=len(encoded_queries.keys()) ** 2, desc=\"Testing Query Embedding Span\") as pbar:\n",
    "    for x_id, x_emb in encoded_queries.items():\n",
    "        sim_lkp[x_id] = {}\n",
    "        for y_id, y_emb in encoded_queries.items():\n",
    "            if x_id == y_id:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "                \n",
    "        dot_product = np.dot(x_emb, y_emb)\n",
    "        magnitude_x = np.linalg.norm(x_emb)\n",
    "        magnitude_y = np.linalg.norm(y_emb)\n",
    "        cosine_similarity = dot_product / (magnitude_x * magnitude_y)\n",
    "        sims.append(cosine_similarity)\n",
    "        sim_lkp[x_id][y_id] = cosine_similarity\n",
    "        pbar.update(1)\n",
    "\n",
    "x = np.array(sims)\n",
    "plt.hist(x, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33a5df02-be63-41c3-9cf0-ec1bacf0d22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Query Embedding Span:   0%|                                                                                                                                         | 300638/22595801761 [14:37<18317:53:18, 342.64it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIFFJREFUeJzt3QuwlGX9B/CHu3gBErmZYJimeMMBLcnLX5NAw9LUklGREnM0dBJMlLybE6RTauGlxgqb0VAmtQQFCUMnxUuoJRhMKgqmXMy4qNx5//M8057OQTxyDgfOeXY/n5l1z+4++553H9+z++X3Ps+zzYqiKAIAQEaaN/YOAADUlQADAGRHgAEAsiPAAADZEWAAgOwIMABAdgQYACA7AgwAkJ2WoUxt3LgxvP3222GXXXYJzZo1a+zdAQC2QFxfd+XKlWH33XcPzZs3r7wAE8NL9+7dG3s3AIB6WLhwYdhjjz0qL8DEykupA9q1a9fYuwMAbIEVK1akAkTpc7ziAkzptFEMLwIMAOTlk4Z/GMQLAGRHgAEAsiPAAADZEWAAgOwIMABAdgQYACA7AgwAkB0BBgDIjgADAGRHgAEAsiPAAADZEWAAgOwIMABAdgQYACA7LRt7BwA+yWcun1z18xtjBzXqvgBNgwoMAJAdAQYAyI5TSEAWp442vc+pJKhsKjAAQHYEGAAgOwIMAJAdY2CALMa+fFwbY2GgMqnAAADZEWAAgOwIMABAdoyBAbIa+wIQqcAA2YcewQcqjwADAGRHgAEAsiPAAADZEWAAgOyYhQQ0KgNwgfoQYIBGIbgAW8MpJAAgOwIMAJAdAQYAyI4AAwBkR4ABALIjwAAA2RFgAIDsCDAAQHYEGAAgOwIMAJAdAQYAyI7vQgK2K9+BBDQEFRgAIDsCDACQHQEGAMiOAAMAVFaAGTt2bGjWrFm4+OKLq+5bvXp1GD58eOjYsWPYeeedw6mnnhoWL15c43kLFiwIgwYNCjvuuGPo3LlzuPTSS8P69etrtJkxY0bo06dPaNOmTdh7773D+PHjt2ZXAYAyUu8A8/zzz4df/OIX4eCDD65x/4gRI8LDDz8cJk6cGJ544onw9ttvh1NOOaXq8Q0bNqTwsnbt2vD000+Hu+++O4WTq6++uqrN/PnzU5tjjz02vPTSSykgnXvuuWHq1Kn13V0AoIw0K4qiqOuT3n///VQduf3228MNN9wQDjnkkHDLLbeE5cuXh06dOoV77703nHbaaant3LlzQ69evcLMmTPD4YcfHh599NFw4oknpmDTpUuX1ObOO+8Ml112WVi6dGlo3bp1+nny5Mlh9uzZVb9z8ODBYdmyZWHKlClbtI8rVqwI7du3T/vUrl27ur5EIONp1G+MHbTNfwewbWzp53e9KjDxFFGskPTv37/G/bNmzQrr1q2rcf9+++0XevTokQJMFK8POuigqvASDRw4MO3wnDlzqtpsuu3YprSNzVmzZk3aRvULAFCe6ryQ3YQJE8ILL7yQTiFtatGiRamC0qFDhxr3x7ASHyu1qR5eSo+XHqutTQwlq1atCm3btv3I7x4zZky47rrr6vpyAIAM1akCs3DhwvC9730v3HPPPWGHHXYITcno0aNTual0ifsKAJSnOgWYeIpoyZIlafxLy5Yt0yUO1P3Zz36Wfo5Vkjg4N45VqS7OQuratWv6OV5vOiupdPuT2sRzYZurvkRxtlJ8vPoFAChPdQowxx13XHj55ZfTzKDS5dBDDw1nnnlm1c+tWrUK06dPr3rOvHnz0rTpfv36pdvxOm4jBqGSadOmpcCx//77V7Wpvo1Sm9I2AIDKVqcxMLvssks48MADa9y30047pTVfSvcPGzYsjBw5Muy6664plFx00UUpeMQZSNGAAQNSUBkyZEi48cYb03iXK6+8Mg0MjlWU6Pzzzw/jxo0Lo0aNCuecc054/PHHw/33359mJgF52p5f4lj6XWYjQflq8G+jvvnmm0Pz5s3TAnZxZlCcPRSnW5e0aNEiTJo0KVxwwQUp2MQANHTo0HD99ddXtenZs2cKK3FNmVtvvTXsscce4a677krbAgCo1zowObAODFRuBaZEBQbys03XgQEAaEwCDACQHQEGAMiOAAMAZEeAAQCyI8AAANkRYACA7AgwAEB2GnwlXoDGXLwOqAwqMEBZByghCsqTAAMAZEeAAQCyI8AAANkRYACA7AgwAEB2BBgAIDsCDACQHQEGAMiOAAMAZEeAAQCyI8AAANnxZY5Ag/P9Q8C2pgIDAGRHgAEAsiPAAADZEWCAihiTY1wOlBcBBgDIjgADAGRHgAEAsiPAAADZEWAAgOwIMABAdgQYACA7AgwAkB0BBgDIjgADAGRHgAEAsiPAAADZadnYOwCUD1+YCGwvAgxQkQHrjbGDGnVfgK3jFBIAkB0BBgDIjgADAGRHgAEAsiPAAADZEWAAgOwIMABAdgQYACA7AgwAkB0BBgDIjgADAGTHdyEBW82XOALbmwoMAJAdAQao2KqRyhHkS4ABALIjwAAA2RFgAIDsCDAAQHYEGAAgOwIMAJAdAQYAyI4AAwBkR4ABALIjwAAA2RFgAIDsCDAAQHYEGACgvAPMHXfcEQ4++ODQrl27dOnXr1949NFHqx5fvXp1GD58eOjYsWPYeeedw6mnnhoWL15cYxsLFiwIgwYNCjvuuGPo3LlzuPTSS8P69etrtJkxY0bo06dPaNOmTdh7773D+PHjt/Z1AgBlpGVdGu+xxx5h7NixYZ999glFUYS77747nHTSSeHFF18MBxxwQBgxYkSYPHlymDhxYmjfvn248MILwymnnBKeeuqp9PwNGzak8NK1a9fw9NNPh3feeSecffbZoVWrVuFHP/pRajN//vzU5vzzzw/33HNPmD59ejj33HNDt27dwsCBA7dNLwB19pnLJzf2LgAVrFkRk8hW2HXXXcNNN90UTjvttNCpU6dw7733pp+juXPnhl69eoWZM2eGww8/PFVrTjzxxPD222+HLl26pDZ33nlnuOyyy8LSpUtD69at088xBM2ePbvqdwwePDgsW7YsTJkyZYv3a8WKFSlELV++PFWLgIZVLgHmjbGDGnsXgHp8ftd7DEyspkyYMCF88MEH6VTSrFmzwrp160L//v2r2uy3336hR48eKcBE8fqggw6qCi9RrKrEnZ0zZ05Vm+rbKLUpbePjrFmzJm2n+gUAKE91DjAvv/xyGt8Sx6fE0zwPPvhg2H///cOiRYtSBaVDhw412sewEh+L4nX18FJ6vPRYbW1iIFm1atXH7teYMWNSYitdunfvXteXBgCUa4DZd999w0svvRSeffbZcMEFF4ShQ4eGV155JTS20aNHp3JT6bJw4cLG3iUAoCkM4o1ilSXODIr69u0bnn/++XDrrbeG008/PaxduzaNValehYmzkOKg3SheP/fcczW2V5qlVL3NpjOX4u14Hqxt27Yfu1+xIhQvAED52+p1YDZu3JjGn8QwE2cTxVlDJfPmzUvTpuMYmShex1NQS5YsqWozbdq0FE7iaahSm+rbKLUpbQMAoGVdT9OccMIJaWDuypUr04yjuGbL1KlT07iTYcOGhZEjR6aZSTGUXHTRRSl4xBlI0YABA1JQGTJkSLjxxhvTeJcrr7wyrR1Tqp7EcTXjxo0Lo0aNCuecc054/PHHw/33359mJgEA1DnAxMpJXLclrt8SA0tc1C6Gly9/+cvp8Ztvvjk0b948LWAXqzJx9tDtt99e9fwWLVqESZMmpbEzMdjstNNOaQzN9ddfX9WmZ8+eKazENWXiqam49sxdd91lDRgAoOHWgWmqrAMD25Z1YIDG/Pyu8yBeoLKVS3DZ9PUIMpAXX+YIAGRHgAEAsiPAAADZEWAAgOwIMABAdgQYACA7AgwAkB0BBgDIjgADAGRHgAEAsiPAAADZEWAA/vudSOX2PU9QzgQYACA7AgwAkB0BBgDIjgADAGRHgAEAsiPAAADZEWAAgOwIMABAdgQYACA7LRt7B4A8WKUWaEpUYACA7AgwAEB2BBgAIDsCDACQHQEGAMiOAAMAZEeAAQCyYx0YoFbWfwGaIhUYACA7AgwAkB0BBgDIjgADAGTHIF6Ajxm0/MbYQY26L8DHU4EBALIjwAAA2RFgAIDsCDAAQHYEGAAgOwIMAJAdAQYAyI4AAwBkR4ABALIjwAAA2fFVAkCty+kDNEUqMABAdgQYACA7AgwAkB0BBgDIjgADAGRHgAEAsiPAAADZEWAAalkPx5o40DQJMABAdgQYACA7AgwAkB0BBgDIjgADAGRHgAEAsiPAAADZEWAAgOy0bOwdAJoOi7bV3i9vjB3U2LsC/JcKDACQHQEGAMiOAAMAZEeAAQDKO8CMGTMmHHbYYWGXXXYJnTt3DieffHKYN29ejTarV68Ow4cPDx07dgw777xzOPXUU8PixYtrtFmwYEEYNGhQ2HHHHdN2Lr300rB+/foabWbMmBH69OkT2rRpE/bee+8wfvz4rXmdAEClBpgnnngihZNnnnkmTJs2Laxbty4MGDAgfPDBB1VtRowYER5++OEwceLE1P7tt98Op5xyStXjGzZsSOFl7dq14emnnw533313CidXX311VZv58+enNscee2x46aWXwsUXXxzOPffcMHXq1IZ63QBAxpoVRVHU98lLly5NFZQYVI4++uiwfPny0KlTp3DvvfeG0047LbWZO3du6NWrV5g5c2Y4/PDDw6OPPhpOPPHEFGy6dOmS2tx5553hsssuS9tr3bp1+nny5Mlh9uzZVb9r8ODBYdmyZWHKlClbtG8rVqwI7du3T/vUrl27+r5EqCimUdfONGrY9rb083urxsDEjUe77rprup41a1aqyvTv37+qzX777Rd69OiRAkwUrw866KCq8BINHDgw7fCcOXOq2lTfRqlNaRubs2bNmrSN6hcAoDzVeyG7jRs3plM7RxxxRDjwwAPTfYsWLUoVlA4dOtRoG8NKfKzUpnp4KT1eeqy2NjGUrFq1KrRt23az43Ouu+66+r4cqGgqL0Bu6l2BiWNh4imeCRMmhKZg9OjRqSJUuixcuLCxdwkAaEoVmAsvvDBMmjQpPPnkk2GPPfaour9r165pcG4cq1K9ChNnIcXHSm2ee+65GtsrzVKq3mbTmUvxdjwXtrnqSxRnK8ULAFD+6lSBieN9Y3h58MEHw+OPPx569uxZ4/G+ffuGVq1ahenTp1fdF6dZx2nT/fr1S7fj9csvvxyWLFlS1SbOaIrhZP/9969qU30bpTalbQAAla1lXU8bxRlGf/jDH9JaMKUxK3G0cKyMxOthw4aFkSNHpoG9MZRcdNFFKXjEGUhRnHYdg8qQIUPCjTfemLZx5ZVXpm2XKijnn39+GDduXBg1alQ455xzUli6//7708wkAIA6VWDuuOOONL7kmGOOCd26dau63HfffVVtbr755jRNOi5gF6dWx9NBDzzwQNXjLVq0SKef4nUMNmeddVY4++yzw/XXX1/VJlZ2YliJVZfevXuHn/zkJ+Guu+5KM5EAALZqHZimzDowsOXMQtoy1oGBMlkHBgAgq3VggPypvAC5UoEBALIjwAAA2RFgAIDsCDAAdRgzZNwQNA0CDACQHQEGAMiOAAMAZEeAAQCyI8AAANkRYACA7AgwAEB2BBgAIDsCDACQHQEGAMiOAAMAZEeAAQCyI8AAANlp2dg7AGxfvk0ZKAcCDMBWhMA3xg5q1H2BSuUUEgCQHQEGAMiOAAMAZEeAAQCyYxAvVAizj4ByogIDAGRHgAEAsiPAAADZEWAAgOwIMABAdgQYACA7AgwAkB0BBgDIjoXsoMxZwA4oRyowAFsZEIVE2P4EGAAgOwIMAJAdAQYAyI4AAwBkR4ABALIjwAAA2RFgAIDsCDAAQHYEGAAgOwIMAJAd34UEZcjS9kC5U4EBALIjwAAA2RFgAIDsCDAAQHYEGIAGGjht8DRsPwIMAJAdAQYAyI51YKCMOIUBVAoVGAAgOwIMAJAdAQYAyI4AAwBkR4ABaEDWg4HtQ4ABALIjwAAA2RFgAIDsCDAAQHYEGAAgOwIMAJAd34UEZcC0XaDS1LkC8+STT4avfvWrYffddw/NmjULDz30UI3Hi6IIV199dejWrVto27Zt6N+/f/jnP/9Zo817770XzjzzzNCuXbvQoUOHMGzYsPD+++/XaPP3v/89HHXUUWGHHXYI3bt3DzfeeGN9XyMAUOkB5oMPPgi9e/cOt91222Yfj0HjZz/7WbjzzjvDs88+G3baaacwcODAsHr16qo2MbzMmTMnTJs2LUyaNCmFovPOO6/q8RUrVoQBAwaEPffcM8yaNSvcdNNN4dprrw2//OUv6/s6AYAy0qyIJZP6PrlZs/Dggw+Gk08+Od2Om4qVmUsuuSR8//vfT/ctX748dOnSJYwfPz4MHjw4/OMf/wj7779/eP7558Ohhx6a2kyZMiV85StfCW+99VZ6/h133BGuuOKKsGjRotC6devU5vLLL0/Vnrlz527RvsUQ1L59+/T7Y6UHyplTSE3PG2MHNfYuQJa29PO7QQfxzp8/P4WOeNqoJO7EF77whTBz5sx0O17H00al8BLF9s2bN08Vm1Kbo48+uiq8RLGKM2/evPCf//xns797zZo16UVXvwA09lcKCJewbTRogInhJYoVl+ri7dJj8bpz5841Hm/ZsmXYdddda7TZ3Daq/45NjRkzJoWl0iWOmwEAylPZTKMePXp0KjeVLgsXLmzsXYJtzr/wgUrVoAGma9eu6Xrx4sU17o+3S4/F6yVLltR4fP369WlmUvU2m9tG9d+xqTZt2qRzZdUvAEB5atAA07NnzxQwpk+fXnVfHIsSx7b069cv3Y7Xy5YtS7OLSh5//PGwcePGNFam1CbOTFq3bl1Vmzhjad999w2f+tSnGnKXAYBKCDBxvZaXXnopXUoDd+PPCxYsSLOSLr744nDDDTeEP/7xj+Hll18OZ599dppZVJqp1KtXr3D88ceH73znO+G5554LTz31VLjwwgvTDKXYLjrjjDPSAN64Pkycbn3fffeFW2+9NYwcObKhXz8AUAkr8f71r38Nxx57bNXtUqgYOnRomio9atSotFZMXNclVlqOPPLINE06LkhXcs8996TQctxxx6XZR6eeempaO6YkDsJ97LHHwvDhw0Pfvn3DbrvtlhbHq75WDABQubZqHZimzDowlCuDdvNjTRho4uvAAABsDwIMAJAdAQYAyI4AAwBkR4ABAMp/GjXQOMw+yv//ndlI0HBUYACA7AgwAEB2BBgAIDsCDACQHQEGAMiOWUjQxJl9BPBRKjAAQHYEGAAgO04hQRPl1FH5saAdNBwVGAAgOyow0ISougBsGRUYACA7AgwAkB0BBqARThU6XQhbR4ABALIjwAAA2RFgAIDsmEYN0Eiqj4OxuB3UjQADTYABnQB14xQSAJAdAQYAyI4AAwBkR4ABALIjwAAA2RFgoBFYSh5g65hGDY1IiAGoHxUYgCZAVQ7qRgUGthMfTgANRwUGAMiOAAMAZEeAAQCyYwwMbGPGvgA0PBUYACA7AgxAE2I6NWwZp5CggfnwoSGPozfGDmrsXYEmSQUGAMiOAAMAZEeAAQCyYwwMNADjXgC2LxUYACA7AgwAkB2nkGArOHUE0DgEGKgHwYXGONasCQP/4xQSAJAdFRioA5UXGpPVeeF/VGAAgOwIMPAxfKkeQNMlwABkRrgGY2DgE/mgAGh6BBjYhMBCLgzqpZI5hQQAZEcFBv5L5YVcWeyOSqQCAwBkRwWGiqbqApAnAYaKJLhQrgzspVI4hQRQhqwVQ7lTgaGieEMHKA8CDGVLWAEzlChfTiEBANlRgaFsqLhA3f5GVGTImQBD9gQXaLi/HaGGXDTpAHPbbbeFm266KSxatCj07t07/PznPw+f//znG3u3aAKEFtg2VGnIRZMdA3PfffeFkSNHhmuuuSa88MILKcAMHDgwLFmypLF3jUZkaigAUbOiKIqm2BVf+MIXwmGHHRbGjRuXbm/cuDF07949XHTRReHyyy//xOevWLEitG/fPixfvjy0a9duO+wx2+JffMIKND2qMmxLW/r53SRPIa1duzbMmjUrjB49uuq+5s2bh/79+4eZM2du9jlr1qxJl5L4wksdwfZx4DVTG2Q7PUZMbJDtANtGff5GZ183cJvsC+Wn9Ln9SfWVJhlg3n333bBhw4bQpUuXGvfH23Pnzt3sc8aMGROuu+66j9wfqzYANK72tzT2HpCblStXpkpMVgGmPmK1Jo6ZKYmnnN57773QsWPH0KxZs9DU02YMWgsXLnS6axP6pnb6p3b6p3b6p3b6p3H6JlZeYnjZfffda23XJAPMbrvtFlq0aBEWL15c4/54u2vXrpt9Tps2bdKlug4dOoScxIPAH8nm6Zva6Z/a6Z/a6Z/a6Z/t3ze1VV6a9Cyk1q1bh759+4bp06fXqKjE2/369WvUfQMAGl+TrMBE8XTQ0KFDw6GHHprWfrnlllvCBx98EL797W839q4BAI2syQaY008/PSxdujRcffXVaSG7Qw45JEyZMuUjA3vLQTz1Fde72fQUGPrmk+if2umf2umf2umfpt03TXYdGACArMbAAADURoABALIjwAAA2RFgAIDsCDDbwG233RY+85nPhB122CF9KeVzzz33sW2POeaYtFLwppdBg/73ZWlxnHWcjdWtW7fQtm3b9J1Q//znP0OuGrp/vvWtb33k8eOPPz5UQv9EcYmBfffdNx0bcWXMESNGhNWrV2/VNiulb6699tqPHDv77bdfyFVd+mfdunXh+uuvD5/97GdT+969e6eZnluzzUrrn3I5fp588snw1a9+Na18G1/DQw899InPmTFjRujTp0+ahbT33nuH8ePHb/9jJ85CouFMmDChaN26dfHrX/+6mDNnTvGd73yn6NChQ7F48eLNtv/3v/9dvPPOO1WX2bNnFy1atCh+85vfVLUZO3Zs0b59++Khhx4q/va3vxVf+9rXip49exarVq0qcrMt+mfo0KHF8ccfX6Pde++9V+Sorv1zzz33FG3atEnX8+fPL6ZOnVp069atGDFiRL23WUl9c8011xQHHHBAjWNn6dKlRY7q2j+jRo0qdt9992Ly5MnFa6+9Vtx+++3FDjvsULzwwgv13mal9U+5HD+PPPJIccUVVxQPPPBAnJVcPPjgg7W2f/3114sdd9yxGDlyZPHKK68UP//5z9P78pQpU7brsSPANLDPf/7zxfDhw6tub9iwIf0RjBkzZouef/PNNxe77LJL8f7776fbGzduLLp27VrcdNNNVW2WLVuW3ph/97vfFZXeP6UAc9JJJxXloK79E9t+6UtfqnFffFM54ogj6r3NSuqb+AHUu3fvohzUtX9imBs3blyN+0455ZTizDPPrPc2K61/yun4KdmSABPDXQxu1Z1++unFwIEDt+ux4xRSA1q7dm2YNWtWOsVT0rx583R75syZW7SNX/3qV2Hw4MFhp512Srfnz5+fFvKrvs34HRGxHLel2yzn/qlezuzcuXM6XXDBBReEf//73yE39emfL37xi+k5pdLs66+/Hh555JHwla98pd7brJS+KYmnY2PpfK+99gpnnnlmWLBgQchNffpnzZo1qbRfXTzV9pe//KXe26yk/imn46euYp9V78to4MCBVX25vY4dAaYBvfvuu2HDhg0fWS043o4h5JPEN9rZs2eHc889t+q+0vPqu81y758ojnf57W9/m74r68c//nF44oknwgknnJB+V7n3zxlnnJHO0x955JGhVatW6Xx9HDf0gx/8oN7brJS+ieI/BOK5+zi24Y477kj/YDjqqKPSN+HmpD79Ez9wfvrTn6YP4Phdc9OmTQsPPPBAeOedd+q9zUrqn3I6fuoq9tnm+jJ+Q/WqVau227EjwDQhsbpw0EEHpe9+Ysv7J1Zkvva1r6XHTj755DBp0qTw/PPPp6pMuYuv8Uc/+lG4/fbbwwsvvJDeYCdPnhx++MMfhkq3JX0Tg+43vvGNcPDBB6cPrFihWbZsWbj//vtDubv11lvDPvvskwadxi/QvfDCC9N3zcV/KbNl/VPJx09T4EhtQLvttlto0aJFWLx4cY374+2uXbvW+tz4RZUTJkwIw4YNq3F/6Xn12WYl9M/mxFJu/F2vvvpqKPf+ueqqq8KQIUNSVSoGuK9//evpQ3vMmDHpX41b0+fl3jeb06FDh/C5z32uIo6dTp06pdkm8W/rzTffDHPnzg0777xz+vup7zYrqX/K6fipq9hnm+vLdu3apdNs2+vYEWAaUEzpffv2TacySuIbZbzdr1+/Wp87ceLEdM71rLPOqnF/z5490//w6tuMZbpnn332E7dZCf2zOW+99VYaAxOnnZd7/3z44Ycf+RdzfOOI4ni8renzcu+bzXn//ffDa6+9VhHHTkkc5/HpT386rF+/Pvz+978PJ5100lZvsxL6p5yOn7qKfVa9L6N4iq3Ul9vt2Gmw4cBUTR2LM4TGjx+fppedd955aerYokWL0uNDhgwpLr/88o8878gjj0yjuDcnTqOO2/jDH/5Q/P3vf08zbnKeRt2Q/bNy5cri+9//fjFz5sw0VfZPf/pT0adPn2KfffYpVq9eXZR7/8RZEHFWVpyRFqc2PvbYY8VnP/vZ4pvf/OYWb7OS++aSSy4pZsyYkY6dp556qujfv3+x2267FUuWLClyU9f+eeaZZ4rf//73aYrwk08+mWZsxfeV//znP1u8zUrvn3I5flauXFm8+OKL6RJjwU9/+tP085tvvpkej/0S+2fTadSXXnpp8Y9//KO47bbbNjuNelsfOwLMNhDnxPfo0SPNgY9TyeIfQsn//d//pWm/1c2dOzcdNPENdnPiVOqrrrqq6NKlSzogjjvuuGLevHlFrhqyfz788MNiwIABRadOnYpWrVoVe+65Z1pvIMc32Pr0z7p164prr702fTDHNSq6d+9efPe7363xJvtJ26zkvomhOE6Xjdv79Kc/nW6/+uqrRa7q0j/xg7dXr17pPaVjx47pA+pf//pXnbZZ6f1TLsfPn//85/Qeu+ml1B/xOvbPps855JBD0mvfa6+9aqzNtb2OnWbxPw1XzwEA2PaMgQEAsiPAAADZEWAAgOwIMABAdgQYACA7AgwAkB0BBgDIjgADAGRHgAEAsiPAAADZEWAAgOwIMABAyM3/A11LP2Kc29mkAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"data/cleaned_base_data.json\", \"r\") as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "negatives: pd.DataFrame = pd.read_csv(\"data/negatives.csv\")[\"review_text\"]\n",
    "\n",
    "triplets = []\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Test the model\n",
    "\n",
    "sims = []\n",
    "sim_lkp = {}\n",
    "\"\"\"\n",
    "encoded_queries = {}\n",
    "with tqdm(total=len(data[\"query\"]), desc=\"embedding_queries\") as pbar:\n",
    "    for key, value in data[\"query\"].items():\n",
    "        emb = model.encode(value)\n",
    "        encoded_queries[key] = emb\n",
    "        pbar.update(1)\n",
    "\"\"\"\n",
    "with tqdm(total=len(encoded_queries.keys()) ** 2, desc=\"Testing Query Embedding Span\") as pbar:\n",
    "    for x_id, x_emb in encoded_queries.items():\n",
    "        sim_lkp[x_id] = {}\n",
    "        for y_id, y_emb in encoded_queries.items():\n",
    "            if x_id == y_id:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "                \n",
    "        dot_product = np.dot(x_emb, y_emb)\n",
    "        magnitude_x = np.linalg.norm(x_emb)\n",
    "        magnitude_y = np.linalg.norm(y_emb)\n",
    "        cosine_similarity = dot_product / (magnitude_x * magnitude_y)\n",
    "        sims.append(cosine_similarity)\n",
    "        sim_lkp[x_id][y_id] = cosine_similarity\n",
    "        pbar.update(1)\n",
    "\n",
    "x = np.array(sims)\n",
    "plt.hist(x, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec28619c-fe00-4baa-832a-4c8db86b7f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating positive negative pairs:   0%|                                                                                                                                                  | 26/150672 [00:11<18:19:38,  2.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m x:\n\u001b[1;32m     19\u001b[0m     negative \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlist\u001b[39m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorpus\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m---> 20\u001b[0m     negative_emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnegative\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     dot_product \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(anchor_emb, negative_emb)\n\u001b[1;32m     23\u001b[0m     magnitude_A \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(anchor_emb)\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:623\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 623\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    625\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:690\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    689\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:442\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    437\u001b[0m     key: value\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    440\u001b[0m }\n\u001b[0;32m--> 442\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/transformers/pytorch_utils.py:260\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"data/cleaned_base_data.json\", \"r\") as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "negatives: pd.DataFrame = pd.read_csv(\"data/negatives.csv\")[\"review_text\"]\n",
    "\n",
    "total = 0\n",
    "for id, relevant_docs in data[\"relevant_docs\"].items():\n",
    "    total += len(relevant_docs)\n",
    "\n",
    "triplets = []\n",
    "\n",
    "with tqdm(total=total, desc=\"creating positive negative pairs\") as pbar:\n",
    "    for query_id, doc_ids in data[\"relevant_docs\"].items():\n",
    "        anchor = data[\"query\"][query_id]\n",
    "        anchor_emb = model.encode(anchor)\n",
    "        for id in doc_ids:\n",
    "            x = True\n",
    "            while x:\n",
    "                negative_id, negative_query = random.choice(list(data[\"query\"].items()))\n",
    "                negative_emb = model.encode(negative_query)\n",
    "                \n",
    "                dot_product = np.dot(anchor_emb, negative_emb)\n",
    "                magnitude_A = np.linalg.norm(anchor_emb)\n",
    "                magnitude_B = np.linalg.norm(negative_emb)\n",
    "                cosine_similarity = dot_product / (magnitude_A * magnitude_B)\n",
    "                if cosine_similarity <= 0.81:\n",
    "                    _id = random.choice(data[\"relevant_cods\"])\n",
    "                    negative = random.choice(data[]\n",
    "                    x = False\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            triplets.append(\n",
    "                {\n",
    "                    \"anchor\": anchor,\n",
    "                    \"positive\": data[\"corpus\"][id],\n",
    "                    \"negative\": negative,\n",
    "                }\n",
    "            )\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e359a-ada9-4e37-956a-f04a0a53fbac",
   "metadata": {},
   "source": [
    "Now that we have our triplet data, the last thing is to split our data into Train, Test and Validation.\n",
    "We use Train data to finetune our model, we use test data to estimate the performance of the model during training, and validition to ensure that we did not just find a model that fit our test data well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3bc5a38-f5da-4d2f-97fc-53679ad89449",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplet, val_triplet = train_test_split(pd.DataFrame(triplets), test_size=0.2)\n",
    "train_triplet, test_triplet = train_test_split(\n",
    "    pd.DataFrame(train_triplet), test_size=0.2\n",
    ")\n",
    "\n",
    "train_triplet.to_json(\"data/triplet_data_train.json\")\n",
    "test_triplet.to_json(\"data/triplet_data_test.json\")\n",
    "val_triplet.to_json(\"data/triplet_data_val.json\")\n",
    "\n",
    "dataset: DatasetDict = {\n",
    "    \"train\": Dataset.from_pandas(train_triplet, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(test_triplet, preserve_index=False),\n",
    "    \"validation\": Dataset.from_pandas(val_triplet, preserve_index=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a4705d-0c74-4b5c-8f9c-ac263b2ec7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplet, val_triplet = train_test_split(pd.DataFrame(triplets), test_size=0.2)\n",
    "train_triplet, test_triplet = train_test_split(\n",
    "    pd.DataFrame(train_triplet), test_size=0.2\n",
    ")\n",
    "\n",
    "train_triplet.to_json(\"data/triplet_data_train.json\")\n",
    "\n",
    "\n",
    "\n",
    "test_triplet.to_json(\"data/triplet_data_test.json\")\n",
    "val_triplet.to_json(\"data/triplet_data_val.json\")\n",
    "\n",
    "dataset: DatasetDict = {\n",
    "    \"train\": Dataset.from_pandas(train_triplet, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(test_triplet, preserve_index=False),\n",
    "    \"validation\": Dataset.from_pandas(val_triplet, preserve_index=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dce45a-b4d9-46cc-86ef-d199e1e08636",
   "metadata": {},
   "source": [
    "### Baseline Model and Valuation Strategy\n",
    "\n",
    "Our data is now ready, but before we start finetuning our model, we should consider how we are going to decide whether or not the finetuned model is better than the baseline one. \n",
    "\n",
    "To do this we have to do the following steps:\n",
    "1. Download a Baseline model\n",
    "2. Find a valuation method\n",
    "3. Valuate the Baseline model\n",
    "4. Finetune the model\n",
    "5. Valuate the finetuned model\n",
    "\n",
    "The first thing we will do is to download the baseline model from HuggingFace, using the SentenceTransformer library. I have chosen the [multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) model. I have chosen this model because it has a good baseline performance, and is small enough to train on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "503dae50-587a-4fe9-9585-45b00ad99927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Size:  384\n"
     ]
    }
   ],
   "source": [
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Test the model\n",
    "emb = model.encode(\"Hello World\")\n",
    "print(\"Vector Size: \", len(emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35baf91c-f36a-454d-8343-b30188a541e4",
   "metadata": {},
   "source": [
    "Now that we've downloaded our baseline model, let's take a moment to think about how to best evaluate its performance.\n",
    "\n",
    "Now that we've downloaded our baseline model, let's spend some time thinking about how to best evaluate our model.\n",
    "\n",
    "A good place to start is here: [SentenceTransformer Evaluator Classes](https://sbert.net/docs/sentence_transformer/training_overview.html#evaluator).\n",
    "\n",
    "Going through the list, there are several that are interesting, but the one that fits our data best is the [Triplet Evaluator](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.TripletEvaluator).\n",
    "\n",
    "What the Triplet Evaluator does is compare the similarity between the anchor and the positive, as well as between the anchor and the negative. It then returns a percentage representing the proportion of records where the anchor embedding was closer to the positive embedding than to the negative. So, a result of 0.8 means that in 80% of cases, the anchor was closer to the positive than to the negative.\n",
    "\n",
    "We can run the model on our data using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "049b8f12-8632-4f4d-85d6-23eae4e78920",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_evaluator = TripletEvaluator(\n",
    "    anchors=dataset[\"test\"][\"anchor\"],\n",
    "    positives=dataset[\"test\"][\"positive\"],\n",
    "    negatives=dataset[\"test\"][\"negative\"],\n",
    "    name=\"dev_evaluator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "274b8379-29c0-4c5f-a8dd-9ee24ec61a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dev_evaluator_cosine_accuracy': 0.9143446683883667}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_evaluator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba2997-0549-4d03-a975-2959bd6a93df",
   "metadata": {},
   "source": [
    "With our test data, we get a result of 0.91 using our baseline model. This means that in 91% of cases, the baseline model will place the anchor closer to the positive than to the negative.\n",
    "\n",
    "This is a great evaluation to start with, but in reality, this is not what we are truly interested in. We want to know whether inputting a subject will return related chunks of text. So, let's try to write a custom evaluation script. I will base my model on the Recall@K metric. The metric simply tests whether the anchor returns the positive in its top K results.\n",
    "\n",
    "We will modify this to check if any of the related documents are in the top K results. Additionally, we will introduce a metric to show the percentage of relevant documents present in the top K results.\n",
    "\n",
    "Before writing this test, we need to store the embeddings somewhere so we can query them. Many databases can handle this, but I've chosen to use Postgres with the PG Vector extension. Here's how to set it up in your Postgres database:\n",
    "\n",
    "First, install the extension by running the following command:\n",
    "```sql\n",
    "CREATE EXTENSION vector;\n",
    "```\n",
    "Then, we will create two tables to store our embeddings: one for our baseline embeddings and another for the fine-tuned embeddings:\n",
    "```sql\n",
    "CREATE TABLE embeddings_base (id bigserial PRIMARY KEY, speech VARCHAR, context_id VARCHAR, context VARCHAR, embedding vector(384))\n",
    "CREATE TABLE embeddings (id bigserial PRIMARY KEY, speech VARCHAR, context_id VARCHAR, context VARCHAR, embedding vector(384))\n",
    "```\n",
    "You might wonder about the vector(384) datatype. This simply means we are storing a vector of size 384. If you are unsure about the size of your embedding, you can find out by running:\n",
    "```python\n",
    "len(model.encode(\"Hello\"))\n",
    "```\n",
    "Now that our table is ready for embeddings, we can load them. Let's do that using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "085f5d21-6761-4b73-905b-5a5c33321797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(speech: str, context_id: str, context: str, embedding: List, write_to_base: bool) -> None:\n",
    "    conn = pg.connect(\"dbname=vector_rag user=postgres password=postgres\")\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    if write_to_base:\n",
    "        cur.execute(\n",
    "            \"INSERT INTO embeddings_base (speech, context_id, context, embedding) VALUES (%s, %s, %s, %s)\",\n",
    "            (speech, context_id, context, str(embedding)),\n",
    "        )\n",
    "    else:\n",
    "        cur.execute(\n",
    "            \"INSERT INTO embeddings (speech, context_id, context, embedding) VALUES (%s, %s, %s, %s)\",\n",
    "            (speech, context_id, context, str(embedding)),\n",
    "        )\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc690ec7-5dce-4f8d-8dfc-191ae7744681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving embeddings: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1173/1173 [01:24<00:00, 13.90it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(data[\"corpus\"].keys()), desc=\"Saving embeddings\") as pbar:\n",
    "    for id, context in data[\"corpus\"].items():\n",
    "        speech_name = data[\"related_speech\"][id]\n",
    "        embedding = model.encode(context).tolist()\n",
    "        write_to_db(speech_name, id, context, embedding, True)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d029d7d-6017-4e98-b917-a6605b763803",
   "metadata": {},
   "source": [
    "Then we can implement our recall k methods and test our baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69072b26-c9bd-4c83-b335-502f6fdbe58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 Metric:  0.2868937048503612\n",
      "Recall@10 Metric %:  0.2770979736924009\n",
      "Recall@4 Metric:  0.1785345717234262\n",
      "Recall@4 Metric %:  0.1719986240110079\n"
     ]
    }
   ],
   "source": [
    "# Add lookup to make checks faster\n",
    "data[\"query_lk\"] = {}\n",
    "for key, value in data[\"query\"].items():\n",
    "    data[\"query_lk\"][value] = key\n",
    "\n",
    "def recall_k(query: str, model: SentenceTransformer, k: int, data: dict, check_base: bool) -> float:\n",
    "    query_id = data[\"query_lk\"][query]\n",
    "    expected_ids = data[\"relevant_docs\"][query_id]\n",
    "    embedded_query = model.encode(query).tolist()\n",
    "\n",
    "    conn = pg.connect(\"dbname=vector_rag user=postgres password=postgres\")\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    if check_base:\n",
    "        cur.execute(\n",
    "            \"SELECT context_id FROM embeddings_base ORDER BY embedding <=> %s::vector LIMIT %s;\",\n",
    "            (str(embedded_query), str(k)),\n",
    "        )\n",
    "    else: \n",
    "        cur.execute(\n",
    "            \"SELECT context_id FROM embeddings ORDER BY embedding <=> %s::vector LIMIT %s;\",\n",
    "            (str(embedded_query), str(k)),\n",
    "        )\n",
    "        \n",
    "    results = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    min_res = min(len(expected_ids), k)\n",
    "    result_per = len(set(results) & set(expected_ids)) / min_res\n",
    "    \n",
    "    result_k = 1.0 if set(results) & set(expected_ids) else 0.0\n",
    "\n",
    "    return result_k, result_per\n",
    "\n",
    "\n",
    "recall_10 = test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 10, data, True), axis=1\n",
    "    )\n",
    "recall_10 = pd.DataFrame(recall_10.tolist(), index=recall_10.index)\n",
    "print(\"Recall@10 Metric: \", recall_10[0].sum() / test_triplet.shape[0])\n",
    "print(\"Recall@10 Metric %: \", recall_10[1].sum() / test_triplet.shape[0])\n",
    "\n",
    "\n",
    "recall_4 = test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 4, data, True), axis=1\n",
    "    )\n",
    "recall_4 = pd.DataFrame(recall_4.tolist(), index=recall_4.index)\n",
    "print(\"Recall@4 Metric: \", recall_4[0].sum() / test_triplet.shape[0])\n",
    "print(\"Recall@4 Metric %: \", recall_4[1].sum() / test_triplet.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df99a0-9d78-46fc-808c-6395470139a8",
   "metadata": {},
   "source": [
    "With our baseline, we can now try to fine-tune our model to see if we can improve its performance.\n",
    "\n",
    "### Fine-Tuning with Limited Compute and Memory\n",
    "Although we've chosen a very small model, my laptop does not have enough memory to train it locally. This leaves me with two options:\n",
    "\n",
    "Rent a Bigger Machine: This would certainly make the process easier, but I've always found it more interesting to work within limitations.\n",
    "Work Within Constraints: I've decided to take the more challenging route and explore ways to fine-tune the model using limited resources.\n",
    "One promising solution is to perform fine-tuning using [Low Rank Adaptation (LoRA)](https://arxiv.org/pdf/2106.09685). \n",
    "\n",
    "When using LoRA, we freeze all the parameters in the baseline model and train a smaller adaptation layer, which is then multiplied onto the original weights. This significantly reduces the number of parameters we need to train. According to the original paper, this approach can reduce memory requirements threefold.\n",
    "![LowRank Adaptation](images/LoRA.png)\n",
    "\n",
    "Let's see the difference in trainable parameters before and after applying the LoRA adapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d803f126-ef4d-4b65-86f5-dce3ee669d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model parameters:    117,653,760\n",
      "----------------------------------------------------------------------\n",
      "Total trainable parameters before LoRA: 117,653,760\n",
      "Trainable Percentage trainable before LoRA:     100.00%\n",
      "----------------------------------------------------------------------\n",
      "Total trainable parameters after LoRA: 669,696\n",
      "Trainable Percentage trainable after LoRA:     0.28%\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainable_params = 0\n",
    "all_params = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    all_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "        # print(f\"{name}: shape={param.shape}, params={param.numel()}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total model parameters:    {all_params:,}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Total trainable parameters before LoRA: {trainable_params:,}\")\n",
    "print(f\"Trainable Percentage trainable before LoRA:     {100 * trainable_params / all_params:.2f}%\")\n",
    "print(\"-\" * 70)\n",
    "## Adding LoRA Adapter\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model.add_adapter(peft_config)\n",
    "\n",
    "trainable_params_lora = 0\n",
    "for name, param in model.named_parameters():\n",
    "    all_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params_lora += param.numel()\n",
    "print(f\"Total trainable parameters after LoRA: {trainable_params_lora:,}\")\n",
    "print(f\"Trainable Percentage trainable after LoRA:     {100 * trainable_params_lora / all_params:.2f}%\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1d55f9-163e-4e1c-8fbd-11b0e2fb1f0b",
   "metadata": {},
   "source": [
    "As you can see, we reduce the trainable parameters by 99.72%, which is quite significant and ensures that this can run on my Mac. Now, let's get to the training.\n",
    "\n",
    "First, we need to decide on a loss function. To do that, I referred to the [Sentence Transformer Loss Overview](https://sbert.net/docs/sentence_transformer/loss_overview.html), and found that MultipleNegativesRankingLoss fits my data well.\n",
    "\n",
    "Second, we need to set some hyperparameters in our training arguments. There is no one-size-fits-all solution when choosing hyperparameters. From what I've read, the best approach is to experiment. Below are the hyperparameters I've chosen. I’d like to point out that the low batch size of 8 is primarily due to my memory limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f41d29b0-bf5e-4df4-8206-6a879b02e7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3086644b5f4a65a7fed11e9f862987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1452' max='1452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1452/1452 45:54, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.108800</td>\n",
       "      <td>1.862558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.956600</td>\n",
       "      <td>0.832888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.886300</td>\n",
       "      <td>0.748309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.823600</td>\n",
       "      <td>0.715428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.772200</td>\n",
       "      <td>0.690172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.783000</td>\n",
       "      <td>0.671355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.737700</td>\n",
       "      <td>0.661641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.823900</td>\n",
       "      <td>0.651434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.743400</td>\n",
       "      <td>0.644150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.661100</td>\n",
       "      <td>0.641188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.674800</td>\n",
       "      <td>0.637479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.785900</td>\n",
       "      <td>0.635036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.824200</td>\n",
       "      <td>0.633360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.540200</td>\n",
       "      <td>0.632758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1452, training_loss=0.8919132413450351, metrics={'train_runtime': 2756.9916, 'train_samples_per_second': 4.213, 'train_steps_per_second': 0.527, 'total_flos': 0.0, 'train_loss': 0.8919132413450351, 'epoch': 2.9979338842975207})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"../models/multilingual-e5-small-finetune-danish-subject\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    bf16=True,  \n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch according to the documentation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=25,\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3803d63b-01f7-4222-bcaa-15c126013bc6",
   "metadata": {},
   "source": [
    "Now that we have a trained model, and before we test it, I would like to share a bit about what I look for in the logging. First, I check to see if there is a steadily decreasing validation loss throughout the training process.\n",
    "\n",
    "What can sometimes happen is that while the training loss continues to decrease, the validation loss stops improving. This is usually a sign of overfitting the model.\n",
    "\n",
    "Second, I notice that even though my training loss is generally decreasing, there are occasional spikes. This is probably due to the small batch size. Unfortunately, I cannot change this because of memory limitations.\n",
    "\n",
    "Now, we can run our test again to see if performance has improved. To do this, we need to re-embed our corpus using the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "803239e9-7e38-4255-8c1d-7752d5e48ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving embeddings: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1173/1173 [02:38<00:00,  7.39it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(data[\"corpus\"].keys()), desc=\"Saving embeddings\") as pbar:\n",
    "    for id, context in data[\"corpus\"].items():\n",
    "        speech_name = data[\"related_speech\"][id]\n",
    "        embedding = model.encode(context).tolist()\n",
    "        write_to_db(speech_name, id, context, embedding, False)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d26a4b-31f5-4230-9e40-1c221af8083b",
   "metadata": {},
   "source": [
    "And finally we can test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6916e302-4054-4400-b26a-5b2fe0914582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 Metric:  0.4716202270381837\n",
      "Recall@10 Metric %:  0.44813381492948057\n",
      "Recall@4 Metric:  0.3240454076367389\n",
      "Recall@4 Metric %:  0.30822153422772613\n"
     ]
    }
   ],
   "source": [
    "\n",
    "recall_10 = test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 10, data, False), axis=1\n",
    "    )\n",
    "recall_10 = pd.DataFrame(recall_10.tolist(), index=recall_10.index)\n",
    "print(\"Recall@10 Metric: \", recall_10[0].sum() / test_triplet.shape[0])\n",
    "print(\"Recall@10 Metric %: \", recall_10[1].sum() / test_triplet.shape[0])\n",
    "\n",
    "\n",
    "recall_4 = test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 4, data, False), axis=1\n",
    "    )\n",
    "recall_4 = pd.DataFrame(recall_4.tolist(), index=recall_4.index)\n",
    "print(\"Recall@4 Metric: \", recall_4[0].sum() / test_triplet.shape[0])\n",
    "print(\"Recall@4 Metric %: \", recall_4[1].sum() / test_triplet.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da67cc5-bd32-40c1-b430-e7ee1b68b869",
   "metadata": {},
   "source": [
    "GREAT SUCCES!!!!! It looks like we've improved our model on all metrics. We did not have to send our data anywhere :D \n",
    "\n",
    "Before we finish for today, I would like to add a few words to the recall metrics. To b honest the result does not look that great.\n",
    "The reason is that, we are looking for very specific values. But when we look at the actual data what we see is the queries finding \n",
    "relevant text. Just not the specific documents we were looking for. The reason is that when our model generated subjects. It could generate\n",
    "multiple subjects which are very close in meaning to eachother. \n",
    "\n",
    "For instance we have 143 subjects with the word *pandemic* (pandemi in danish), this means that when testing for one of the pandemic subjects, \n",
    "we might get a relevant chunk from a different pandemic subject.\n",
    "\n",
    "I Would like for you to think at these measures as a way to compare the model performance before and after finetuning, and not as an absolute truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9966a83e-77c3-4d34-8919-745c5ce575d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([subject for subject in data[\"query_lk\"].keys() if \"pandemi\" in subject.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410fd7f-f8e6-4c36-af1c-9b601e08ef59",
   "metadata": {},
   "source": [
    "Now lets built a simple RAG application. Having the model this does not take a lot of code, which can be found below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52999739-a9ef-412f-81e9-82eef582cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "METTE_PROMPT_TEMPLATE = \"\"\"\\\n",
    "Du er en LLM som giver svarer på hvad Mette Frederiksen syntes om {question}. \\\n",
    "Du må kun besvarer baseret af de nedenstående citater, Du skal kun komme med et enkelt svar som er på Dansk \\\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\"\"\"\n",
    "\n",
    "def rag():\n",
    "    while True:\n",
    "        ### First we ask the user for input\n",
    "        query = input(\"Hvilket emne vil du høre Mette Frederiksens mening om? Skriv \\\\stop hvis du vil lukke programmet. \\n\\n\")\n",
    "\n",
    "        ### Check if user wants to exit\n",
    "        if query == \"\\\\stop\":\n",
    "            print(\"Farvel\")\n",
    "            break\n",
    "            \n",
    "        print(\"Tænker ...\")\n",
    "        emb = model.encode(query).tolist() ### Encode input\n",
    "        \n",
    "        print(\"Henter citater...\")\n",
    "        ### Query Database for relevant context\n",
    "        conn = pg.connect(\"dbname=vector_rag user=postgres password=postgres\")\n",
    "        conn.autocommit = True\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        cur.execute(\n",
    "            \"SELECT speech, context FROM embeddings ORDER BY embedding <-> %s::vector LIMIT 5;\",\n",
    "            (str(emb),),\n",
    "        )\n",
    "        \n",
    "        result = cur.fetchall()\n",
    "        \n",
    "        context_all = []\n",
    "        speeches =  []\n",
    "        for row in result:\n",
    "            speech = row[0]\n",
    "            speeches.append(speech)\n",
    "            context_all.append(f\"'{row[1]}'\")\n",
    "\n",
    "        ### Generate Prompt\n",
    "        prompt = METTE_PROMPT_TEMPLATE.format(context_str=\"\\n\\n\".join(list(set(context_all))), question=query)\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"Prøver at formulere mig...\")\n",
    "        \n",
    "        res: ChatResponse = chat(model='phi4', messages=[\n",
    "          {\n",
    "            'role': 'user',\n",
    "            'content': prompt,\n",
    "          },\n",
    "        ])\n",
    "        print(res.message.content)\n",
    "        print(\"\\n\\nLink til Taler og Citater som er valgt:\")\n",
    "        for row in result:\n",
    "            print(\"-\" * 100)\n",
    "            print(f\"Context:\\n{row[1]}\")\n",
    "            print(f\"\\nLink: https://www.dansketaler.dk/tale/{row[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4da060a7-4f24-4094-b8c7-a44574c398cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvilket emne vil du høre Mette Frederiksens mening om? Skriv \\stop hvis du vil lukke programmet. \n",
      "\n",
      " Kunstig Intelligens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tænker ...\n",
      "Henter citater...\n",
      "Prøver at formulere mig...\n",
      "This passage reflects on the current societal landscape, focusing particularly on the role of artificial intelligence (AI) and its implications. Here's a breakdown:\n",
      "\n",
      "1. **Artificial Intelligence and Change**:\n",
      "   - The text highlights AI as a transformative force in society, drawing parallels to past technological revolutions like computers defeating humans at chess.\n",
      "   - It acknowledges both positive potential—such as solving complex societal problems—and negative aspects, including misuse for misinformation or manipulation.\n",
      "\n",
      "2. **Ethical Concerns and Regulation**:\n",
      "   - There is an urgent call for ethical considerations and regulation regarding AI's development and use to prevent harm and ensure it benefits society democratically.\n",
      "   - The passage suggests that current mechanisms are insufficient, particularly in protecting vulnerable groups like children from potential exploitation by AI technologies.\n",
      "\n",
      "3. **Societal Responsibility**:\n",
      "   - Adults bear the responsibility of guiding technological advancements responsibly. This includes creating regulations and ensuring AI serves humanity's best interests.\n",
      "   - There is an emphasis on safeguarding personal data and maintaining open democratic dialogue as technology evolves rapidly.\n",
      "\n",
      "4. **Educational Initiatives**:\n",
      "   - The author describes a visit to FGU in Hvidovre, Denmark, an educational institution providing support to youths facing various challenges, such as bullying or academic difficulties.\n",
      "   - It underscores the importance of supportive learning environments and tailored education approaches for those who struggle within traditional systems.\n",
      "\n",
      "Overall, the passage calls for proactive measures to harness AI's potential while addressing its ethical and societal risks. It stresses collaboration between technological innovation and moral responsibility to foster a future that benefits all members of society.\n",
      "\n",
      "\n",
      "Link til Taler og Citater som er valgt:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:\n",
      "Det, jeg lige har læst op, er ikke skrevet af mig – eller af noget andet menneske for den sags skyld. Det er skrevet af kunstig intelligens, ChatGPT. Og selv om det måske ikke rammer helt plet, hverken med ordene eller detaljerne i regeringens arbejdsprogram eller for den sags skyld antallet af punktummer, er det både fascinerende og skræmmende, hvad det er i stand til: på få sekunder at skrive en tale, løse en universitetsopgave, udarbejde en rapport med så stor overbevisning, at de færreste af os vil tro, at det var en robot og ikke et menneske, der stod bag. Kunstig intelligens er ikke fremtiden; den er virkeligheden. Og den vil forandre vores samfund i et omfang, som jeg slet ikke tror vi forstår endnu. Hvorfor starter jeg med det her i dag i en tale, der skal samle op på det folketingsår, som vi netop afslutter? Det gør jeg, fordi jeg i stedet for at se tilbage og vil bruge min taletid i dag på at se fremad. I dag vil jeg henvende mig til alle unge i Danmark. Det er jeres fremtid, den her tale kommer til at handle om – en fremtid, som på en lang række områder adskiller sig markant fra jeres forældres og bedsteforældres. Den kunstige intelligens, der brager frem netop nu, er bare ét eksempel på de nye livsvilkår, der gælder for jer. Om de unge i dag er der sagt mange ord. I er blevet kaldt skrøbelige, snowflakes, curlingbørn. Der er ikke noget af det, der i dag er mit ærinde. Jeg ser en generation, der er fuld af drømme og håb med vilje til at forme jeres eget liv. I er en generation, der er historisk engageret i det samfund, der er omkring jer. Det ser vi ikke kun i Danmark, men også på Færøerne og i Grønland. I kunsten og i kulturen, i klimakampen, i kampen for global retfærdighed er det jer unge, der sætter præg på det og rækker langt ud over rigsfællesskabets grænser. Med jer bærer I en stærk bevidsthed om, hvor I kommer fra, en stolthed over jeres rødder og jeres sprog. Rodfæstede, men med blikket rettet udad og opsatte på at skabe forandringer er I unge bannerførere for en ny måde at gå ud i verden på. Når vi kigger på jer unge i det hele taget, vokser I op i en tid, der synes mere usikker med klimaforandringer, økonomisk uro og nye skarpe modsætninger i verden mellem os, der vil demokrati og frihed, og dem, der vil det modsatte. Krigen i Ukraine – et brutalt angreb på et fredeligt europæisk land – var et brud med alt det, vi troede var givet. I dag ser vi i al sin klarhed, at fred og fremgang ikke er noget, der kommer af sig selv, men noget, vi vedvarende må kæmpe for. Danmark er et af de lande, der støtter Ukraine allermest set i forhold til vores størrelse, og det gør vi med en selvfølgelighed, der klæder vores land. Vi gør det først og fremmest for at hjælpe soldaterne på slagmarken, men vi gør det også for at sikre danske børn og unge en fremtid med fred og frihed. Udfordringer er der masser af, men der er også grund til håb: et vestligt sammenhold, der står stærkere end i mange år, en robust, stærk dansk økonomi, nye løsninger på klimaforandringerne, stærke danske værdier, et levende fællesskab, en vidunderlig jordklode og dig midt i det hele – og lige rundt om hjørnet en dansk sommer med festivaler og lyse nætter, hvor I forhåbentlig kommer til at feste, til solen står op. Vi former selv vores fremtid, og sammen kan vi både gør den grønnere, lysere og friere. Og hver og en kan gøre en forskel, hvis vi og I griber de muligheder, der åbner sig hver eneste dag i vores fantastiske lille land. Lad mig starte der, med mulighederne. I de kommende uger vil tusindvis af glade unge strømme ud i vores gaderne, studenter med huer på hovedet og faglærte med svendebreve i hånden, og der vil være stolte forældre, taler, Dannebrog, fester og fællesskab. En ny årgang er klar til at indtage verden. Uddannelse åbner døren til fremtiden for hver enkelt af jer, og I gør vores samfund dygtigere og dygtigere for hver generation.Netop derfor er det regeringens klare ambition at få endnu flere med i fællesskabet. Jeg sagde for få sekunder siden, at Danmark er et land med mange muligheder. Men hånden på hjertet ved vi jo godt, at de muligheder er skævt fordelt. Det er bare lettere at gribe livet, hvis barndommen har været god. Det er lettere at gribe livet, hvis mor og far bakker op, og sådan er det desværre ikke for alle. Vi har stadig væk ikke et samfund, der reelt giver de samme muligheder til alle. Vi vil som regering gøre alt, hvad vi kan, for, at I unge, der lige nu hverken er i gang med uddannelse eller har job, får hjælp til at komme videre. For uddannelse og skole er horisonter, der udvides. Det er frihed, det er ånd, det er dannelse, det er viden, og det er evner og kompetencer. Og det er jeres mulighed for at få et godt arbejdsliv. I skal ud på et arbejdsmarked, hvor I først og fremmest er ventet med længsel. Takket være den ansvarlige økonomiske politik, som skiftende regeringer i Danmark har ført, er dansk økonomi i dag stærkere, end vi havde turdet håbe på. Aldrig før har så mange danskere været i arbejde, og det vilde er, at der er efterspørgsel på endnu flere. Men det er også et arbejdsmarked i hastig forandring, og det bringer mig tilbage til den kunstige intelligens. Det var en revolution, da en computer for knap 30 år siden for første gang besejrede et menneske i skak. I dag har robotterne for længst overhalet os, når det handler om at løse de mest komplicerede opgaver og overskue kolossale mængder af viden og data, og perspektiverne er svimlende. Kunstig intelligens kan flytte os fremad på en lang række områder. Måske kan den hjælpe os med at kurere alvorlige sygdomme, finde svar på de samfundsudfordringer, vi står over for, og i øvrigt løse opgaver, så vi kan frigøre tid til andre og vigtigere ting. Det er med andre ord en kraftfuld og potent teknologi, men det er også en teknologi, som vi kommer til at se blive misbrugt til at skabe forvrængede virkeligheder uden for demokratisk kontrol. Allerede nu er det svært at vide, om en tekst, et billede eller en tale er rigtig eller falsk. Den vil blive brugt til misinformation og hadtale og til at manipulere den offentlige samtale. Der er stærke algoritmer, der skaber ekkokamre. Der er børn, der får en kunstig ven på telefonen, som taler til dem, skaber tillid og fører samtaler. I sidste uge kom det frem – og man tror, det er løgn – at den kan vejlede vores børn om selvskade. Det er over alle grænser for etik og moral. I børn må ikke være forsøgskaniner i et eksperiment, som ingen af os voksne kender konsekvenserne af. Der er ikke tid til at være naive. Vi bliver nødt til at få greb om den her udvikling, og vi bliver nødt til at finde svar på de dilemmaer, som teknologien stiller os overfor. Hvordan sikrer vi en ansvarlig brug af kunstig intelligens? Hvordan beskytter vi vores data og personlige oplysninger? Hvordan holder vi fast i en åben demokratisk debat? Jeg siger ikke, vi skal stoppe den teknologiske udvikling, men vi skal sikre – og det er vores forpligtelse – at teknologien bliver brugt til gavn for mennesker, for samfundet og for vores demokrati. Og kære alle sammen: Vi er bagud. Og det bliver nødt til at være vores løfte, fra os voksne til jer børn og unge, at vi skal passe meget, meget bedre på jer, at vi skal regulere, og at vi skal turde at tage ansvaret på os. For et par uger siden besøgte jeg FGU ude i Hvidovre på Vestegnen. Det er et ret fantastisk uddannelsestilbud til unge, der sådan lige er ved at finde deres vej i livet. Jeg mødte selvfølgelig først og fremmest en dejlig flok unge mennesker. Nogle af dem har haft en ret svær start på livet. Mobning fortæller mange om, og ordblindhed er desværre stadig væk noget, der er udfordringer med, og der er tunge sociale problemer. Andre var skoletrætte efter mødet med en folkeskole, hvor timerne har føltes lidt for lange og bøgerne nok lidt for tykke, hvis ellers de har haft bøger – det vender jeg tilbage til. På FGU har de unge heldigvis fået gejst og mod på tilværelsen. Godt hjulpet på vej af nogle dygtige lærere og gode rammer har de taget magten over deres egen fremtid.\n",
      "\tLink: https://www.dansketaler.dk/tale/c28a68e8-caae-4e5f-b946-49a85a424610\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:\n",
      "»Det har været en ære og en udfordring at lede en bred regering i det seneste folketingsår. Vi har arbejdet hårdt for at samarbejde på tværs af partier og sikre en stærk og bæredygtig fremtid for Danmark. Vi har taget skridt til at bekæmpe klimaforandringerne og sikre et mere retfærdigt og inkluderende samfund, hvor alle borgere har lige muligheder. Vi har også arbejdet på at styrke vores sundheds- og socialsystem, så alle borgere kan få den hjælp, de har brug for. Selvom vi har oplevet udfordringer og modstand undervejs, er jeg stolt af, hvad vi har opnået sammen i det seneste folketingsår. Vi vil fortsætte med at samarbejde på tværs af partier og arbejde hårdt for at sikre en bedre fremtid for alle danskere. Tak for jeres tillid og samarbejde i det seneste folketingsår. Vi ser frem til at fortsætte dette arbejde i fremtiden.« Det, jeg lige har læst op, er ikke skrevet af mig – eller af noget andet menneske for den sags skyld. Det er skrevet af kunstig intelligens, ChatGPT. Og selv om det måske ikke rammer helt plet, hverken med ordene eller detaljerne i regeringens arbejdsprogram eller for den sags skyld antallet af punktummer, er det både fascinerende og skræmmende, hvad det er i stand til: på få sekunder at skrive en tale, løse en universitetsopgave, udarbejde en rapport med så stor overbevisning, at de færreste af os vil tro, at det var en robot og ikke et menneske, der stod bag. Kunstig intelligens er ikke fremtiden; den er virkeligheden. Og den vil forandre vores samfund i et omfang, som jeg slet ikke tror vi forstår endnu. Hvorfor starter jeg med det her i dag i en tale, der skal samle op på det folketingsår, som vi netop afslutter? Det gør jeg, fordi jeg i stedet for at se tilbage og vil bruge min taletid i dag på at se fremad. I dag vil jeg henvende mig til alle unge i Danmark. Det er jeres fremtid, den her tale kommer til at handle om – en fremtid, som på en lang række områder adskiller sig markant fra jeres forældres og bedsteforældres. Den kunstige intelligens, der brager frem netop nu, er bare ét eksempel på de nye livsvilkår, der gælder for jer.\n",
      "\tLink: https://www.dansketaler.dk/tale/c28a68e8-caae-4e5f-b946-49a85a424610\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:\n",
      "Secretary General. Excellencies. Ladies and gentlemen. “Anyone who isn’t profoundly shocked by quantum theory – has not understood it.” Those are the words of the Danish scientist Niels Bohr. Who laid the foundation for everything we know as quantum physics today. More than 100 years ago, Niels Bohr took the first steps – right here in Copenhagen. Back then, he knew that his discoveries would not only shock his fellow scientists. But also fundamentally change how we perceive and understand the world.\n",
      "\tLink: https://www.dansketaler.dk/tale/30de6816-731b-4a19-ab60-1ec4a933d4fa\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:\n",
      "In other words, ladies and gentlemen: A lot is at stake! And all of us gathered here today have a great responsibility on our shoulders. Quantum technology is today an inevitable part of our security framework in NATO. But as you know – better than anyone else: Quantum technology takes time, skills and funds to develop. If we want to ensure our common security in the future. And unlock the quantum potential in our societies. We have to lay the groundwork now. And we – as responsible governments – need to provide the right conditions for research and industry to prosper and grow.\n",
      "\tLink: https://www.dansketaler.dk/tale/30de6816-731b-4a19-ab60-1ec4a933d4fa\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:\n",
      "Jeg oplever, at jeres tillidsfolk – og nu er jeg selv datter af en tillidsmand, så jeg ved hvor vigtigt det er at have sådan en ved sin side igennem et helt liv. Jeg oplever, at jeres tillidsfolk, og i øvrigt også jeres medlemmer, er optaget af fremtiden, men aldrig bange for den. Det er ikke – tror jeg – mange steder i verden, at lønmodtagere er glade for robotter. Eller hvor kunstig intelligens ikke kun ses som en trussel og en udfordring. Her i Danmark, der går I til det hele med den samme tilgang, som den, der har kendetegnet Dansk Metal altid. At teknologien skal bruges til at gøre Danmark stærkere og komme danske lønmodtagere til gode. Som gør den enkelte dygtigere og hjælper os med at øge produktiviteten. Det vil for os aldrig nogensinde være et mål, at teknologi skal erstatte gode danske arbejdspladser. Men der, hvor den kan hjælpe os. Gøre livet lettere for den enkelte. Styrke vores model. Ja, der kommer vi selvfølgelig til at bruge den. For det er en af de vigtigste måder at sikre, at vi også i fremtiden har et stærkt dansk erhvervsliv med tilstrækkeligt mange gode arbejdspladser.\n",
      "\tLink: https://www.dansketaler.dk/tale/fd3fadd1-d63b-409f-b7f4-f679e6f9dc64\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvilket emne vil du høre Mette Frederiksens mening om? Skriv \\stop hvis du vil lukke programmet. \n",
      "\n",
      " Børnefamiliers rolle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tænker ...\n",
      "Henter citater...\n",
      "Prøver at formulere mig...\n",
      "Mette Frederiksen understregede betydningen af at tage sig af børnefamilier og fremhævede vigtigheden af at møde hver enkelt barn med et ord om tillid. Hun anerkendte de mange pædagoger, der arbejder med vores børn, men påpegede også den afgørende rolle for familien og samfundet i at sikre børnenes trivsel. Frederiksen talte om konkrete eksempler på forsømmelse og vold, hvilket understregede behovet for et stærkt samfund, der kan beskytte og hjælpe alle børn. Hun betragtede det som en kamp for både nuværende sikkerhed og fremtidig trivsel af danske børn.\n",
      "\n",
      "\n",
      "Link til Taler og Citater som er valgt:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:\n",
      "Men lige bag jer, der står en kæreste. En ægtefælle. En mor. En far. En bror. En ven. Et barn. Eller en lillesøster som mig selv.\n",
      "\tLink: https://www.dansketaler.dk/tale/3cf235ec-fa96-4e5f-8e4f-a66ccf5f8d2c\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:\n",
      "Det er en kamp for vores sikkerhed. Og for vores børns fremtid.\n",
      "\tLink: https://www.dansketaler.dk/tale/3cf235ec-fa96-4e5f-8e4f-a66ccf5f8d2c\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:\n",
      "Kan jeg forstå, at landets pædagoger gerne vil anerkendes for deres vigtige arbejde med vores børn? Ja.\n",
      "\tLink: https://www.dansketaler.dk/tale/665d44a1-f5ce-4f5f-b9f8-e3c35d13253a\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:\n",
      "Jeg kan stadig huske hende. En helt lille bitte pige. Ikke engang et år gammel endnu. Hun lå og sov i armene hos en af de voksne. Med sonde i næsen. Jeg spurgte selvfølgelig til, hvorfor der sad slanger i det lille ansigt. Pigen var blevet forsømt så meget, at hun havde lukket ned for sin egen sutterefleks. Det er noget af det mest instinktive, vi mennesker har. Hun var blevet fjernet fra sin mor. Udsultet. Alene. Jeg mødte pigen på et børnehjem for nogle år siden. Og jeg tænker tit på, hvordan det går hende. Har hun fået en god og kærlig plejefamilie? Spiser hun? Har hun levet på forskellige institutioner? Har hun fået den hjælp, hun skal have? Den lille pige er ikke alene. Hver gang, der er samlet 100 dejlige danske børn på en græsplæne. Så er der én af dem, der ikke kan være hjemme hos deres mor og far. De er blandt de 14.000 danske børn og unge, der er anbragt uden for hjemmet. De fleste børn på den plæne. De har det godt. Fordi deres forældre er der for dem. Og fordi vi sammen har skabt et samfund, der er så stærkt, at vi kan tage os af hinanden. Men der er stadig for mange af vores børn, der ikke har det godt nok. Kan I huske Brønderslev-sagen? Tønder-sagen? Indespærringer. Vold. Misbrug. Og hvad med dem, vi aldrig hører om? I dag vil jeg gerne tale om Danmarks børn. Dem, der har det godt. Og dem, der skal have det meget, meget bedre.\n",
      "\tLink: https://www.dansketaler.dk/tale/9c51c5ca-e866-40ec-97c0-222f7d519bb5\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:\n",
      "Vi skal møde hvert eneste barn og hver eneste ung med de vigtigste ord: Vi tror på dig.\n",
      "\tLink: https://www.dansketaler.dk/tale/665d44a1-f5ce-4f5f-b9f8-e3c35d13253a\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvilket emne vil du høre Mette Frederiksens mening om? Skriv \\stop hvis du vil lukke programmet. \n",
      "\n",
      " \\stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farvel\n"
     ]
    }
   ],
   "source": [
    "rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c15553-cb7f-482e-82ad-483a047275e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
