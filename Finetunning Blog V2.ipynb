{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ea92989-1dfb-4eaa-a445-359ee69428b8",
   "metadata": {},
   "source": [
    "# A realistic approach to finetuning in an enterprise environment\n",
    "You’d have to be living under a rock not to notice the massive advancements in the LLM (Large Language Model) space since the release of ChatGPT in 2022. These breakthroughs have driven many companies and organizations to invest heavily in pushing the space forward, resulting in a wealth of open-source models that can be leveraged to build data products.\n",
    "\n",
    "This is an excellent starting point, but many companies possess a significant amount of internal documents and other proprietary data that could greatly enhance their data products. Ideally, they would want to incorporate this data into their tools. To address this need, many providers have introduced fine-tuning services. For example, OpenAI offers fine-tuning for their GPT-4 model. This is a fantastic service that undoubtedly provides value to many clients. However, for data engineers like me who live and work in the EU, this is often not a viable option.\n",
    "\n",
    "The EU is governed by the GDPR (General Data Protection Regulation), which imposes strict rules on how data can be used. Even before GDPR, many large enterprises were reluctant to share their data with external partners, making it challenging to use external fine-tuning services without undergoing resource-intensive compliance reviews. Consequently, many organizations opt to focus on other areas instead.\n",
    "\n",
    "The goal of this blog post is to demonstrate how you can fine-tune some of these models in a restricted enterprise environment. To illustrate this, I will run everything on my personal laptop, a 2021 MacBook Pro with an M1 Max chip and 32 GB of RAM. While this is a fairly powerful machine, I believe it is realistic to expect access to equivalent resources within an enterprise setting. I’ll show you how to achieve this step-by-step.\n",
    "\n",
    "\n",
    "## The Project\n",
    "To make this post more relatable, I’ll guide you through the process of building a Retrieval-Augmented Generation (RAG) system from scratch, where we will fine-tune the embedding algorithm. While building a RAG is an interesting project in itself, my primary hope is not only to show you how to build a RAG but also to empower you to experiment with creating your own products, even when you don’t have access to unlimited resources.\n",
    "\n",
    "Before diving in, let’s take a look at what exactly we’ll be building.\n",
    "\n",
    "### Retrieval Agumented Generation (RAG)\n",
    "A RAG is a method to enhance an existing chatbot with your own data. Essentially, it involves building a layer on top of the chatbot that takes user input and enriches it with relevant context from your internal documents. (This, by the way, is a different approach to integrating your own data into a chatbot.)\n",
    "\n",
    "The RAG enriches the original user query by embedding it and searching through a database of your internal documents, finding documents that are similar to the user's query.\n",
    "\n",
    "A simplified illustration of the RAG architecture might look like this:\n",
    "\n",
    "![Illustration of RAG architecture](./images/rag_illustration.png)\n",
    "\n",
    "### Mette (Frederiksen, Danish PM) Bot\n",
    "Our RAG will be an application where you can input a topic and retrieve the Danish Prime Minister Mette Frederiksen's opinion on the subject. The RAG will be enriched with speeches given by Mette Frederiksen.\n",
    "\n",
    "The challenge is that these speeches are mostly in Danish. Since Danish is a relatively small language (approximately 6 million native speakers), it can be difficult to find a highly effective open-source embedding model. Even when choosing a multilingual model, danish typically represents only a small fraction of the training data.\n",
    "\n",
    "Therefore, our approach will be to download a small multilingual embedding model and improve it for our specific data through fine-tuning.\n",
    "\n",
    "\n",
    "## Fintunning Pipeline\n",
    "Now that we have some background on what I'm trying to achieve lets look at the steps we need to take. \n",
    "\n",
    "1. Data Retrieval (We will not go through this, because every case is different)\n",
    "2. Data Preparation\n",
    "3. Establish Baseline for non Finetuned Model\n",
    "4. Finetuning\n",
    "5. Test to see if the model performance has improved\n",
    "6. Sample code for simple RAG\n",
    "\n",
    "As shown I will not go through the data retrieval part. I will skip this part, because the recommended approach varies depending on your data source. You can find my data in the folder data/taler\n",
    "\n",
    "To avoid having to import libraries all over the place lets import the needed dependencies here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "125960c4-4081-4bb7-bcfe-006ea70e7b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "import ollama\n",
    "import uuid\n",
    "import json\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import psycopg as pg\n",
    "from ollama import chat, ChatResponse\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from peft import LoraConfig, TaskType\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e19dd5-092a-4dc0-b1ec-a203415fdf0d",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "One of the most critical steps when fine-tuning a model is preparing your data properly. Before diving in, it's important to consider the specific goal of your application.\n",
    "\n",
    "For example, imagine you are building an application where users can input a list of symptoms and receive the 10 most likely diagnoses. In this scenario, you need a model that can match the embedding of the symptom list with the embeddings of the diagnoses. Conversely, if your goal were to retrieve applicable laws based on a user's question, you would need to train the model to match questions to relevant legal texts.\n",
    "\n",
    "In this example, our objective is to create a model that can take a subject (e.g., Crime, Unemployment, Climate Change) and match it with public speeches made by the Danish Prime Minister, Mette Frederiksen.\n",
    "\n",
    "I have a folder containing 153 speeches by the Prime Minister. However, because these speeches cover a wide range of subjects, I need to break them into smaller, more manageable chunks.\n",
    "\n",
    "A simple approach would be to split each speech into chunks of a fixed number of characters (e.g., 1,000 characters) with some overlap between chunks. This naive method can work quite well in many scenarios.\n",
    "\n",
    "However, I encourage you to analyze your data for natural breaks that could serve as chunking boundaries. For instance, if you were working with legal texts, it might make sense to assign each article to its own chunk.\n",
    "\n",
    "When reviewing my data, I noticed that longer speeches often included lines like **** or ---- as separators between different sections. I can use these as delimiters when chunking the text. For speeches that don't contain these separators, I will fall back on the naive approach, but instead of chunking by characters, I'll chunk by lines to maintain more natural context within each chunk.\n",
    "\n",
    "This approach to data preparation will help ensure that our fine-tuned model produces more relevant and accurate results.\n",
    "The code I wrote to handle chunking looks like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536928a-ac92-46f2-8e95-d33ad9b37bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "files = glob.glob(\"*.txt\", root_dir=\"data/taler\") # find all files in the folder data/taler\n",
    "speech_chunks = {}\n",
    "with tqdm(total=len(files), desc=\"Processing Files\") as fpbar:\n",
    "    for file_name in files:\n",
    "        speech_name = file_name.replace(\".txt\", \"\")\n",
    "        with open(f\"data/taler/{file_name}\", \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            total += len(lines)\n",
    "        context_splits = []\n",
    "\n",
    "        char_split = False\n",
    "        for line in lines:\n",
    "            if re.search(\"[a-zA-Z]\", line) is None and \"\\n\" != line:\n",
    "                char_split = True\n",
    "                break\n",
    "\n",
    "        if char_split:\n",
    "            context = []\n",
    "            for line in lines:\n",
    "                if re.search(\"[a-zA-Z]\", line) is None and \"\\n\" != line:\n",
    "                    context_splits.append(\" \".join(context))\n",
    "                    context = []\n",
    "                else:\n",
    "                    context.append(line)\n",
    "            if context != []:\n",
    "                context_splits.append(\" \".join(context))\n",
    "        else:\n",
    "            chunk_size = 10  # group size\n",
    "            overlap = 2  # overlap size\n",
    "            context_splits = [\n",
    "                \" \".join(lines[i : i + chunk_size])\n",
    "                for i in range(0, len(lines), chunk_size - overlap)\n",
    "            ]\n",
    "\n",
    "        speech_chunks[speech_name] = context_splits\n",
    "        fpbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305f155-488f-4e45-9401-1ee6d0052758",
   "metadata": {},
   "source": [
    "We now have a Python object that looks something like this:\n",
    "\n",
    "```Python\n",
    "{\n",
    "    \"speech-1\": [\"chunk 1\", \"chunk 2\", \"chunk 3\"],\n",
    "    \"speech-2\": [\"chunk 1\", \"chunk 2\", \"chunk 3\"],\n",
    "    ...\n",
    "    \"speech-n\": [\"chunk 1\", \"chunk 2\", \"chunk 3\"],\n",
    "}\n",
    "```\n",
    "\n",
    "It might be tempting to think we can simply feed this data into a \"magic AI model box\" and instantly get a neat embedding algorithm. Unfortunately, it’s not that straightforward.\n",
    "\n",
    "As mentioned earlier, our goal is to create a model where a user can input a subject and retrieve a relevant chunk. To achieve this, we need not only the chunks themselves but also associated user inputs—in this case, the corresponding subjects. Since we don’t have predefined user inputs, we need to generate them.\n",
    "\n",
    "One approach would be to manually annotate each chunk with subjects, which would undoubtedly yield the best results. However, this method would be far too time-consuming. Instead, we’ll generate synthetic data using an LLM (Large Language Model). For this task, I’ll use Microsoft’s phi4 model, running locally on my machine through [ollama](https://ollama.com) (If for some reason you are not allowed to download ollama, you can use the transformer library to create the same functionality in python, read more here: [Transformer Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)). I've chosen this model because it offers a good balance between computational efficiency and output quality. However, I encourage you to experiment with different models to find what works best for your scenario.\n",
    "\n",
    "The method for generating this synthetic data involves feeding each chunk of text into the model and asking it to generate relevant subjects.\n",
    "\n",
    "Below is the prompt I use. The ## part is not part of the actual prompt but rather an English translation for non-Danish speakers:\n",
    "\n",
    "```\n",
    "Kontekst er nedenfor. ## Context below\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Givet den givne kontekst og ingen anden viden. ## Given the context and no other knowledge\n",
    "Genere op til 5 emner som kan beskrive konteksten,. ## Generate up to 5 subjects which can describe the context\n",
    "Hvis der ikke er emner som let kan beskrive konteksten, besvar med <|NAN|> ## If there are no subjects which easily describe the context reply with <|NAN|>\n",
    "\n",
    "Du må kun svarer med emnerne formattet skal være: Emne 1|Emne 2|...|Emne n| ## You are only allowed to reply with subjects in the following format: Subject 1| Subject 2|...|Subject n|\n",
    "```\n",
    "\n",
    "Let's try this approach with a toy example to see how it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b4464-f82a-4095-addb-4be2fa3494a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\\\n",
    "Kontekst er nedenfor.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Givet den givne kontekst og ingen anden viden.\n",
    "Genere op til 5 emner som kan beskrive konteksten,. \n",
    "Hvis der ikke er emner som let kan beskrive konteksten, besvar med <|NAN|>\n",
    "\n",
    "Du må kun svarer med emnerne formattet skal være: Emne 1|Emne 2|...|Emne n|\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143fb02-4ec2-449f-b4bf-db6ae4544382",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = speech_chunks[\n",
    "    \"mette-frederiksens-aabningstale-ved-folketingets-aabningsdebat\"\n",
    "][1]\n",
    "prompt = PROMPT_TEMPLATE.format(context_str=context)\n",
    "res = ollama.chat(\"phi4\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(f\"Response from phi4\\n--------------------------------------------------------------\\n{res.message.content}\\n--------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e321fd0-760e-4465-a963-6807bd90bb38",
   "metadata": {},
   "source": [
    "As you can see we get subjects back in the way we want, and can now start generating data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa85b8-ab7b-4789-95ee-299fbc1d29e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"speech\": {},\n",
    "    \"queries\": {},\n",
    "    \"corpus\": {},\n",
    "    \"relevant_docs\": {},\n",
    "    \"related_speech\": {},\n",
    "}\n",
    "\n",
    "## This part is just to be able to track how far we are\n",
    "total = 0\n",
    "for speech, chunks in speech_chunks.items():\n",
    "    total += len(chunks)\n",
    "\n",
    "# TODO Batching \n",
    "with tqdm(total=total, desc=\"Generating Queries\") as pbar:\n",
    "    for speech, chunks in speech_chunks.items():\n",
    "        speech_id = str(uuid.uuid4())\n",
    "        dataset[\"speech\"][speech_id] = speech\n",
    "        for chunk in chunks:\n",
    "            content_id = str(uuid.uuid4())\n",
    "            dataset[\"corpus\"][content_id] = chunk\n",
    "            dataset[\"related_speech\"][content_id] = speech_id\n",
    "            prompt = PROMPT_TEMPLATE.format(context_str=chunk)\n",
    "            res = ollama.chat(\"phi4\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "            reply = res.message.content\n",
    "            if \"<|NAN|>\" in reply:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            for query in reply.split(\"|\"):\n",
    "                query_id = str(uuid.uuid4())\n",
    "                dataset[\"queries\"][query_id] = query\n",
    "                dataset[\"relevant_docs\"][query_id] = [content_id]\n",
    "            pbar.update(1)\n",
    "\n",
    "with open(\"data/base_data.json\", \"w\") as f:\n",
    "    f.write(json.dumps(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12cb6c2-40cf-4f15-88a0-a6ffd3ca1e82",
   "metadata": {},
   "source": [
    "This process will take a while. On my MacBook Pro, it takes approximately 2.5 hours. However, if you have access to a machine with a more powerful GPU, such as an NVIDIA A100, you can significantly speed this up. Alternatively, you can use the pre-generated data, which is available in data/base_data.json.\n",
    "\n",
    "The current code uses a \"relevant document\" setup. In this approach, each query (subject) and chunk is assigned a unique UUID, with a separate structure that maps each query to its relevant subjects.\n",
    "\n",
    "Before moving forward, we need to address a couple of things:\n",
    "\n",
    "1. **Remove Duplicate Subjects:** It’s possible that the same subject appears multiple times, so we need to clean this up.\n",
    "2. **Organize Data for Model Training:** The data needs to be structured appropriately for training our model.\n",
    "\n",
    "Additionally, I’ve added a related speech object to help us look up which speech a particular chunk originates from. This will prove useful later on. However, the data is currently not in the best shape, so let's do some cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d188d69-f89c-43e1-b2ea-9b836d8b2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset\n",
    "\n",
    "# Strips away redundant whitespace from the subjects and \n",
    "# removes subjects which are note actual subjects\n",
    "queries = defaultdict(list)\n",
    "for key, value in data[\"queries\"].items():\n",
    "    if value == \"\" or \"--\" in value:\n",
    "        continue\n",
    "    queries[data[\"queries\"][key].strip()].append(key)\n",
    "\n",
    "# We go through the list chunks, and remove parts that does\n",
    "# That does not include any information. This part is very \n",
    "# dependent on your data. So look through and implement the\n",
    "# rules that makes sense for you\n",
    "\n",
    "corpus = defaultdict(list)\n",
    "pop_keys = []\n",
    "for key, value in data[\"corpus\"].items():\n",
    "    value = \" \".join(value.replace(\"Tale\\n \\n \\n \\n \\n \\n \\n \", \"\").split())\n",
    "    if value.isspace() or len(value) <= 60:\n",
    "        pop_keys.append(key)\n",
    "    data[\"corpus\"][key] = value\n",
    "    corpus[value].append(key)\n",
    "\n",
    "for key in pop_keys:\n",
    "    data[\"corpus\"].pop(key)\n",
    "    data[\"related_speech\"].pop(key)\n",
    "\n",
    "\n",
    "# We create a new query structure, which removes duplicate queries\n",
    "# and remap the relevant documents to the new query ids\n",
    "relevant_docs = defaultdict(list)\n",
    "new_queries = {}\n",
    "for query in queries.keys():\n",
    "    id = str(uuid.uuid4())\n",
    "    new_queries[id] = query\n",
    "    query_ids = queries[query]\n",
    "    for query_id in query_ids:\n",
    "        for doc_id in data[\"relevant_docs\"][query_id]:\n",
    "            if doc_id not in pop_keys:\n",
    "                relevant_docs[id].append(doc_id)\n",
    "\n",
    "clean_data = {}\n",
    "clean_data[\"query\"] = new_queries\n",
    "clean_data[\"relevant_docs\"] = relevant_docs\n",
    "clean_data[\"related_speech\"] = data[\"related_speech\"]\n",
    "clean_data[\"corpus\"] = data[\"corpus\"]\n",
    "\n",
    "with open(\"data/cleaned_base_data.json\", \"w\") as out:\n",
    "    json.dump(clean_data, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a81dd35-3604-42dc-8ab5-9727dad183c3",
   "metadata": {},
   "source": [
    "Once we have cleaned the data, the next step is to determine how we want to train our model. A crucial part of this process involves selecting an appropriate loss function. The loss function is responsible for penalizing the model when it makes incorrect predictions and rewarding it when it makes correct predictions. It essentially guides the model toward better performance during training.\n",
    "\n",
    "For the type of fine-tuning we are aiming for, it is common to organize the data into triplets. The triplet data format consists of:\n",
    "\n",
    "- **Anchor:** The subject or query.\n",
    "- **Positive:** A piece of context related to the anchor.\n",
    "- **Negative:** A piece of context unrelated to the anchor.\n",
    "  \n",
    "An example of this format might look like:\n",
    "\n",
    "- **Anchor:** Defense Spending\n",
    "- **Positive:** \"The new aircraft carrier is over budget but will bring much-needed capabilities.\"\n",
    "- **Negative:** \"It's cold today, but the sun is out, making it nice if you wear the right layers.\"\n",
    "  \n",
    "However, a keen observer might notice that our dataset currently contains only positives. Therefore, we need to generate negatives to complete our triplets.\n",
    "\n",
    "There are several ways to achieve this:\n",
    "\n",
    "- **Generate Negatives with an LLM:** Similar to how we generated subjects earlier, we could use a large language model to create negative samples.\n",
    "- **Use an Unrelated Text Corpus:** Another approach is to introduce entirely unrelated text as negatives. For example, if you are working with clinical records, you could download annual reports from Goldman Sachs and use that text as your negative data.\n",
    "  \n",
    "For this project, I opted for the second approach. I downloaded a dataset containing customer reviews and randomly selected reviews as negative samples.\n",
    "\n",
    "Here's the code snippet demonstrating this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "388dd815-c8cf-4f4e-bbba-d2378aa05d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'relevant_docs', 'corpus'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08068f51-a842-4948-9d82-2421f3bc14df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Queries:   1%|██                                                                                                                                                                 | 1849/150319 [00:44<59:54, 41.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding Queries\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m uid, query \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 11\u001b[0m         data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedded_queries\u001b[39m\u001b[38;5;124m'\u001b[39m][uid] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding Documents\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:        \n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:623\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 623\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    625\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:690\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    689\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:442\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    437\u001b[0m     key: value\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    440\u001b[0m }\n\u001b[0;32m--> 442\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:977\u001b[0m, in \u001b[0;36mXLMRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    975\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 977\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    990\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:632\u001b[0m, in \u001b[0;36mXLMRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    621\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    622\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    623\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    629\u001b[0m         output_attentions,\n\u001b[1;32m    630\u001b[0m     )\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:563\u001b[0m, in \u001b[0;36mXLMRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    560\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    561\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 563\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/transformers/pytorch_utils.py:260\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:575\u001b[0m, in \u001b[0;36mXLMRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 575\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:473\u001b[0m, in \u001b[0;36mXLMRobertaIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 473\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"data/cleaned_base_data.json\", \"r\") as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "data['embedded_queries'] = {}\n",
    "data['embedded_corpus'] = {}\n",
    "with tqdm(total=len(data[\"query\"].items()), desc=\"Embedding Queries\") as pbar:\n",
    "    for uid, query in data[\"query\"].items():\n",
    "        data['embedded_queries'][uid] = model.encode(query)\n",
    "        pbar.update(1)\n",
    "        \n",
    "with tqdm(total=len(data[\"query\"].items()), desc=\"Embedding Documents\") as pbar:        \n",
    "    for uid, doc in data[\"corpus\"].items():\n",
    "        data['embedded_corpus'][uid] = model.encode(doc)\n",
    "        pbar.update(1)\n",
    "\n",
    "max_sim = 0.0\n",
    "min_sim = 1.0\n",
    "with tqdm(total=(len(data[\"embedded_queries\"].items()) * len(data[\"embedded_corpus\"].items())), desc=\"calculating min/max similarities\") as pbar:\n",
    "    for query_id, query_emb in data['embedded_queries'].items():\n",
    "        for doc_id, doc_emb in data['embedded_corpus'].items():\n",
    "            cos_sim = dot(query_emb, doc_emb) / (norm(query_emb) * norm(doc_emb))\n",
    "            if cos_sim > max_sim:\n",
    "                max_sim = cos_sim\n",
    "            if cos_sim < min_sim:\n",
    "                min_sim = cos_sim\n",
    "            pbar.update(1)\n",
    "\n",
    "print(f\"Max: {max_sim}\")\n",
    "print(f\"Min: {min_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec28619c-fe00-4baa-832a-4c8db86b7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/cleaned_base_data.json\", \"r\") as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "negatives: pd.DataFrame = pd.read_csv(\"data/negatives.csv\")[\"review_text\"]\n",
    "\n",
    "total = 0\n",
    "for id, relevant_docs in data[\"relevant_docs\"].items():\n",
    "    total += len(relevant_docs)\n",
    "\n",
    "triplets = []\n",
    "with tqdm(total=total, desc=\"creating positive negative pairs\") as pbar:\n",
    "    for query_id, doc_ids in data[\"relevant_docs\"].items():\n",
    "        anchor = data[\"query\"][query_id]\n",
    "        for id in doc_ids:\n",
    "            triplets.append(\n",
    "                {\n",
    "                    \"anchor\": anchor,\n",
    "                    \"positive\": data[\"corpus\"][id],\n",
    "                    \"negative\": negatives.sample().values[0],\n",
    "                }\n",
    "            )\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e359a-ada9-4e37-956a-f04a0a53fbac",
   "metadata": {},
   "source": [
    "Now that we have our triplet data, the last thing is to split our data into Train, Test and Validation.\n",
    "We use Train data to finetune our model, we use test data to estimate the performance of the model during training, and validition to ensure that we did not just find a model that fit our test data well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc5a38-f5da-4d2f-97fc-53679ad89449",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplet, val_triplet = train_test_split(pd.DataFrame(triplets), test_size=0.2)\n",
    "train_triplet, test_triplet = train_test_split(\n",
    "    pd.DataFrame(train_triplet), test_size=0.2\n",
    ")\n",
    "\n",
    "train_triplet.to_json(\"data/triplet_data_train.json\")\n",
    "test_triplet.to_json(\"data/triplet_data_test.json\")\n",
    "val_triplet.to_json(\"data/triplet_data_val.json\")\n",
    "\n",
    "dataset: DatasetDict = {\n",
    "    \"train\": Dataset.from_pandas(train_triplet, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(test_triplet, preserve_index=False),\n",
    "    \"validation\": Dataset.from_pandas(val_triplet, preserve_index=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dce45a-b4d9-46cc-86ef-d199e1e08636",
   "metadata": {},
   "source": [
    "### Baseline Model and Valuation Strategy\n",
    "\n",
    "Our data is now ready, but before we start finetuning our model, we should consider how we are going to decide whether or not the finetuned model is better than the baseline one. \n",
    "\n",
    "To do this we have to do the following steps:\n",
    "1. Download a Baseline model\n",
    "2. Find a valuation method\n",
    "3. Valuate the Baseline model\n",
    "4. Finetune the model\n",
    "5. Valuate the finetuned model\n",
    "\n",
    "The first thing we will do is to download the baseline model from HuggingFace, using the SentenceTransformer library. I have chosen the [multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) model. I have chosen this model because it has a good baseline performance, and is small enough to train on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503dae50-587a-4fe9-9585-45b00ad99927",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Test the model\n",
    "emb = model.encode(\"Hello World\")\n",
    "print(\"Vector Size: \", len(emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35baf91c-f36a-454d-8343-b30188a541e4",
   "metadata": {},
   "source": [
    "Now that we've downloaded our baseline model, let's take a moment to think about how to best evaluate its performance.\n",
    "\n",
    "Now that we've downloaded our baseline model, let's spend some time thinking about how to best evaluate our model.\n",
    "\n",
    "A good place to start is here: [SentenceTransformer Evaluator Classes](https://sbert.net/docs/sentence_transformer/training_overview.html#evaluator).\n",
    "\n",
    "Going through the list, there are several that are interesting, but the one that fits our data best is the [Triplet Evaluator](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.TripletEvaluator).\n",
    "\n",
    "What the Triplet Evaluator does is compare the similarity between the anchor and the positive, as well as between the anchor and the negative. It then returns a percentage representing the proportion of records where the anchor embedding was closer to the positive embedding than to the negative. So, a result of 0.8 means that in 80% of cases, the anchor was closer to the positive than to the negative.\n",
    "\n",
    "We can run the model on our data using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b8f12-8632-4f4d-85d6-23eae4e78920",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_evaluator = TripletEvaluator(\n",
    "    anchors=dataset[\"test\"][\"anchor\"],\n",
    "    positives=dataset[\"test\"][\"positive\"],\n",
    "    negatives=dataset[\"test\"][\"negative\"],\n",
    "    name=\"dev_evaluator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274b8379-29c0-4c5f-a8dd-9ee24ec61a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_evaluator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba2997-0549-4d03-a975-2959bd6a93df",
   "metadata": {},
   "source": [
    "With our test data, we get a result of 0.91 using our baseline model. This means that in 91% of cases, the baseline model will place the anchor closer to the positive than to the negative.\n",
    "\n",
    "This is a great evaluation to start with, but in reality, this is not what we are truly interested in. We want to know whether inputting a subject will return related chunks of text. So, let's try to write a custom evaluation script. I will base my model on the Recall@K metric. The metric simply tests whether the anchor returns the positive in its top K results.\n",
    "\n",
    "We will modify this to check if any of the related documents are in the top K results. Additionally, we will introduce a metric to show the percentage of relevant documents present in the top K results.\n",
    "\n",
    "Before writing this test, we need to store the embeddings somewhere so we can query them. Many databases can handle this, but I've chosen to use Postgres with the PG Vector extension. Here's how to set it up in your Postgres database:\n",
    "\n",
    "First, install the extension by running the following command:\n",
    "```sql\n",
    "CREATE EXTENSION vector;\n",
    "```\n",
    "Then, we will create two tables to store our embeddings: one for our baseline embeddings and another for the fine-tuned embeddings:\n",
    "```sql\n",
    "CREATE TABLE embeddings_base (id bigserial PRIMARY KEY, speech VARCHAR, context_id VARCHAR, context VARCHAR, embedding vector(384))\n",
    "CREATE TABLE embeddings (id bigserial PRIMARY KEY, speech VARCHAR, context_id VARCHAR, context VARCHAR, embedding vector(384))\n",
    "```\n",
    "You might wonder about the vector(384) datatype. This simply means we are storing a vector of size 384. If you are unsure about the size of your embedding, you can find out by running:\n",
    "```python\n",
    "len(model.encode(\"Hello\"))\n",
    "```\n",
    "Now that our table is ready for embeddings, we can load them. Let's do that using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f5d21-6761-4b73-905b-5a5c33321797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(speech: str, context_id: str, context: str, embedding: List, write_to_base: bool) -> None:\n",
    "    conn = pg.connect(\"dbname=vector_rag user=postgres password=postgres\")\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    if write_to_base:\n",
    "        cur.execute(\n",
    "            \"INSERT INTO embeddings_base (speech, context_id, context, embedding) VALUES (%s, %s, %s, %s)\",\n",
    "            (speech, context_id, context, str(embedding)),\n",
    "        )\n",
    "    else:\n",
    "        cur.execute(\n",
    "            \"INSERT INTO embeddings (speech, context_id, context, embedding) VALUES (%s, %s, %s, %s)\",\n",
    "            (speech, context_id, context, str(embedding)),\n",
    "        )\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc690ec7-5dce-4f8d-8dfc-191ae7744681",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(data[\"corpus\"].keys()), desc=\"Saving embeddings\") as pbar:\n",
    "    for id, context in data[\"corpus\"].items():\n",
    "        speech_name = data[\"related_speech\"][id]\n",
    "        embedding = model.encode(context).tolist()\n",
    "        write_to_db(speech_name, id, context, embedding, True)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d029d7d-6017-4e98-b917-a6605b763803",
   "metadata": {},
   "source": [
    "Then we can implement our recall k methods and test our baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69072b26-c9bd-4c83-b335-502f6fdbe58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lookup to make checks faster\n",
    "data[\"query_lk\"] = {}\n",
    "for key, value in data[\"query\"].items():\n",
    "    data[\"query_lk\"][value] = key\n",
    "\n",
    "def recall_k(query: str, model: SentenceTransformer, k: int, data: dict, check_base: bool) -> float:\n",
    "    query_id = data[\"query_lk\"][query]\n",
    "    expected_ids = data[\"relevant_docs\"][query_id]\n",
    "    embedded_query = model.encode(query).tolist()\n",
    "\n",
    "    conn = pg.connect(\"dbname=vector_rag user=postgres password=postgres\")\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    if check_base:\n",
    "        cur.execute(\n",
    "            \"SELECT context_id FROM embeddings_base ORDER BY embedding <=> %s::vector LIMIT %s;\",\n",
    "            (str(embedded_query), str(k)),\n",
    "        )\n",
    "    else: \n",
    "        cur.execute(\n",
    "            \"SELECT context_id FROM embeddings ORDER BY embedding <=> %s::vector LIMIT %s;\",\n",
    "            (str(embedded_query), str(k)),\n",
    "        )\n",
    "        \n",
    "    results = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    min_res = min(len(expected_ids), k)\n",
    "    result_per = len(set(results) & set(expected_ids)) / min_res\n",
    "    \n",
    "    result_k = 1.0 if set(results) & set(expected_ids) else 0.0\n",
    "\n",
    "    return result_k, result_per\n",
    "\n",
    "\n",
    "recall_10 = test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 10, data, True), axis=1\n",
    "    )\n",
    "recall_10 = pd.DataFrame(recall_10.tolist(), index=recall_10.index)\n",
    "print(\"Recall@10 Metric: \", recall_10[0].sum() / test_triplet.shape[0])\n",
    "print(\"Recall@10 Metric %: \", recall_10[1].sum() / test_triplet.shape[0])\n",
    "\n",
    "\n",
    "recall_4 = test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 4, data, True), axis=1\n",
    "    )\n",
    "recall_4 = pd.DataFrame(recall_4.tolist(), index=recall_4.index)\n",
    "print(\"Recall@4 Metric: \", recall_4[0].sum() / test_triplet.shape[0])\n",
    "print(\"Recall@4 Metric %: \", recall_4[1].sum() / test_triplet.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df99a0-9d78-46fc-808c-6395470139a8",
   "metadata": {},
   "source": [
    "With our baseline, we can now try to fine-tune our model to see if we can improve its performance.\n",
    "\n",
    "### Fine-Tuning with Limited Compute and Memory\n",
    "Although we've chosen a very small model, my laptop does not have enough memory to train it locally. This leaves me with two options:\n",
    "\n",
    "Rent a Bigger Machine: This would certainly make the process easier, but I've always found it more interesting to work within limitations.\n",
    "Work Within Constraints: I've decided to take the more challenging route and explore ways to fine-tune the model using limited resources.\n",
    "One promising solution is to perform fine-tuning using [Low Rank Adaptation (LoRA)](https://arxiv.org/pdf/2106.09685). \n",
    "\n",
    "When using LoRA, we freeze all the parameters in the baseline model and train a smaller adaptation layer, which is then multiplied onto the original weights. This significantly reduces the number of parameters we need to train. According to the original paper, this approach can reduce memory requirements threefold.\n",
    "![LowRank Adaptation](images/LoRA.png)\n",
    "\n",
    "Let's see the difference in trainable parameters before and after applying the LoRA adapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803f126-ef4d-4b65-86f5-dce3ee669d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = 0\n",
    "all_params = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    all_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "        # print(f\"{name}: shape={param.shape}, params={param.numel()}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total model parameters:    {all_params:,}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Total trainable parameters before LoRA: {trainable_params:,}\")\n",
    "print(f\"Trainable Percentage trainable before LoRA:     {100 * trainable_params / all_params:.2f}%\")\n",
    "print(\"-\" * 70)\n",
    "## Adding LoRA Adapter\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model.add_adapter(peft_config)\n",
    "\n",
    "trainable_params_lora = 0\n",
    "for name, param in model.named_parameters():\n",
    "    all_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params_lora += param.numel()\n",
    "print(f\"Total trainable parameters after LoRA: {trainable_params_lora:,}\")\n",
    "print(f\"Trainable Percentage trainable after LoRA:     {100 * trainable_params_lora / all_params:.2f}%\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1d55f9-163e-4e1c-8fbd-11b0e2fb1f0b",
   "metadata": {},
   "source": [
    "As you can see, we reduce the trainable parameters by 99.72%, which is quite significant and ensures that this can run on my Mac. Now, let's get to the training.\n",
    "\n",
    "First, we need to decide on a loss function. To do that, I referred to the [Sentence Transformer Loss Overview](https://sbert.net/docs/sentence_transformer/loss_overview.html), and found that MultipleNegativesRankingLoss fits my data well.\n",
    "\n",
    "Second, we need to set some hyperparameters in our training arguments. There is no one-size-fits-all solution when choosing hyperparameters. From what I've read, the best approach is to experiment. Below are the hyperparameters I've chosen. I’d like to point out that the low batch size of 8 is primarily due to my memory limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d29b0-bf5e-4df4-8206-6a879b02e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"../models/multilingual-e5-small-finetune-danish-subject\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    bf16=True,  \n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch according to the documentation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=25,\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3803d63b-01f7-4222-bcaa-15c126013bc6",
   "metadata": {},
   "source": [
    "Now that we have a trained model, and before we test it, I would like to share a bit about what I look for in the logging. First, I check to see if there is a steadily decreasing validation loss throughout the training process.\n",
    "\n",
    "What can sometimes happen is that while the training loss continues to decrease, the validation loss stops improving. This is usually a sign of overfitting the model.\n",
    "\n",
    "Second, I notice that even though my training loss is generally decreasing, there are occasional spikes. This is probably due to the small batch size. Unfortunately, I cannot change this because of memory limitations.\n",
    "\n",
    "Now, we can run our test again to see if performance has improved. To do this, we need to re-embed our corpus using the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803239e9-7e38-4255-8c1d-7752d5e48ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(data[\"corpus\"].keys()), desc=\"Saving embeddings\") as pbar:\n",
    "    for id, context in data[\"corpus\"].items():\n",
    "        speech_name = data[\"related_speech\"][id]\n",
    "        embedding = model.encode(context).tolist()\n",
    "        write_to_db(speech_name, id, context, embedding, False)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d26a4b-31f5-4230-9e40-1c221af8083b",
   "metadata": {},
   "source": [
    "And finally we can test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6916e302-4054-4400-b26a-5b2fe0914582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recall_10 = test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 10, data, False), axis=1\n",
    "    )\n",
    "recall_10 = pd.DataFrame(recall_10.tolist(), index=recall_10.index)\n",
    "print(\"Recall@10 Metric: \", recall_10[0].sum() / test_triplet.shape[0])\n",
    "print(\"Recall@10 Metric %: \", recall_10[1].sum() / test_triplet.shape[0])\n",
    "\n",
    "\n",
    "recall_4 = test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 4, data, False), axis=1\n",
    "    )\n",
    "recall_4 = pd.DataFrame(recall_4.tolist(), index=recall_4.index)\n",
    "print(\"Recall@4 Metric: \", recall_4[0].sum() / test_triplet.shape[0])\n",
    "print(\"Recall@4 Metric %: \", recall_4[1].sum() / test_triplet.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da67cc5-bd32-40c1-b430-e7ee1b68b869",
   "metadata": {},
   "source": [
    "GREAT SUCCES!!!!! It looks like we've improved our model on all metrics. We did not have to send our data anywhere :D \n",
    "\n",
    "Before we finish for today, I would like to add a few words to the recall metrics. To b honest the result does not look that great.\n",
    "The reason is that, we are looking for very specific values. But when we look at the actual data what we see is the queries finding \n",
    "relevant text. Just not the specific documents we were looking for. The reason is that when our model generated subjects. It could generate\n",
    "multiple subjects which are very close in meaning to eachother. \n",
    "\n",
    "For instance we have 143 subjects with the word *pandemic* (pandemi in danish), this means that when testing for one of the pandemic subjects, \n",
    "we might get a relevant chunk from a different pandemic subject.\n",
    "\n",
    "I Would like for you to think at these measures as a way to compare the model performance before and after finetuning, and not as an absolute truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966a83e-77c3-4d34-8919-745c5ce575d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([subject for subject in data[\"query_lk\"].keys() if \"pandemi\" in subject.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410fd7f-f8e6-4c36-af1c-9b601e08ef59",
   "metadata": {},
   "source": [
    "Now lets built a simple RAG application. Having the model this does not take a lot of code, which can be found below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52999739-a9ef-412f-81e9-82eef582cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "METTE_PROMPT_TEMPLATE = \"\"\"\\\n",
    "Du er en LLM som giver svarer på hvad Mette Frederiksen syntes om {question}. \\\n",
    "Du må kun besvarer baseret af de nedenstående citater, Du skal kun komme med et enkelt svar som er på Dansk \\\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\"\"\"\n",
    "\n",
    "def rag():\n",
    "    while True:\n",
    "        ### First we ask the user for input\n",
    "        query = input(\"Hvilket emne vil du høre Mette Frederiksens mening om? Skriv \\\\stop hvis du vil lukke programmet. \\n\\n\")\n",
    "\n",
    "        ### Check if user wants to exit\n",
    "        if query == \"\\\\stop\":\n",
    "            print(\"Farvel\")\n",
    "            break\n",
    "            \n",
    "        print(\"Tænker ...\")\n",
    "        emb = model.encode(query).tolist() ### Encode input\n",
    "        \n",
    "        print(\"Henter citater...\")\n",
    "        ### Query Database for relevant context\n",
    "        conn = pg.connect(\"dbname=vector_rag user=postgres password=postgres\")\n",
    "        conn.autocommit = True\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        cur.execute(\n",
    "            \"SELECT speech, context FROM embeddings ORDER BY embedding <-> %s::vector LIMIT 5;\",\n",
    "            (str(emb),),\n",
    "        )\n",
    "        \n",
    "        result = cur.fetchall()\n",
    "        \n",
    "        context_all = []\n",
    "        speeches =  []\n",
    "        for row in result:\n",
    "            speech = row[0]\n",
    "            speeches.append(speech)\n",
    "            context_all.append(f\"'{row[1]}'\")\n",
    "\n",
    "        ### Generate Prompt\n",
    "        prompt = METTE_PROMPT_TEMPLATE.format(context_str=\"\\n\\n\".join(list(set(context_all))), question=query)\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"Prøver at formulere mig...\")\n",
    "        \n",
    "        res: ChatResponse = chat(model='phi4', messages=[\n",
    "          {\n",
    "            'role': 'user',\n",
    "            'content': prompt,\n",
    "          },\n",
    "        ])\n",
    "        print(res.message.content)\n",
    "        print(\"\\n\\nLink til Taler og Citater som er valgt:\")\n",
    "        for row in result:\n",
    "            print(\"-\" * 100)\n",
    "            print(f\"Context:\\n{row[1]}\")\n",
    "            print(f\"\\nLink: https://www.dansketaler.dk/tale/{row[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da060a7-4f24-4094-b8c7-a44574c398cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c15553-cb7f-482e-82ad-483a047275e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
