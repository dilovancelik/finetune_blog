{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "634da49b-3024-4772-a709-0387dcca5b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW, Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR, LRScheduler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c74ccc-67b2-4efd-98d8-395da0268107",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "model = SentenceTransformer(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32986693-ba07-4297-800b-0234eb9475a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAdapter(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        # Create a linear layer of size input_dim in both ends. This will match our original embedding \n",
    "        self.linear: nn.Linear = nn.Linear(input_dim, input_dim) \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c24a70ff-913c-4397-93df-e3d390bc8956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, base_model: SentenceTransformer):\n",
    "        self.data = data\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        query = item['anchor']\n",
    "        positive = item['positive']\n",
    "        negative = item['negative']\n",
    "        \n",
    "        query_emb = self.base_model.encode(query, convert_to_tensor=True)\n",
    "        positive_emb = self.base_model.encode(positive, convert_to_tensor=True)\n",
    "        negative_emb = self.base_model.encode(negative, convert_to_tensor=True)\n",
    "        \n",
    "        return query_emb, positive_emb, negative_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f165ca83-59ee-4a4f-b555-290986c69d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json(\"data/triplet_data_train.json\")\n",
    "df_test = pd.read_json(\"data/triplet_data_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7bac18bc-0ac8-4e99-8000-784c272b1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TripletDataset(df_train, model)\n",
    "dataset_test = TripletDataset(df_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30e6cec5-8143-4a7f-939b-5472c7bac4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_schedule_with_warmup(optimizer: Optimizer, num_warmup_steps: int, num_training_steps: int) -> LRScheduler:\n",
    "    def lr_lambda(current_step: int) -> float:\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
    "    return LambdaLR(optimizer, lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c4b4ca43-a91f-433b-b986-0bb5a724236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    base_model: SentenceTransformer,\n",
    "    dataset_train: TripletDataset,\n",
    "    dataset_test: TripletDataset,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    warmup_steps: int,\n",
    "    max_grad_norm: float,\n",
    "    margin: int,\n",
    "    save_every_epoch: int\n",
    ") -> LinearAdapter:\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    \n",
    "    adapter = LinearAdapter(base_model.get_sentence_embedding_dimension()).to(device)\n",
    "\n",
    "    triplet_loss = nn.TripletMarginLoss()\n",
    "    optimizer = AdamW(adapter.parameters(), lr=learning_rate)\n",
    "    \n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    total_steps = len(dataloader_train) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "    \n",
    "    with tqdm(total=epochs, desc=\"Training\") as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            total_loss_train = 0\n",
    "            total_loss_test = 0\n",
    "            \n",
    "            for batch in dataloader_train:\n",
    "                query_emb, positive_emb, negative_emb = [x.to(device) for x in batch]\n",
    "                \n",
    "                # Forward pass\n",
    "                adapted_query_emb = adapter(query_emb)\n",
    "                \n",
    "                # Compute loss\n",
    "                train_loss = triplet_loss(adapted_query_emb, positive_emb, negative_emb)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                clip_grad_norm_(adapter.parameters(), max_grad_norm)\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                total_loss_train += train_loss.item()\n",
    "                \n",
    "            for batch in dataloader_train:\n",
    "                query_emb, positive_emb, negative_emb = [x.to(device) for x in batch]\n",
    "                \n",
    "                # Forward pass\n",
    "                adapted_query_emb = adapter(query_emb)\n",
    "                \n",
    "                # Compute loss\n",
    "                test_loss = triplet_loss(adapted_query_emb, positive_emb, negative_emb)\n",
    "                \n",
    "                total_loss_test += test_loss.item()\n",
    "            pbar.update(1)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {total_loss_train/len(dataloader_train):.4f}, Test Loss: {total_loss_test/len(dataloader_test):.4f}\")\n",
    "            if (epoch + 1) % save_every_epoch == 0:\n",
    "                save_dict = {\n",
    "                    'adapter_state_dict': adapter.state_dict(),\n",
    "                    'adapter_kwargs': {\n",
    "                        'epochs': (epoch + 1),\n",
    "                        'batch_size': batch_size,\n",
    "                        'learning_rate': learning_rate,\n",
    "                        'warmup_steps': warmup_steps,\n",
    "                        'max_grad_norm': max_grad_norm,\n",
    "                        'margin': margin,\n",
    "                    }\n",
    "                }\n",
    "                torch.save(save_dict, f\"models/adapter_{epoch}.pth\")\n",
    "\n",
    "    save_dict = {\n",
    "        'adapter_state_dict': adapter.state_dict(),\n",
    "        'adapter_kwargs': {\n",
    "            'epochs': (epoch + 1),\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'warmup_steps': warmup_steps,\n",
    "            'max_grad_norm': max_grad_norm,\n",
    "            'margin': margin,\n",
    "        }\n",
    "    }\n",
    "    torch.save(save_dict, f\"models/adapter_{epoch}_final.pth\")\n",
    "    return adapter\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a031df52-f8ee-44c3-9b50-a9df1f15e5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|█▎                                                | 1/40 [04:45<3:05:27, 285.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Train Loss: 0.9687, Test Loss: 3.5269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|██▌                                               | 2/40 [12:55<4:17:01, 405.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40, Train Loss: 0.8301, Test Loss: 2.9346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|███▊                                              | 3/40 [17:32<3:34:04, 347.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40, Train Loss: 0.7361, Test Loss: 2.8066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█████                                             | 4/40 [22:08<3:11:20, 318.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40, Train Loss: 0.7209, Test Loss: 2.7830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|██████▎                                           | 5/40 [26:44<2:57:03, 303.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40, Train Loss: 0.7185, Test Loss: 2.7741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|███████▌                                          | 6/40 [31:20<2:46:40, 294.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40, Train Loss: 0.7161, Test Loss: 2.7710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|████████▊                                         | 7/40 [35:48<2:37:04, 285.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40, Train Loss: 0.7148, Test Loss: 2.7659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██████████                                        | 8/40 [40:20<2:29:58, 281.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40, Train Loss: 0.7142, Test Loss: 2.7642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|███████████▎                                      | 9/40 [44:55<2:24:21, 279.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40, Train Loss: 0.7127, Test Loss: 2.7596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|████████████▎                                    | 10/40 [49:29<2:18:50, 277.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40, Train Loss: 0.7124, Test Loss: 2.7570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|█████████████▍                                   | 11/40 [53:57<2:12:49, 274.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40, Train Loss: 0.7120, Test Loss: 2.7563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██████████████▋                                  | 12/40 [58:30<2:07:51, 273.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40, Train Loss: 0.7105, Test Loss: 2.7529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███████████████▎                               | 13/40 [1:03:03<2:03:13, 273.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40, Train Loss: 0.7105, Test Loss: 2.7502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|████████████████▍                              | 14/40 [1:07:27<1:57:25, 270.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40, Train Loss: 0.7098, Test Loss: 2.7499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|█████████████████▋                             | 15/40 [1:11:53<1:52:14, 269.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40, Train Loss: 0.7090, Test Loss: 2.7451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████████████████▊                            | 16/40 [1:16:22<1:47:45, 269.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40, Train Loss: 0.7087, Test Loss: 2.7482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|███████████████████▉                           | 17/40 [1:20:52<1:43:16, 269.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40, Train Loss: 0.7078, Test Loss: 2.7432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|█████████████████████▏                         | 18/40 [1:25:15<1:38:08, 267.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40, Train Loss: 0.7072, Test Loss: 2.7410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|██████████████████████▎                        | 19/40 [1:29:43<1:33:40, 267.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40, Train Loss: 0.7073, Test Loss: 2.7385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|███████████████████████▌                       | 20/40 [1:34:14<1:29:34, 268.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40, Train Loss: 0.7072, Test Loss: 2.7378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|████████████████████████▋                      | 21/40 [1:38:39<1:24:45, 267.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40, Train Loss: 0.7064, Test Loss: 2.7370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████████████████████████▊                     | 22/40 [1:43:05<1:20:05, 266.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40, Train Loss: 0.7068, Test Loss: 2.7357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|███████████████████████████                    | 23/40 [1:47:34<1:15:50, 267.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40, Train Loss: 0.7062, Test Loss: 2.7330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|████████████████████████████▏                  | 24/40 [1:52:04<1:11:32, 268.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40, Train Loss: 0.7054, Test Loss: 2.7347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|█████████████████████████████▍                 | 25/40 [1:56:28<1:06:43, 266.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40, Train Loss: 0.7050, Test Loss: 2.7325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████████████████████████████▌                | 26/40 [2:00:55<1:02:19, 267.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40, Train Loss: 0.7049, Test Loss: 2.7306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|█████████████████████████████████                | 27/40 [2:05:27<58:09, 268.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40, Train Loss: 0.7047, Test Loss: 2.7308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████████████████████████████████▎              | 28/40 [2:09:52<53:28, 267.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40, Train Loss: 0.7050, Test Loss: 2.7290\n"
     ]
    }
   ],
   "source": [
    "adapter_kwargs = {\n",
    "    'epochs': 40,\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 2e-4,\n",
    "    'warmup_steps': 60,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'margin': 1.0,\n",
    "    'save_every_epoch': 5\n",
    "}\n",
    "trained_adapter = train(model, dataset_train, dataset_test, **adapter_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c68c5-91f2-43ca-b64e-568e9723df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> int:\n",
    "\n",
    "    cos = torch.nn.CosineSimilarity(dim=-1)\n",
    "    return 1 if cos(anchor, positive) > cos(anchor, negative) else 0\n",
    "\n",
    "    \n",
    "for epoch in range(0, 11, 5):\n",
    "    if epoch == 0:\n",
    "        accuracy_per = df_val.apply(\n",
    "            lambda x: accuracy(\n",
    "                torch.Tensor(x[\"embedded_anchor\"]),\n",
    "                torch.Tensor(x[\"embedded_positive\"]),\n",
    "                torch.Tensor(x[\"embedded_negative\"])\n",
    "            ), axis=1).sum() / df_val.shape[0]\n",
    "    else:\n",
    "        loaded_dict = torch.load(f\"models/adapter_{epoch}.pth\")\n",
    "        adapter = LinearAdapter(model.get_sentence_embedding_dimension())  \n",
    "        adapter.load_state_dict(loaded_dict['adapter_state_dict'])\n",
    "        accuracy_per = df_val.apply(\n",
    "            lambda x: accuracy(\n",
    "                adapter(torch.Tensor(x[\"embedded_anchor\"])),\n",
    "                torch.Tensor(x[\"embedded_positive\"]),\n",
    "                torch.Tensor(x[\"embedded_negative\"])\n",
    "            ), axis=1).sum() / df_val.shape[0]\n",
    "\n",
    "    print(f\"Epoch: {epoch}, accuracy: {accuracy_per}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b41312-2526-480b-8d7b-5376e9128f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
