{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ea92989-1dfb-4eaa-a445-359ee69428b8",
   "metadata": {},
   "source": [
    "# A realistic approach to finetuning in an enterprise environment\n",
    "You’d have to be living under a rock not to notice the massive advancements in the LLM (Large Language Model) space since the release of ChatGPT in 2022. These breakthroughs have driven many companies and organizations to invest heavily in pushing the space forward, resulting in a wealth of open-source models that can be leveraged to build data products.\n",
    "\n",
    "This is an excellent starting point, but many companies possess a significant amount of internal documents and other proprietary data that could greatly enhance their data products. Ideally, they would want to incorporate this data into their tools. To address this need, many providers have introduced fine-tuning services. For example, OpenAI offers fine-tuning for their GPT-4 model. This is a fantastic service that undoubtedly provides value to many clients. However, for data engineers like me who live and work in the EU, this is often not a viable option.\n",
    "\n",
    "The EU is governed by the GDPR (General Data Protection Regulation), which imposes strict rules on how data can be used. Even before GDPR, many large enterprises were reluctant to share their data with external partners, making it challenging to use external fine-tuning services without undergoing resource-intensive compliance reviews. Consequently, many organizations opt to focus on other areas instead.\n",
    "\n",
    "The goal of this blog post is to demonstrate how you can fine-tune some of these models in a restricted enterprise environment. To illustrate this, I will run everything on my personal laptop, a 2021 MacBook Pro with an M1 Max chip and 32 GB of RAM. While this is a fairly powerful machine, I believe it is realistic to expect access to equivalent resources within an enterprise setting. I’ll show you how to achieve this step-by-step.\n",
    "\n",
    "## Me\n",
    "First a bit about who I am. My name is Dilovan Celik, you are probably reading this on my GitHub. If you want to read more information from me, you can visit my substack [Thoughts on Data](https://dilovan.substack.com) or connect with me on [LinkedIn](https://www.linkedin.com/in/dilovancelik/). Now on to the main event. \n",
    "\n",
    "## The Project\n",
    "To make this post more relatable, I’ll guide you through the process of building a Retrieval-Augmented Generation (RAG) system from scratch, where we will fine-tune the embedding algorithm. While building a RAG is an interesting project in itself, my primary hope is not only to show you how to build a RAG but also to empower you to experiment with creating your own products, even when you don’t have access to unlimited resources.\n",
    "\n",
    "Before diving in, let’s take a look at what exactly we’ll be building.\n",
    "\n",
    "### Retrieval Agumented Generation (RAG)\n",
    "A RAG is a method to enhance an existing chatbot with your own data. Essentially, it involves building a layer on top of the chatbot that takes user input and enriches it with relevant context from your internal documents. (This, by the way, is a different approach to integrating your own data into a chatbot.)\n",
    "\n",
    "The RAG enriches the original user query by embedding it and searching through a database of your internal documents, finding documents that are similar to the user's query.\n",
    "\n",
    "A simplified illustration of the RAG architecture might look like this:\n",
    "\n",
    "![Illustration of RAG architecture](./images/rag_illustration.png)\n",
    "\n",
    "### Mette (Frederiksen, Danish PM) Bot\n",
    "Our RAG will be an application where you can input a topic and retrieve the Danish Prime Minister Mette Frederiksen's opinion on the subject. The RAG will be enriched with speeches given by Mette Frederiksen.\n",
    "\n",
    "The challenge is that these speeches are mostly in Danish. Since Danish is a relatively small language (approximately 6 million native speakers), it can be difficult to find a highly effective open-source embedding model. Even when choosing a multilingual model, danish typically represents only a small fraction of the training data.\n",
    "\n",
    "Therefore, our approach will be to download a small multilingual embedding model and improve it for our specific data through fine-tuning.\n",
    "\n",
    "\n",
    "## Fintunning Pipeline\n",
    "Now that we have some background on what I'm trying to achieve lets look at the steps we need to take. \n",
    "\n",
    "1. Data Retrieval (We will not go through this, because every case is different)\n",
    "2. Data Preparation\n",
    "3. Establish Baseline for non Finetuned Model\n",
    "4. Finetuning\n",
    "5. Test to see if the model performance has improved\n",
    "6. Sample code for simple RAG\n",
    "\n",
    "As shown I will not go through the data retrieval part. I will skip this part, because the recommended approach varies depending on your data source. You can find my data in the folder data/taler\n",
    "\n",
    "To avoid having to import libraries all over the place lets import the needed dependencies here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "125960c4-4081-4bb7-bcfe-006ea70e7b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "import ollama\n",
    "import uuid\n",
    "import json\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import psycopg as pg\n",
    "from ollama import chat, ChatResponse\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from peft import LoraConfig, TaskType\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e19dd5-092a-4dc0-b1ec-a203415fdf0d",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "One of the most critical steps when fine-tuning a model is preparing your data properly. Before diving in, it's important to consider the specific goal of your application.\n",
    "\n",
    "For example, imagine you are building an application where users can input a list of symptoms and receive the 10 most likely diagnoses. In this scenario, you need a model that can match the embedding of the symptom list with the embeddings of the diagnoses. Conversely, if your goal were to retrieve applicable laws based on a user's question, you would need to train the model to match questions to relevant legal texts.\n",
    "\n",
    "In this example, our objective is to create a model that can take a subject (e.g., Crime, Unemployment, Climate Change) and match it with public speeches made by the Danish Prime Minister, Mette Frederiksen.\n",
    "\n",
    "I have a folder containing 153 speeches by the Prime Minister. However, because these speeches cover a wide range of subjects, I need to break them into smaller, more manageable chunks.\n",
    "\n",
    "A simple approach would be to split each speech into chunks of a fixed number of characters (e.g., 1,000 characters) with some overlap between chunks. This naive method can work quite well in many scenarios.\n",
    "\n",
    "However, I encourage you to analyze your data for natural breaks that could serve as chunking boundaries. For instance, if you were working with legal texts, it might make sense to assign each article to its own chunk.\n",
    "\n",
    "When reviewing my data, I noticed that longer speeches often included lines like **** or ---- as separators between different sections. I can use these as delimiters when chunking the text. For speeches that don't contain these separators, I will fall back on the naive approach, but instead of chunking by characters, I'll chunk by lines to maintain more natural context within each chunk.\n",
    "\n",
    "This approach to data preparation will help ensure that our fine-tuned model produces more relevant and accurate results.\n",
    "The code I wrote to handle chunking looks like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3536928a-ac92-46f2-8e95-d33ad9b37bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 2105.70it/s]\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "files = glob.glob(\"*.txt\", root_dir=\"data/taler\") # find all files in the folder data/taler\n",
    "speech_chunks = {}\n",
    "with tqdm(total=len(files), desc=\"Processing Files\") as fpbar:\n",
    "    for file_name in files:\n",
    "        speech_name = file_name.replace(\".txt\", \"\")\n",
    "        with open(f\"data/taler/{file_name}\", \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            total += len(lines)\n",
    "        context_splits = []\n",
    "\n",
    "        char_split = False\n",
    "        for line in lines:\n",
    "            if re.search(\"[a-zA-Z]\", line) is None and \"\\n\" != line:\n",
    "                char_split = True\n",
    "                break\n",
    "\n",
    "        if char_split:\n",
    "            context = []\n",
    "            for line in lines:\n",
    "                if re.search(\"[a-zA-Z]\", line) is None and \"\\n\" != line:\n",
    "                    context_splits.append(\" \".join(context))\n",
    "                    context = []\n",
    "                else:\n",
    "                    context.append(line)\n",
    "            if context != []:\n",
    "                context_splits.append(\" \".join(context))\n",
    "        else:\n",
    "            chunk_size = 10  # group size\n",
    "            overlap = 2  # overlap size\n",
    "            context_splits = [\n",
    "                \" \".join(lines[i : i + chunk_size])\n",
    "                for i in range(0, len(lines), chunk_size - overlap)\n",
    "            ]\n",
    "\n",
    "        speech_chunks[speech_name] = context_splits\n",
    "        fpbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305f155-488f-4e45-9401-1ee6d0052758",
   "metadata": {},
   "source": [
    "We now have a Python object that looks something like this:\n",
    "\n",
    "```Python\n",
    "{\n",
    "    \"speech-1\": [\"chunk 1\", \"chunk 2\", \"chunk 3\"],\n",
    "    \"speech-2\": [\"chunk 1\", \"chunk 2\", \"chunk 3\"],\n",
    "    ...\n",
    "    \"speech-n\": [\"chunk 1\", \"chunk 2\", \"chunk 3\"],\n",
    "}\n",
    "```\n",
    "\n",
    "It might be tempting to think we can simply feed this data into a \"magic AI model box\" and instantly get a neat embedding algorithm. Unfortunately, it’s not that straightforward.\n",
    "\n",
    "As mentioned earlier, our goal is to create a model where a user can input a subject and retrieve a relevant chunk. To achieve this, we need not only the chunks themselves but also associated user inputs—in this case, the corresponding subjects. Since we don’t have predefined user inputs, we need to generate them.\n",
    "\n",
    "One approach would be to manually annotate each chunk with subjects, which would undoubtedly yield the best results. However, this method would be far too time-consuming. Instead, we’ll generate synthetic data using an LLM (Large Language Model). For this task, I’ll use Microsoft’s phi4 model, running locally on my machine through [ollama](https://ollama.com) (If for some reason you are not allowed to download ollama, you can use the transformer library to create the same functionality in python, read more here: [Transformer Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)). I've chosen this model because it offers a good balance between computational efficiency and output quality. However, I encourage you to experiment with different models to find what works best for your scenario.\n",
    "\n",
    "The method for generating this synthetic data involves feeding each chunk of text into the model and asking it to generate relevant subjects.\n",
    "\n",
    "Below is the prompt I use. The ## part is not part of the actual prompt but rather an English translation for non-Danish speakers:\n",
    "\n",
    "```\n",
    "Kontekst er nedenfor. ## Context below\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Givet den givne kontekst og ingen anden viden. ## Given the context and no other knowledge\n",
    "Genere op til 5 emner som kan beskrive konteksten,. ## Generate up to 5 subjects which can describe the context\n",
    "Hvis der ikke er emner som let kan beskrive konteksten, besvar med <|NAN|> ## If there are no subjects which easily describe the context reply with <|NAN|>\n",
    "\n",
    "Du må kun svarer med emnerne formattet skal være: Emne 1|Emne 2|...|Emne n| ## You are only allowed to reply with subjects in the following format: Subject 1| Subject 2|...|Subject n|\n",
    "```\n",
    "\n",
    "Let's try this approach with a toy example to see how it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084b4464-f82a-4095-addb-4be2fa3494a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\\\n",
    "Kontekst er nedenfor.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Givet den givne kontekst og ingen anden viden.\n",
    "Genere op til 5 emner som kan beskrive konteksten,. \n",
    "Hvis der ikke er emner som let kan beskrive konteksten, besvar med <|NAN|>\n",
    "\n",
    "Du må kun svarer med emnerne formattet skal være: Emne 1|Emne 2|...|Emne n|\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6143fb02-4ec2-449f-b4bf-db6ae4544382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from phi4\n",
      "--------------------------------------------------------------\n",
      "Velfærdspolitik i Danmark|Socialdemokratisk politisk vision|Fokus på social retfærdighed og lige muligheder|Kritik af lavere ydelser til børn og kriminalpolitik|Økonomisk ansvarlighed i det offentlige sektor\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "context = speech_chunks[\n",
    "    \"mette-frederiksens-aabningstale-ved-folketingets-aabningsdebat\"\n",
    "][1]\n",
    "prompt = PROMPT_TEMPLATE.format(context_str=context)\n",
    "res = ollama.chat(\"phi4\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(f\"Response from phi4\\n--------------------------------------------------------------\\n{res.message.content}\\n--------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e321fd0-760e-4465-a963-6807bd90bb38",
   "metadata": {},
   "source": [
    "As you can see we get subjects back in the way we want, and can now start generating data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88aa85b8-ab7b-4789-95ee-299fbc1d29e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Queries: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1333/1333 [2:12:48<00:00,  5.98s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = {\n",
    "    \"speech\": {},\n",
    "    \"queries\": {},\n",
    "    \"corpus\": {},\n",
    "    \"relevant_docs\": {},\n",
    "    \"related_speech\": {},\n",
    "}\n",
    "\n",
    "## This part is just to be able to track how far we are\n",
    "total = 0\n",
    "for speech, chunks in speech_chunks.items():\n",
    "    total += len(chunks)\n",
    "\n",
    "# TODO Batching \n",
    "with tqdm(total=total, desc=\"Generating Queries\") as pbar:\n",
    "    for speech, chunks in speech_chunks.items():\n",
    "        speech_id = str(uuid.uuid4())\n",
    "        dataset[\"speech\"][speech_id] = speech\n",
    "        for chunk in chunks:\n",
    "            content_id = str(uuid.uuid4())\n",
    "            dataset[\"corpus\"][content_id] = chunk\n",
    "            dataset[\"related_speech\"][content_id] = speech_id\n",
    "            prompt = PROMPT_TEMPLATE.format(context_str=chunk)\n",
    "            res = ollama.chat(\"phi4\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "            reply = res.message.content\n",
    "            if \"<|NAN|>\" in reply:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            for query in reply.split(\"|\"):\n",
    "                query_id = str(uuid.uuid4())\n",
    "                dataset[\"queries\"][query_id] = query\n",
    "                dataset[\"relevant_docs\"][query_id] = [content_id]\n",
    "            pbar.update(1)\n",
    "\n",
    "with open(\"data/base_data.json\", \"w\") as f:\n",
    "    f.write(json.dumps(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12cb6c2-40cf-4f15-88a0-a6ffd3ca1e82",
   "metadata": {},
   "source": [
    "This process will take a while. On my MacBook Pro, it takes approximately 2.5 hours. However, if you have access to a machine with a more powerful GPU, such as an NVIDIA A100, you can significantly speed this up. Alternatively, you can use the pre-generated data, which is available in data/base_data.json.\n",
    "\n",
    "The current code uses a \"relevant document\" setup. In this approach, each query (subject) and chunk is assigned a unique UUID, with a separate structure that maps each query to its relevant subjects.\n",
    "\n",
    "Before moving forward, we need to address a couple of things:\n",
    "\n",
    "1. **Remove Duplicate Subjects:** It’s possible that the same subject appears multiple times, so we need to clean this up.\n",
    "2. **Organize Data for Model Training:** The data needs to be structured appropriately for training our model.\n",
    "\n",
    "Additionally, I’ve added a related speech object to help us look up which speech a particular chunk originates from. This will prove useful later on. However, the data is currently not in the best shape, so let's do some cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d188d69-f89c-43e1-b2ea-9b836d8b2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset\n",
    "\n",
    "# Strips away redundant whitespace from the subjects and \n",
    "# removes subjects which are note actual subjects\n",
    "queries = defaultdict(list)\n",
    "for key, value in data[\"queries\"].items():\n",
    "    if value == \"\" or \"--\" in value:\n",
    "        continue\n",
    "    queries[data[\"queries\"][key].strip()].append(key)\n",
    "\n",
    "# We go through the list chunks, and remove parts that does\n",
    "# That does not include any information. This part is very \n",
    "# dependent on your data. So look through and implement the\n",
    "# rules that makes sense for you\n",
    "\n",
    "corpus = defaultdict(list)\n",
    "pop_keys = []\n",
    "for key, value in data[\"corpus\"].items():\n",
    "    value = \" \".join(value.replace(\"Tale\\n \\n \\n \\n \\n \\n \\n \", \"\").split())\n",
    "    if value.isspace() or len(value) <= 60:\n",
    "        pop_keys.append(key)\n",
    "    data[\"corpus\"][key] = value\n",
    "    corpus[value].append(key)\n",
    "\n",
    "for key in pop_keys:\n",
    "    data[\"corpus\"].pop(key)\n",
    "    data[\"related_speech\"].pop(key)\n",
    "\n",
    "\n",
    "# We create a new query structure, which removes duplicate queries\n",
    "# and remap the relevant documents to the new query ids\n",
    "relevant_docs = defaultdict(list)\n",
    "new_queries = {}\n",
    "for query in queries.keys():\n",
    "    id = str(uuid.uuid4())\n",
    "    new_queries[id] = query\n",
    "    query_ids = queries[query]\n",
    "    for query_id in query_ids:\n",
    "        for doc_id in data[\"relevant_docs\"][query_id]:\n",
    "            if doc_id not in pop_keys:\n",
    "                relevant_docs[id].append(doc_id)\n",
    "\n",
    "clean_data = {}\n",
    "clean_data[\"query\"] = new_queries\n",
    "clean_data[\"relevant_docs\"] = relevant_docs\n",
    "clean_data[\"related_speech\"] = data[\"related_speech\"]\n",
    "clean_data[\"corpus\"] = data[\"corpus\"]\n",
    "\n",
    "with open(\"data/cleaned_base_data.json\", \"w\") as out:\n",
    "    json.dump(clean_data, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a81dd35-3604-42dc-8ab5-9727dad183c3",
   "metadata": {},
   "source": [
    "Once we have cleaned the data, the next step is to determine how we want to train our model. A crucial part of this process involves selecting an appropriate loss function. The loss function is responsible for penalizing the model when it makes incorrect predictions and rewarding it when it makes correct predictions. It essentially guides the model toward better performance during training.\n",
    "\n",
    "For the type of fine-tuning we are aiming for, it is common to organize the data into triplets. The triplet data format consists of:\n",
    "\n",
    "- **Anchor:** The subject or query.\n",
    "- **Positive:** A piece of context related to the anchor.\n",
    "- **Negative:** A piece of context unrelated to the anchor.\n",
    "  \n",
    "An example of this format might look like:\n",
    "\n",
    "- **Anchor:** Defense Spending\n",
    "- **Positive:** \"The new aircraft carrier is over budget but will bring much-needed capabilities.\"\n",
    "- **Negative:** \"It's cold today, but the sun is out, making it nice if you wear the right layers.\"\n",
    "  \n",
    "However, a keen observer might notice that our dataset currently contains only positives. Therefore, we need to generate negatives to complete our triplets.\n",
    "\n",
    "There are several ways to achieve this:\n",
    "\n",
    "- **Generate Negatives with an LLM:** Similar to how we generated subjects earlier, we could use a large language model to create negative samples.\n",
    "- **Use an Unrelated Text Corpus:** Another approach is to introduce entirely unrelated text as negatives. For example, if you are working with clinical records, you could download annual reports from Goldman Sachs and use that text as your negative data.\n",
    "  \n",
    "For this project, I opted for the second approach. I downloaded a dataset containing customer reviews and randomly selected reviews as negative samples.\n",
    "\n",
    "Here's the code snippet demonstrating this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "388dd815-c8cf-4f4e-bbba-d2378aa05d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['speech', 'queries', 'corpus', 'relevant_docs', 'related_speech'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec28619c-fe00-4baa-832a-4c8db86b7f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating positive negative pairs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5998/5998 [00:02<00:00, 2108.13it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/cleaned_base_data.json\", \"r\") as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "negatives: pd.DataFrame = pd.read_csv(\"data/negatives.csv\")[\"review_text\"]\n",
    "\n",
    "total = 0\n",
    "for id, relevant_docs in data[\"relevant_docs\"].items():\n",
    "    total += len(relevant_docs)\n",
    "\n",
    "triplets = []\n",
    "with tqdm(total=total, desc=\"creating positive negative pairs\") as pbar:\n",
    "    for query_id, doc_ids in data[\"relevant_docs\"].items():\n",
    "        anchor = data[\"query\"][query_id]\n",
    "        for id in doc_ids:\n",
    "            triplets.append(\n",
    "                {\n",
    "                    \"anchor\": anchor,\n",
    "                    \"positive\": data[\"corpus\"][id],\n",
    "                    \"negative\": negatives.sample().values[0],\n",
    "                }\n",
    "            )\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e359a-ada9-4e37-956a-f04a0a53fbac",
   "metadata": {},
   "source": [
    "Now that we have our triplet data, the last thing is to split our data into Train, Test and Validation.\n",
    "We use Train data to finetune our model, we use test data to estimate the performance of the model during training, and validition to ensure that we did not just find a model that fit our test data well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3bc5a38-f5da-4d2f-97fc-53679ad89449",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplet, val_triplet = train_test_split(pd.DataFrame(triplets), test_size=0.2)\n",
    "train_triplet, test_triplet = train_test_split(\n",
    "    pd.DataFrame(train_triplet), test_size=0.2\n",
    ")\n",
    "\n",
    "train_triplet.to_json(\"data/triplet_data_train.json\")\n",
    "test_triplet.to_json(\"data/triplet_data_test.json\")\n",
    "val_triplet.to_json(\"data/triplet_data_val.json\")\n",
    "\n",
    "dataset: DatasetDict = {\n",
    "    \"train\": Dataset.from_pandas(train_triplet, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(test_triplet, preserve_index=False),\n",
    "    \"validation\": Dataset.from_pandas(val_triplet, preserve_index=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dce45a-b4d9-46cc-86ef-d199e1e08636",
   "metadata": {},
   "source": [
    "### Baseline Model and Valuation Strategy\n",
    "\n",
    "Our data is now ready, but before we start finetuning our model, we should consider how we are going to decide whether or not the finetuned model is better than the baseline one. \n",
    "\n",
    "To do this we have to do the following steps:\n",
    "1. Download a Baseline model\n",
    "2. Find a valuation method\n",
    "3. Valuate the Baseline model\n",
    "4. Finetune the model\n",
    "5. Valuate the finetuned model\n",
    "\n",
    "The first thing we will do is to download the baseline model from HuggingFace, using the SentenceTransformer library. I have chosen the [multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) model. I have chosen this model because it has a good baseline performance, and is small enough to train on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "503dae50-587a-4fe9-9585-45b00ad99927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Size:  384\n"
     ]
    }
   ],
   "source": [
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Test the model\n",
    "emb = model.encode(\"Hello World\")\n",
    "print(\"Vector Size: \", len(emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35baf91c-f36a-454d-8343-b30188a541e4",
   "metadata": {},
   "source": [
    "Now that we've downloaded our baseline model, let's take a moment to think about how to best evaluate its performance.\n",
    "\n",
    "Now that we've downloaded our baseline model, let's spend some time thinking about how to best evaluate our model.\n",
    "\n",
    "A good place to start is here: [SentenceTransformer Evaluator Classes](https://sbert.net/docs/sentence_transformer/training_overview.html#evaluator).\n",
    "\n",
    "Going through the list, there are several that are interesting, but the one that fits our data best is the [Triplet Evaluator](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.TripletEvaluator).\n",
    "\n",
    "What the Triplet Evaluator does is compare the similarity between the anchor and the positive, as well as between the anchor and the negative. It then returns a percentage representing the proportion of records where the anchor embedding was closer to the positive embedding than to the negative. So, a result of 0.8 means that in 80% of cases, the anchor was closer to the positive than to the negative.\n",
    "\n",
    "We can run the model on our data using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "049b8f12-8632-4f4d-85d6-23eae4e78920",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_evaluator = TripletEvaluator(\n",
    "    anchors=dataset[\"test\"][\"anchor\"],\n",
    "    positives=dataset[\"test\"][\"positive\"],\n",
    "    negatives=dataset[\"test\"][\"negative\"],\n",
    "    name=\"dev_evaluator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "274b8379-29c0-4c5f-a8dd-9ee24ec61a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dev_evaluator_cosine_accuracy': 0.9281250238418579}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_evaluator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba2997-0549-4d03-a975-2959bd6a93df",
   "metadata": {},
   "source": [
    "With our test data, we get a result of 0.91 using our baseline model. This means that in 91% of cases, the baseline model will place the anchor closer to the positive than to the negative.\n",
    "\n",
    "This is a great evaluation to start with, but in reality, this is not what we are truly interested in. We want to know whether inputting a subject will return related chunks of text. So, let's try to write a custom evaluation script. I will base my model on the Recall@K metric. The metric simply tests whether the anchor returns the positive in its top K results.\n",
    "\n",
    "We will modify this to check if any of the related documents are in the top K results. Additionally, we will introduce a metric to show the percentage of relevant documents present in the top K results.\n",
    "\n",
    "Before writing this test, we need to store the embeddings somewhere so we can query them. Many databases can handle this, but I've chosen to use Postgres with the PG Vector extension. Here's how to set it up in your Postgres database:\n",
    "\n",
    "First, install the extension by running the following command:\n",
    "```sql\n",
    "CREATE EXTENSION vector;\n",
    "```\n",
    "Then, we will create two tables to store our embeddings: one for our baseline embeddings and another for the fine-tuned embeddings:\n",
    "```sql\n",
    "CREATE TABLE embeddings_base (id bigserial PRIMARY KEY, speech VARCHAR, context_id VARCHAR, context VARCHAR, embedding vector(384))\n",
    "CREATE TABLE embeddings (id bigserial PRIMARY KEY, speech VARCHAR, context_id VARCHAR, context VARCHAR, embedding vector(384))\n",
    "```\n",
    "You might wonder about the vector(384) datatype. This simply means we are storing a vector of size 384. If you are unsure about the size of your embedding, you can find out by running:\n",
    "```python\n",
    "len(model.encode(\"Hello\"))\n",
    "```\n",
    "Now that our table is ready for embeddings, we can load them. Let's do that using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "085f5d21-6761-4b73-905b-5a5c33321797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(speech: str, context_id: str, context: str, embedding: List, write_to_base: bool) -> None:\n",
    "    conn = pg.connect(\"dbname=vector_rag user=postgres password=postgres\")\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    if write_to_base:\n",
    "        cur.execute(\n",
    "            \"INSERT INTO embeddings_base (speech, context_id, context, embedding) VALUES (%s, %s, %s, %s)\",\n",
    "            (speech, context_id, context, str(embedding)),\n",
    "        )\n",
    "    else:\n",
    "        cur.execute(\n",
    "            \"INSERT INTO embeddings (speech, context_id, context, embedding) VALUES (%s, %s, %s, %s)\",\n",
    "            (speech, context_id, context, str(embedding)),\n",
    "        )\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc690ec7-5dce-4f8d-8dfc-191ae7744681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1173/1173 [01:31<00:00, 12.84it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(data[\"corpus\"].keys()), desc=\"Saving embeddings\") as pbar:\n",
    "    for id, context in data[\"corpus\"].items():\n",
    "        speech_name = data[\"related_speech\"][id]\n",
    "        embedding = model.encode(context).tolist()\n",
    "        write_to_db(speech_name, id, context, embedding, True)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d029d7d-6017-4e98-b917-a6605b763803",
   "metadata": {},
   "source": [
    "Then we can implement our recall k methods and test our baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69072b26-c9bd-4c83-b335-502f6fdbe58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 Metric:  0.3072916666666667\n",
      "Recall@10 Metric %:  0.28973958333333333\n",
      "Recall@4 Metric:  0.21458333333333332\n",
      "Recall@4 Metric %:  0.20355902777777776\n"
     ]
    }
   ],
   "source": [
    "# Add lookup to make checks faster\n",
    "data[\"query_lk\"] = {}\n",
    "for key, value in data[\"query\"].items():\n",
    "    data[\"query_lk\"][value] = key\n",
    "\n",
    "def recall_k(query: str, model: SentenceTransformer, k: int, data: dict, check_base: bool) -> float:\n",
    "    query_id = data[\"query_lk\"][query]\n",
    "    expected_ids = data[\"relevant_docs\"][query_id]\n",
    "    embedded_query = model.encode(query).tolist()\n",
    "\n",
    "    conn = pg.connect(\"dbname=vector_rag user=postgres password=postgres\")\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    if check_base:\n",
    "        cur.execute(\n",
    "            \"SELECT context_id FROM embeddings_base ORDER BY embedding <=> %s::vector LIMIT %s;\",\n",
    "            (str(embedded_query), str(k)),\n",
    "        )\n",
    "    else: \n",
    "        cur.execute(\n",
    "            \"SELECT context_id FROM embeddings ORDER BY embedding <=> %s::vector LIMIT %s;\",\n",
    "            (str(embedded_query), str(k)),\n",
    "        )\n",
    "        \n",
    "    results = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    min_res = min(len(expected_ids), k)\n",
    "    result_per = len(set(results) & set(expected_ids)) / min_res\n",
    "    \n",
    "    result_k = 1.0 if set(results) & set(expected_ids) else 0.0\n",
    "\n",
    "    return result_k, result_per\n",
    "\n",
    "\n",
    "recall_10 = test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 10, data, True), axis=1\n",
    "    )\n",
    "recall_10 = pd.DataFrame(recall_10.tolist(), index=recall_10.index)\n",
    "print(\"Recall@10 Metric: \", recall_10[0].sum() / test_triplet.shape[0])\n",
    "print(\"Recall@10 Metric %: \", recall_10[1].sum() / test_triplet.shape[0])\n",
    "\n",
    "\n",
    "recall_4 = test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 4, data, True), axis=1\n",
    "    )\n",
    "recall_4 = pd.DataFrame(recall_4.tolist(), index=recall_4.index)\n",
    "print(\"Recall@4 Metric: \", recall_4[0].sum() / test_triplet.shape[0])\n",
    "print(\"Recall@4 Metric %: \", recall_4[1].sum() / test_triplet.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df99a0-9d78-46fc-808c-6395470139a8",
   "metadata": {},
   "source": [
    "With our baseline, we can now try to fine-tune our model to see if we can improve its performance.\n",
    "\n",
    "### Fine-Tuning with Limited Compute and Memory\n",
    "Although we've chosen a very small model, my laptop does not have enough memory to train it locally. This leaves me with two options:\n",
    "\n",
    "Rent a Bigger Machine: This would certainly make the process easier, but I've always found it more interesting to work within limitations.\n",
    "Work Within Constraints: I've decided to take the more challenging route and explore ways to fine-tune the model using limited resources.\n",
    "One promising solution is to perform fine-tuning using [Low Rank Adaptation (LoRA)](https://arxiv.org/pdf/2106.09685). \n",
    "\n",
    "When using LoRA, we freeze all the parameters in the baseline model and train a smaller adaptation layer, which is then multiplied onto the original weights. This significantly reduces the number of parameters we need to train. According to the original paper, this approach can reduce memory requirements threefold.\n",
    "![LowRank Adaptation](images/LoRA.png)\n",
    "\n",
    "Let's see the difference in trainable parameters before and after applying the LoRA adapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d803f126-ef4d-4b65-86f5-dce3ee669d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model parameters:    117,653,760\n",
      "----------------------------------------------------------------------\n",
      "Total trainable parameters before LoRA: 117,653,760\n",
      "Trainable Percentage trainable before LoRA:     100.00%\n",
      "----------------------------------------------------------------------\n",
      "Total trainable parameters after LoRA: 669,696\n",
      "Trainable Percentage trainable after LoRA:     0.28%\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainable_params = 0\n",
    "all_params = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    all_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "        # print(f\"{name}: shape={param.shape}, params={param.numel()}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total model parameters:    {all_params:,}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Total trainable parameters before LoRA: {trainable_params:,}\")\n",
    "print(f\"Trainable Percentage trainable before LoRA:     {100 * trainable_params / all_params:.2f}%\")\n",
    "print(\"-\" * 70)\n",
    "## Adding LoRA Adapter\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model.add_adapter(peft_config)\n",
    "\n",
    "trainable_params_lora = 0\n",
    "for name, param in model.named_parameters():\n",
    "    all_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params_lora += param.numel()\n",
    "print(f\"Total trainable parameters after LoRA: {trainable_params_lora:,}\")\n",
    "print(f\"Trainable Percentage trainable after LoRA:     {100 * trainable_params_lora / all_params:.2f}%\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1d55f9-163e-4e1c-8fbd-11b0e2fb1f0b",
   "metadata": {},
   "source": [
    "As you can see, we reduce the trainable parameters by 99.72%, which is quite significant and ensures that this can run on my Mac. Now, let's get to the training.\n",
    "\n",
    "First, we need to decide on a loss function. To do that, I referred to the [Sentence Transformer Loss Overview](https://sbert.net/docs/sentence_transformer/loss_overview.html), and found that MultipleNegativesRankingLoss fits my data well.\n",
    "\n",
    "Second, we need to set some hyperparameters in our training arguments. There is no one-size-fits-all solution when choosing hyperparameters. From what I've read, the best approach is to experiment. Below are the hyperparameters I've chosen. I’d like to point out that the low batch size of 8 is primarily due to my memory limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f41d29b0-bf5e-4df4-8206-6a879b02e7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c044a750364a3b913925c76a803c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1440' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1440/1440 38:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.010200</td>\n",
       "      <td>1.863809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.004400</td>\n",
       "      <td>0.859534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.779300</td>\n",
       "      <td>0.780737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.891900</td>\n",
       "      <td>0.730496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.768600</td>\n",
       "      <td>0.709299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.697083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.661300</td>\n",
       "      <td>0.684948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.609200</td>\n",
       "      <td>0.671644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.653900</td>\n",
       "      <td>0.663791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.639600</td>\n",
       "      <td>0.655931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.674600</td>\n",
       "      <td>0.652125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.563400</td>\n",
       "      <td>0.651511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.638600</td>\n",
       "      <td>0.649064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.648243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1440, training_loss=0.8518072108427683, metrics={'train_runtime': 2311.7553, 'train_samples_per_second': 4.981, 'train_steps_per_second': 0.623, 'total_flos': 0.0, 'train_loss': 0.8518072108427683, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"../models/multilingual-e5-small-finetune-danish-subject\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    bf16=True,  \n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch according to the documentation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=25,\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3803d63b-01f7-4222-bcaa-15c126013bc6",
   "metadata": {},
   "source": [
    "Now that we have a trained model, and before we test it, I would like to share a bit about what I look for in the logging. First, I check to see if there is a steadily decreasing validation loss throughout the training process.\n",
    "\n",
    "What can sometimes happen is that while the training loss continues to decrease, the validation loss stops improving. This is usually a sign of overfitting the model.\n",
    "\n",
    "Second, I notice that even though my training loss is generally decreasing, there are occasional spikes. This is probably due to the small batch size. Unfortunately, I cannot change this because of memory limitations.\n",
    "\n",
    "Now, we can run our test again to see if performance has improved. To do this, we need to re-embed our corpus using the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "803239e9-7e38-4255-8c1d-7752d5e48ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1173/1173 [03:05<00:00,  6.31it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(data[\"corpus\"].keys()), desc=\"Saving embeddings\") as pbar:\n",
    "    for id, context in data[\"corpus\"].items():\n",
    "        speech_name = data[\"related_speech\"][id]\n",
    "        embedding = model.encode(context).tolist()\n",
    "        write_to_db(speech_name, id, context, embedding, False)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d26a4b-31f5-4230-9e40-1c221af8083b",
   "metadata": {},
   "source": [
    "And finally we can test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6916e302-4054-4400-b26a-5b2fe0914582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 Metric:  0.41041666666666665\n",
      "Recall@10 Metric %:  0.38942708333333337\n",
      "Recall@4 Metric:  0.284375\n",
      "Recall@4 Metric %:  0.2659722222222222\n"
     ]
    }
   ],
   "source": [
    "\n",
    "recall_10 = test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 10, data, False), axis=1\n",
    "    )\n",
    "recall_10 = pd.DataFrame(recall_10.tolist(), index=recall_10.index)\n",
    "print(\"Recall@10 Metric: \", recall_10[0].sum() / test_triplet.shape[0])\n",
    "print(\"Recall@10 Metric %: \", recall_10[1].sum() / test_triplet.shape[0])\n",
    "\n",
    "\n",
    "recall_4 = test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 4, data, False), axis=1\n",
    "    )\n",
    "recall_4 = pd.DataFrame(recall_4.tolist(), index=recall_4.index)\n",
    "print(\"Recall@4 Metric: \", recall_4[0].sum() / test_triplet.shape[0])\n",
    "print(\"Recall@4 Metric %: \", recall_4[1].sum() / test_triplet.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da67cc5-bd32-40c1-b430-e7ee1b68b869",
   "metadata": {},
   "source": [
    "GREAT SUCCES!!!!! It looks like we've improved our model on all metrics. We did not have to send our data anywhere :D \n",
    "\n",
    "Before we finish for today, I would like to add a few words to the recall metrics. To b honest the result does not look that great.\n",
    "The reason is that, we are looking for very specific values. But when we look at the actual data what we see is the queries finding \n",
    "relevant text. Just not the specific documents we were looking for. The reason is that when our model generated subjects. It could generate\n",
    "multiple subjects which are very close in meaning to eachother. \n",
    "\n",
    "For instance we have 143 subjects with the word *pandemic* (pandemi in danish), this means that when testing for one of the pandemic subjects, \n",
    "we might get a relevant chunk from a different pandemic subject.\n",
    "\n",
    "I Would like for you to think at these measures as a way to compare the model performance before and after finetuning, and not as an absolute truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9966a83e-77c3-4d34-8919-745c5ce575d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([subject for subject in data[\"query_lk\"].keys() if \"pandemi\" in subject.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410fd7f-f8e6-4c36-af1c-9b601e08ef59",
   "metadata": {},
   "source": [
    "Now lets built a simple RAG application. Having the model this does not take a lot of code, which can be found below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52999739-a9ef-412f-81e9-82eef582cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "METTE_PROMPT_TEMPLATE = \"\"\"\\\n",
    "Du er en LLM som giver svarer på hvad Mette Frederiksen syntes om {question}. \\\n",
    "Du må kun besvarer baseret af de nedenstående citater, Du skal kun komme med et enkelt svar som er på Dansk \\\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\"\"\"\n",
    "\n",
    "def rag():\n",
    "    while True:\n",
    "        ### First we ask the user for input\n",
    "        query = input(\"Hvilket emne vil du høre Mette Frederiksens mening om? Skriv \\\\stop hvis du vil lukke programmet. \\n\\n\")\n",
    "\n",
    "        ### Check if user wants to exit\n",
    "        if query == \"\\\\stop\":\n",
    "            print(\"Farvel\")\n",
    "            break\n",
    "            \n",
    "        print(\"Tænker ...\")\n",
    "        emb = model.encode(query).tolist() ### Encode input\n",
    "        \n",
    "        print(\"Henter citater...\")\n",
    "        ### Query Database for relevant context\n",
    "        conn = pg.connect(\"dbname=vector_rag user=postgres password=postgres\")\n",
    "        conn.autocommit = True\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        cur.execute(\n",
    "            \"SELECT speech, context FROM embeddings ORDER BY embedding <-> %s::vector LIMIT 5;\",\n",
    "            (str(emb),),\n",
    "        )\n",
    "        \n",
    "        result = cur.fetchall()\n",
    "        \n",
    "        context_all = []\n",
    "        speeches =  []\n",
    "        for row in result:\n",
    "            speech = row[0]\n",
    "            speeches.append(speech)\n",
    "            context_all.append(f\"'{row[1]}'\")\n",
    "\n",
    "        ### Generate Prompt\n",
    "        prompt = METTE_PROMPT_TEMPLATE.format(context_str=\"\\n\\n\".join(list(set(context_all))), question=query)\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"Prøver at formulere mig...\")\n",
    "        \n",
    "        res: ChatResponse = chat(model='phi4', messages=[\n",
    "          {\n",
    "            'role': 'user',\n",
    "            'content': prompt,\n",
    "          },\n",
    "        ])\n",
    "        print(res.message.content)\n",
    "        print(\"\\n\\nLink til Taler og Citater som er valgt:\")\n",
    "        for row in result:\n",
    "            print(\"-\" * 100)\n",
    "            print(f\"Context:\\n{row[1]}\")\n",
    "            print(f\"\\nLink: https://www.dansketaler.dk/tale/{row[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4da060a7-4f24-4094-b8c7-a44574c398cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hvilket emne vil du høre Mette Frederiksens mening om? Skriv \\stop hvis du vil lukke programmet. \n",
      "\n",
      " Integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tænker ...\n",
      "Henter citater...\n",
      "Prøver at formulere mig...\n",
      "Mette Frederiksen udtrykker sig om integration med fokus på de udfordringer Danmark står overfor i integrationsindsatsen. Hun nævner, at mange udlændinge i Danmark klarer sig godt og bidrager til samfundet, men der er også betydelige problemer såsom indvandrerbetingede parallelle samfund, høj kriminalitetsrate blandt unge med indvandrerbaggrund og sværheder for kvinder med minoritetsbaggrund på arbejdsmarkedet. Frederiksen understreger, at integration er en kompleks proces uden hurtige løsninger, og at Danmark skal finde måder at håndtere integrationsudfordringerne på i en bæredygtig og inkluderende måde.\n",
      "\n",
      "\n",
      "Link til Taler og Citater som er valgt:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:\n",
      "There is no quick-fix. No easy solutions. The EU Pact on Migration and Asylum is a solid basis to build on. But we also need broad and equal partnerships. And commitment for the long-term. The partnerships that the EU has entered into is a good example of that. First with the EU-Türkiye-statement. More recently partnerships with Tunisia, Egypt and Mauritania. And increased EU cooperation with countries in the Western Balkans. Partnerships are the right way forward. It is the only way forward. We have to step up cooperation in all areas. Green transition, health, trade, security and migration. Countries in Africa, the Middle East and the Balkans hold so much potential. Now is the time to find the right solutions.\n",
      "\n",
      "Link: https://www.dansketaler.dk/tale/b3403027-36f9-4c0a-bd7e-2591472f61c5\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:\n",
      "There is no quick-fix. No easy solutions. The EU Pact on Migration and Asylum is a solid basis to build on. But we also need broad and equal partnerships. And commitment for the long-term. The partnerships that the EU has entered into is a good example of that. First with the EU-Türkiye-statement. More recently partnerships with Tunisia, Egypt and Mauritania. And increased EU cooperation with countries in the Western Balkans. Partnerships are the right way forward. It is the only way forward. We have to step up cooperation in all areas. Green transition, health, trade, security and migration. Countries in Africa, the Middle East and the Balkans hold so much potential. Now is the time to find the right solutions.\n",
      "\n",
      "Link: https://www.dansketaler.dk/tale/79d50ab7-539e-4160-82ac-bc09acb69750\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:\n",
      "Jeg er overbevist om, at den tid, hvor man kunne slippe af sted med symbolpolitik, forslag om lejre i Kenya, straksopbremsning og snuptagsløsninger, er en saga blot. Kristian Thulesen-Dahl. Lars Løkke: Velkommen til virkeligheden. Mennesker banker på vores dør. Og de skal have hjælp. Europa må arbejde sammen. Vi skal løfte vores ansvar som land og som folk. Vi skal bruge al vores styrke og alt vores overskud til at give mennesker på flugt den beskyttelse, de har behov for. Men vi er samtidig forpligtede til at finde holdbare løsninger. Løsninger, der ikke kun handler om her og nu. Løsninger, der skal række ind i vores fremtid. Og jeg ved godt, at der sikkert er nogle af jer, der vil tænke: Når der er flygtninge, der vandrer på de danske veje, så er der ikke plads til at snakke integration. Det mener jeg, vi er tvunget til. Vi har som samfund været udfordret af integrationsproblemer i årtier. Og der venter en hverdag lige rundt om hjørnet. Mange udlændinge, der bor i Danmark, lever gode liv her. De arbejder, skaber unik værdi og bidrager. De vil integration. Men der er også store problemer. Desværre. Alt for mange kvinder med minoritetsbaggrund lever hjemme uden at gå på arbejde. Mange af de unge mænd er underrepræsenteret i uddannelsessystemet, men overrepræsenteret, når det kommer til kriminalitet. Og der er parallelsamfund i Danmark.\n",
      "\n",
      "Link: https://www.dansketaler.dk/tale/82b26393-0640-4086-b93a-5e8b9e5652c4\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:\n",
      "This is why the ‘Pact for the Future’ is such an important step forward for the United Nations. Thank you, Namibia and Germany for your lead for the Pact. And for the commitment of Secretary General Guterres. It has not always been easy. But it has been a great example of successful collaboration. Negotiations have focused on concrete and action-oriented results. With the necessary willingness to find workable compromises. It is truly significant that we have been able to adopt the Pact by consensus.\n",
      "\n",
      "Link: https://www.dansketaler.dk/tale/6804d4b9-f7ed-4789-b2e7-c1a521315fe6\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Context:\n",
      "Jeg ved, at Kim Østerby er med her i dag, formanden for Fængselsforbundet. Kære kongres; jeg vil gerne have, at I sammen med mig sender en direkte hilsen til fængselsbetjentene. Det kan godt være, I ofte bliver overset i den offentlige debat. Men vi ser jer. Vi ved, hvor hårdt arbejde I har. Og vi vil aldrig acceptere volden mod jer. Bandekonflikten, meget af kriminaliteten og det rå arbejdsmiljø i fængslerne hænger desværre uløseligt sammen med en gruppe af unge med indvandrerbaggrund. Lad det være sagt med det samme. Vi har mange med indvandrerbaggrund i Danmark, der klarer sig godt. Som bidrager. Jeg kan jo se det mange steder i vores samfund. Buschauffører, farmaceuter, læger og social- og sundhedshjælpere. Piger, der griber mulighederne i Danmark. Familier, der bakker op. Mennesker, der kommer hertil. Kæmper for at blive en del af vores samfund. Det er godt. Det ændrer bare ikke på, at vi samtidig står med nogle kolossalt store problemer. På grund af en fejlslagen integrationsindsats. Lad mig give jer et eksempel. Jeg besøgte for nogen tid siden en vuggestue og børnehave i Tingbjerg, sammen med Frank, vores overborgmester i København. Ikke ét af børnene havde dansk oprindelse. Og som én af pædagogerne sagde: ”Hvordan skal vi integrere en minoritet, når der ikke længere er en majoritet at blive integreret sammen med”? København er gået foran med sociale normeringer. Det er rigtig godt. Men København kan ikke gøre det alene. Det er derfor, vi er så optaget af, at Danmark skal kunne følge med. Vores fællesskab skal kunne følge med. Kort sagt: Der er en grænse for, hvor mange nye udlændinge vi kan tage imod.\n",
      "\n",
      "Link: https://www.dansketaler.dk/tale/1184a9b1-aaf0-41cc-9731-940aa8a83179\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m, in \u001b[0;36mrag\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrag\u001b[39m():\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m### First we ask the user for input\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m         query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHvilket emne vil du høre Mette Frederiksens mening om? Skriv \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mstop hvis du vil lukke programmet. \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m### Check if user wants to exit\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m query \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/substack/finetune_blog/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c15553-cb7f-482e-82ad-483a047275e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
