{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a small language, a case for fine-tuning\n",
    "\n",
    "There has been a lot of development in the natural language processing space, especially since ChatGPT was introduced by OpenAI, it seems like a lot of ressources are being spent making LLMs better, and as they become better, people have been improving other things as well such as embeddings. However I live in a small country (Denmark), with a small language with around 6 million people speaking the language, even if we are really generous and clump together Norwegian, Danish and Swedish went is only around 22 million speakers.  \n",
    "\n",
    "If you are a big company or organisation who plans on training the next opensource model you’ll probably chose a bigger language like French, German or Spanish. And even if you are choosing train a multilingual model, there is a big chance, that danish will only be in a small fraction of the training data. \n",
    "\n",
    "Previously I've written about embeddings, and how they can be tool for you as a data engineer (Teaching a machine to read, how LLM's comprehend text). However if you cannot find a good embedding model for your data, it won’t help you that much. Therefore we need to solve this problem.aAs I see it there are two approaches to handle this. The first is to translate your documents to a bigger language like English. This approach has a lot of drawbacks, first the process of translating can be ressource intensive, and even more important, we loose a lot of information in the process.\n",
    "\n",
    "The second approach, is to fine-tune our model with our own data. I personally believe, this to be a much better approach. First we can increase the performance by quite a lot, second we are not just making the model better at working with danish text. We are making the model work better with our Danish text. Let’s say we have a lot of contracts we want to embed, the type of language used would be different than if we were embedding code documentation, or clinical records. I fine-tuned model will given the same ressources, perform better than a generalised one. Since it has had more time training on the type of data which is important to us. \n",
    "\n",
    "In the next part I will go through the steps you need to complete to get to the fine-tuning model.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "One of the most important things when finetuning data, would be to gather data. But before we start data gathering, we need to think about what we are building.\n",
    "\n",
    "Let’s say you are building an application where you can send in a list of symptoms and retrieve the 10 most likely diagnoses. In this case we want a model which can match the embedding of the list of symptoms with the embeddings of the diagnosis. If on the other hand we wanted a model which could retrieve the applicabale laws when given a user questions. We would need to train our model to match the questions to the applicable law. \n",
    "\n",
    "In this example, we will create a model which can take a subject (Crime, Unemployment, Climate Change) and match it with public speeches made by the Danish Prime Minister Mette Frederiksen. \n",
    "I have a folder with a 153 speeches by the Prime Minister. But because the speeches can be on a lot of different subjects I will need to break it into chunks. The naive way of doing this would be to just break it into an even number of characters (lets say 1.000) with some overlap between chunks. This can work really well. \n",
    "However I would encourage you to look at your data to see if there are any natural breaks, you can use for chunking. For instance if we were working with laws, we might chose to put each article into a chunk. \n",
    "\n",
    "When I look through my data, I noticed that for longer speeches lines like * * * * or - - - - would be inserted as a seperator between different sections. I could therefor use this as a delimiter. For speeches where this wasn't the case, I would just use the naive approachm but chunk by lines instead of characters. \n",
    "\n",
    "The following code is what I used to chunk my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First lets get our dependencies imported\n",
    "import glob\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "import ollama\n",
    "import uuid\n",
    "import json\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import psycopg as pg\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "from peft import LoraConfig, TaskType\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "chunk_delimiter = \"<chunk>\"\n",
    "files = glob.glob(\"*\", root_dir=\"taler\")\n",
    "speech_chunks = {}\n",
    "with tqdm(total=len(files), desc=\"Processing Files\") as fpbar:\n",
    "    for file_name in glob.glob(\"*\", root_dir=\"taler\"):\n",
    "        speech_name = file_name.replace(\".txt\", \"\")\n",
    "        with open(f\"taler/{file_name}\", \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            total += len(lines)\n",
    "        context_splits = []\n",
    "\n",
    "        char_split = False\n",
    "        for line in lines:\n",
    "            if re.search(\"[a-zA-Z]\", line) is None and \"\\n\" != line:\n",
    "                char_split = True\n",
    "                break\n",
    "\n",
    "        if char_split:\n",
    "            context = []\n",
    "            for line in lines:\n",
    "                if re.search(\"[a-zA-Z]\", line) is None and \"\\n\" != line:\n",
    "                    context_splits.append(\" \".join(context))\n",
    "                    context = []\n",
    "                else:\n",
    "                    context.append(line)\n",
    "            if context != []:\n",
    "                context_splits.append(\" \".join(context))\n",
    "        else:\n",
    "            chunk_size = 10  # group size\n",
    "            overlap = 2  # overlap size\n",
    "            context_splits = [\n",
    "                \" \".join(lines[i : i + chunk_size])\n",
    "                for i in range(0, len(lines), chunk_size - overlap)\n",
    "            ]\n",
    "\n",
    "        speech_chunks[speech_name] = context_splits\n",
    "        fpbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a Python object which looks something like this \n",
    "\n",
    "```Python\n",
    "{\n",
    "    \"speech-1\": [\"chunk 1\", \"chunk 2\", \"chunk 3\"],\n",
    "    \"speech-2\": [\"chunk 1\", \"chunk 2\", \"chunk 3\"],\n",
    "    ...\n",
    "    \"speech-n\": [\"chunk 1\", \"chunk 2\", \"chunk 3\"],\n",
    "}\n",
    "```\n",
    "\n",
    "One might think that we can just feed this information into a magic AI model box and get a really neet embedding algorithm out of it. Unfortunately that is not the case. Like I mentioned before we want to have a model where a user can input a subject, and retrieve a correct chunk. This means we need not just the chunks, but the user input in this case different subjects. Since we do not have the user input, we need to generate them. This could be done manually by going through the chunks and carefully annotating all of them. This approach would definitely give the best result, however it would take up way to much time. Instead we'll generate synthetic data using an LLM. For this case we will use phi4 from microsoft running in ollama on my local machine. I've chosen this model because I believe it has a good tradeoff between compute and result. But test out different models for yourself. The approach to generate this data, is to feed the model with a chunk of text, and ask it to generate the subjects. \n",
    "\n",
    "The context I use is the following the ## part is not in the prompt but an english translation to non danish speakers: \n",
    "\n",
    "```\n",
    "Kontekst er nedenfor. ## Context below\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Givet den givne kontekst og ingen anden viden. ## Given the context and no other knowledge\n",
    "Genere op til 5 emner som kan beskrive konteksten,. ## Generate up to 5 subjects which can describe the context\n",
    "Hvis der ikke er emner som let kan beskrive konteksten, besvar med <|NAN|> ## If there are no subjects which easily describe the context reply with <|NAN|>\n",
    "\n",
    "Du må kun svarer med emnerne formattet skal være: Emne 1|Emne 2|...|Emne n| ## You are only allowed to reply with subjects in the following format: Subject 1| Subject 2|...|Subject n|\n",
    "```\n",
    "\n",
    "Lets try it out with a toy example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\\\n",
    "Kontekst er nedenfor.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Givet den givne kontekst og ingen anden viden.\n",
    "Genere op til 5 emner som kan beskrive konteksten,. \n",
    "Hvis der ikke er emner som let kan beskrive konteksten, besvar med <|NAN|>\n",
    "\n",
    "Du må kun svarer med emnerne formattet skal være: Emne 1|Emne 2|...|Emne n|\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mette-frederiksens-aabningstale-ved-folketingets-aabningsdebat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mspeech_chunks\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmette-frederiksens-aabningstale-ved-folketingets-aabningsdebat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PROMPT_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(context_str\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m      5\u001b[0m res \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi4\u001b[39m\u001b[38;5;124m\"\u001b[39m, messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mette-frederiksens-aabningstale-ved-folketingets-aabningsdebat'"
     ]
    }
   ],
   "source": [
    "context = speech_chunks[\n",
    "    \"mette-frederiksens-aabningstale-ved-folketingets-aabningsdebat\"\n",
    "][1]\n",
    "prompt = PROMPT_TEMPLATE.format(context_str=context)\n",
    "res = ollama.chat(\"phi4\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(res.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case this looks great. If the data doesn't look like you expect it too, you can expirement with a different prompt or maybe even model. \n",
    "For an overview of the models you available through ollama please visit the [ollama site](https://ollama.com/search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Queries:  94%|█████████▍| 1256/1333 [2:21:07<08:39,  6.74s/it] \n"
     ]
    }
   ],
   "source": [
    "dataset = {\n",
    "    \"speech\": {},\n",
    "    \"queries\": {},\n",
    "    \"corpus\": {},\n",
    "    \"relevant_docs\": {},\n",
    "    \"related_speech\": {},\n",
    "}\n",
    "\n",
    "## This part is just to be able to track how far we are\n",
    "total = 0\n",
    "for speech, chunks in speech_chunks.items():\n",
    "    total += len(chunks)\n",
    "\n",
    "with tqdm(total=total, desc=\"Generating Queries\") as pbar:\n",
    "    for speech, chunks in speech_chunks.items():\n",
    "        speech_id = str(uuid.uuid4())\n",
    "        dataset[\"speech\"][speech_id] = speech\n",
    "        for chunk in chunks:\n",
    "            content_id = str(uuid.uuid4())\n",
    "            dataset[\"corpus\"][content_id] = chunk\n",
    "            dataset[\"related_speech\"][content_id] = speech_id\n",
    "            prompt = PROMPT_TEMPLATE.format(context_str=chunk)\n",
    "            res = ollama.chat(\"phi4\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "            reply = res.message.content\n",
    "            if \"<|NAN|>\" in reply:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            for query in reply.split(\"|\"):\n",
    "                query_id = str(uuid.uuid4())\n",
    "                dataset[\"queries\"][query_id] = query\n",
    "                dataset[\"relevant_docs\"][query_id] = [content_id]\n",
    "            pbar.update(1)\n",
    "\n",
    "with open(\"../data/base_data.json\", \"w\") as f:\n",
    "    f.write(json.dumps(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a while on my macbook pro with and m1 max processer and 32 GB of ram it takes approximately 2.5 hours. However if you can get access to machine with a bigger GPU like the A100 you can increase this speed significantly. You can also use the data I have generated which is in located in data/base_data.json\n",
    "\n",
    "The code we have uses a relevant document setup, where we give each query (subject) and chunk a uuid and then have a seperate structure where each query is related to the relevant subjects. Next we need to do two things. \n",
    "\n",
    "We might have the same subject multiple times, we need to clean this up. We also need to think of how to organize the data for training our model. Additionally I've added a related speech object to lookup which speech a given chunk is taken from. This will come in use later. However the data is currently not in the best shape. Let's do some cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = dataset\n",
    "except Exception as e:\n",
    "    with open(\"../data/base_data.json\", \"r\") as f:\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "# Strips away redundant whitespace from the subjects and \n",
    "# removes subjects which are note actual subjects\n",
    "queries = defaultdict(list)\n",
    "for key, value in data[\"queries\"].items():\n",
    "    if value == \"\" or \"--\" in value:\n",
    "        continue\n",
    "    queries[data[\"queries\"][key].strip()].append(key)\n",
    "\n",
    "# We go through the list chunks, and remove parts that does\n",
    "# That does not include any information. This part is very \n",
    "# dependent on your data. So look through and implement the\n",
    "# rules that makes sense for you\n",
    "\n",
    "corpus = defaultdict(list)\n",
    "pop_keys = []\n",
    "for key, value in data[\"corpus\"].items():\n",
    "    value = \" \".join(value.replace(\"Tale\\n \\n \\n \\n \\n \\n \\n \", \"\").split())\n",
    "    if value.isspace() or len(value) <= 60:\n",
    "        pop_keys.append(key)\n",
    "    data[\"corpus\"][key] = value\n",
    "    corpus[value].append(key)\n",
    "\n",
    "for key in pop_keys:\n",
    "    data[\"corpus\"].pop(key)\n",
    "    data[\"related_speech\"].pop(key)\n",
    "\n",
    "\n",
    "# We create a new query structure, which removes duplicate queries\n",
    "# and remap the relevant documents to the new query ids\n",
    "relevant_docs = defaultdict(list)\n",
    "new_queries = {}\n",
    "for query in queries.keys():\n",
    "    id = str(uuid.uuid4())\n",
    "    new_queries[id] = query\n",
    "    query_ids = queries[query]\n",
    "    for query_id in query_ids:\n",
    "        for doc_id in data[\"relevant_docs\"][query_id]:\n",
    "            if doc_id not in pop_keys:\n",
    "                relevant_docs[id].append(doc_id)\n",
    "\n",
    "clean_data = {}\n",
    "clean_data[\"query\"] = new_queries\n",
    "clean_data[\"relevant_docs\"] = relevant_docs\n",
    "clean_data[\"related_speech\"] = data[\"related_speech\"]\n",
    "clean_data[\"corpus\"] = data[\"corpus\"]\n",
    "\n",
    "with open(\"../data/cleaned_base_data.json\", \"w\") as out:\n",
    "    json.dump(clean_data, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the clean data, we need to think about how we want to train our model. When training a model, we need to decide on a loss function. The loss function is what punishes the model when it makes the wrong decision and rewards the model when it makes the right decision. For the type of finetunning we want to do, it is very common to organise your data into triplets. The triplet data format, has an anchor (which in our case is the subject or query), a positive (which is a piece of context which is related to the anchor) and a negative (which is a piece of context which is unrelated). \n",
    "\n",
    "An example of such could be the following\n",
    "\n",
    "**Anchor**: Defence Spending\n",
    "\n",
    "**Positive**: The new air craft carrier is over budget, but will bring much needed capabilities\n",
    "\n",
    "**Negative**: It's cold today, but the sun is out making nice if you wear the right layers.\n",
    "\n",
    "However the astute student will notice that our data only has positives, we therefor needs to add negatives to our dataset. There can be several ways of doing this. One could simply be to do the same as before, and let an LLM generate negatives. Another could be to find a completely unrelated text corpus. Let's say if you are working with clinical records, you could download annual report from Goldman Sachs and use that text as your negative. I chose the second approach and downloaded a file with customer reviews. I then chose random review as my negative. \n",
    "\n",
    "You can see what I've done in the following piece of code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating positive negative pairs: 100%|██████████| 5998/5998 [00:02<00:00, 2086.87it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/cleaned_base_data.json\", \"r\") as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "negatives: pd.DataFrame = pd.read_csv(\"../data/negatives.csv\")[\"review_text\"]\n",
    "\n",
    "total = 0\n",
    "for id, relevant_docs in data[\"relevant_docs\"].items():\n",
    "    total += len(relevant_docs)\n",
    "\n",
    "triplets = []\n",
    "with tqdm(total=total, desc=\"creating positive negative pairs\") as pbar:\n",
    "    for query_id, doc_ids in data[\"relevant_docs\"].items():\n",
    "        anchor = data[\"query\"][query_id]\n",
    "        for id in doc_ids:\n",
    "            triplets.append(\n",
    "                {\n",
    "                    \"anchor\": anchor,\n",
    "                    \"positive\": data[\"corpus\"][id],\n",
    "                    \"negative\": negatives.sample().values[0],\n",
    "                }\n",
    "            )\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our triplet data, the last thing is to split our data into Train, Test and Validation.\n",
    "We use Train data to finetune our model, we use test data to estimate the performance of the model during training, and validition to ensure that we did not just find a model that fit our test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplet, val_triplet = train_test_split(pd.DataFrame(triplets), test_size=0.2)\n",
    "train_triplet, test_triplet = train_test_split(\n",
    "    pd.DataFrame(train_triplet), test_size=0.2\n",
    ")\n",
    "\n",
    "train_triplet.to_json(\"../data/triplet_data_train.json\")\n",
    "test_triplet.to_json(\"../data/triplet_data_test.json\")\n",
    "val_triplet.to_json(\"../data/triplet_data_val.json\")\n",
    "\n",
    "dataset: DatasetDict = {\n",
    "    \"train\": Dataset.from_pandas(train_triplet, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(test_triplet, preserve_index=False),\n",
    "    \"validation\": Dataset.from_pandas(val_triplet, preserve_index=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline and Valuation Strategy\n",
    "\n",
    "Our data is now ready, but before we start finetuning our model, we should consider how we are going to decide whether or not the finetuned model is better than the baseline one. \n",
    "\n",
    "To do this we have to do the following steps:\n",
    "1. Download a Baseline model\n",
    "2. Find a valuation method\n",
    "3. Valuate the Baseline model\n",
    "4. Finetune the model\n",
    "5. Valuate the finetuned model\n",
    "\n",
    "The first thing we will do is to download the baseline model from HuggingFace, using the SentenceTransformer library. I have chosen the [multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) model. I have chosen this model because it has a good baseline performance, and is small enough to train on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Test the model\n",
    "emb = model.encode(\"Hello World\")\n",
    "len(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've downloaded our baseline model, lets spend some time thinking about how to best valuate our model. \n",
    "\n",
    "A good place to start is here: [SentenceTransformer Evaluator Classes](https://sbert.net/docs/sentence_transformer/training_overview.html#evaluator).\n",
    "\n",
    "Going through the list there are several which are interesting, but the one that fits our data best is the the [Triplet Evaluator](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.TripletEvaluator).\n",
    "\n",
    "What the Triplet Evaluator does is that it compares similarity between the anchor and the positive as well as between the anchor and negative. It then returns a percentage which is the percentage of records where the anchor embedding was closer to the positive embedding than to the negative. So a result of 0.8 means that in 80 % of cases the anchor was closer to the positive than to the negative. \n",
    "\n",
    "We can run the model on our data using the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_evaluator = TripletEvaluator(\n",
    "    anchors=dataset[\"test\"][\"anchor\"],\n",
    "    positives=dataset[\"test\"][\"positive\"],\n",
    "    negatives=dataset[\"test\"][\"negative\"],\n",
    "    name=\"dev_evaluator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dev_evaluator_cosine_accuracy': 0.9104166626930237}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_evaluator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our test data we get an result of 0.91 using our baseline model. This means that in 91 % of the cases, the baseline model will place the anchor closer to the positive than the negative. \n",
    "\n",
    "This is great valuation to start with, but in reality this is not what we are interested in. We are interested in wether or not inputing a subject will give us related chunks of text. So lets try to write a custom valuation script. I will base my model on the Recall K metric. The metric simple test whether the anchor returns the positive in its top k results. \n",
    "\n",
    "We will change to test if any of the related documents are in the top k result. And we will have an additional metric to show how large a percentage of the relevant documents are in the top k results. \n",
    "\n",
    "Before we write this test. We have to store the embeddings somewhere so we can query them. We can use many databases for this. But I've chosen to use postgres with teh PG Vector extension. This is how to set it up in your postgres database.\n",
    "\n",
    "First install the extension by running the following command:\n",
    "```sql\n",
    "CREATE EXTENSION vector;\n",
    "```\n",
    "Then we will create to tables to store our embeddings one for our baseline embeddings and one for the finetuned embeddings:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE embeddings_base (id bigserial PRIMARY KEY, speech VARCHAR, context_id VARCHAR, context VARCHAR, embedding vector(384))\n",
    "CREATE TABLE embeddings (id bigserial PRIMARY KEY, speech VARCHAR, context_id VARCHAR, context VARCHAR, embedding vector(384))\n",
    "```\n",
    "\n",
    "You might wonder about the *vector(384)* datatype. This just means we are saving a vector of size 384. If you are wondering what size your embedding is, you can simply run:\n",
    "```python\n",
    "len(model.encode(\"Hello\"))\n",
    "```\n",
    "\n",
    "Now that our table is ready for embeddings, we can load them. To do this I use the following code. Lets do that using the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(speech: str, context_id: str, context: str, embedding: List, write_to_base: bool) -> None:\n",
    "    conn = pg.connect(\"dbname=vector_rag user=postgres password=postgres\")\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    if write_to_base:\n",
    "        cur.execute(\n",
    "            \"INSERT INTO embeddings_base (speech, context_id, context, embedding) VALUES (%s, %s, %s, %s)\",\n",
    "            (speech, context_id, context, str(embedding)),\n",
    "        )\n",
    "    else:\n",
    "        cur.execute(\n",
    "            \"INSERT INTO embeddings (speech, context_id, context, embedding) VALUES (%s, %s, %s, %s)\",\n",
    "            (speech, context_id, context, str(embedding)),\n",
    "        )\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(data[\"corpus\"].keys()), desc=\"Saving embeddings\") as pbar:\n",
    "    for id, context in data[\"corpus\"].items():\n",
    "        speech_name = data[\"related_speech\"][id]\n",
    "        embedding = model.encode(context).tolist()\n",
    "        write_to_db(speech_name, id, context, embedding, True)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can implement our recall k methods and test our baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 Metric:  0.403125\n",
      "Recall@4 Metric:  0.29270833333333335\n",
      "Recall@10 Metric %:  0.3677571097883598\n",
      "Recall@4 Metric %:  0.2636284722222222\n"
     ]
    }
   ],
   "source": [
    "data[\"query_lk\"] = {}\n",
    "for key, value in data[\"query\"].items():\n",
    "    data[\"query_lk\"][value] = key\n",
    "\n",
    "def recall_k(query: str, model: SentenceTransformer, k: int, data: dict, check_base: bool, check_percentage: bool) -> float:\n",
    "    query_id = data[\"query_lk\"][query]\n",
    "    expected_ids = data[\"relevant_docs\"][query_id]\n",
    "    embedded_query = model.encode(query).tolist()\n",
    "\n",
    "    conn = pg.connect(\"dbname=vector_rag user=postgres password=postgres\")\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    if check_base:\n",
    "        cur.execute(\n",
    "            \"SELECT context_id FROM embeddings_base ORDER BY embedding <=> %s::vector LIMIT %s;\",\n",
    "            (str(embedded_query), str(k)),\n",
    "        )\n",
    "    else: \n",
    "        cur.execute(\n",
    "            \"SELECT context_id FROM embeddings ORDER BY embedding <=> %s::vector LIMIT %s;\",\n",
    "            (str(embedded_query), str(k)),\n",
    "        )\n",
    "        \n",
    "    results = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    if check_percentage:\n",
    "        min_res = min(len(expected_ids), k)\n",
    "        result = len(set(results) & set(expected_ids)) / min_res\n",
    "    else:\n",
    "        result = 1.0 if set(results) & set(expected_ids) else 0.0\n",
    "\n",
    "    return result\n",
    "\n",
    "recall_10 = (\n",
    "    test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 10, data, True, False), axis=1\n",
    "    ).sum()\n",
    "    / test_triplet.shape[0]\n",
    ")\n",
    "print(\"Recall@10 Metric: \", recall_10)\n",
    "recall_4 = (\n",
    "    test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 4, data, True, False), axis=1\n",
    "    ).sum()\n",
    "    / test_triplet.shape[0]\n",
    ")\n",
    "print(\"Recall@4 Metric: \", recall_4)\n",
    "\n",
    "\n",
    "recall_10 = (\n",
    "    test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 10, data, True, True), axis=1\n",
    "    ).sum()\n",
    "    / test_triplet.shape[0]\n",
    ")\n",
    "print(\"Recall@10 Metric %: \", recall_10)\n",
    "recall_4 = (\n",
    "    test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 4, data, True, True), axis=1\n",
    "    ).sum()\n",
    "    / test_triplet.shape[0]\n",
    ")\n",
    "print(\"Recall@4 Metric %: \", recall_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our baseline, we can now try an finetune our model to see if we can improve the performance. \n",
    "\n",
    "### Finetunning with limited compute and memory. \n",
    "Although we've chosen a very small model, my laptop will not have enough memory to train it locally. \n",
    "This gives me two options, the first is to just rent a bigger machine, but since I always found it more interesting to work within limitations. \n",
    "I've decided not to do that. Instead I've been researching different ways to do this. \n",
    "\n",
    "One of the solutions to solve this issue, is to do fine tunning using [Low Rank Adaptation (LoRA)](https://arxiv.org/pdf/2106.09685). \n",
    "When using Lora, we freeze all the parameters in the baseline model, and train a smaller adaption layer, which is then multiplied on\n",
    "to the original weights. This greatly reduces the amount of parameters we train, and according to the original paper, can reduce the \n",
    "memory requirements threefold. \n",
    "\n",
    "Lets see the difference in trainable parameters before and after applying the lora adapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Total model parameters:    117,653,760\n",
      "Total trainable parameters before LoRA: 117,653,760\n",
      "Trainable Percentage trainable before LoRA:     100.00%\n",
      "Total trainable parameters after LoRA: 669,696\n",
      "Trainable Percentage trainable after LoRA:     0.28%\n"
     ]
    }
   ],
   "source": [
    "trainable_params = 0\n",
    "all_params = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    all_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "        # print(f\"{name}: shape={param.shape}, params={param.numel()}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(f\"Total model parameters:    {all_params:,}\")\n",
    "print(f\"Total trainable parameters before LoRA: {trainable_params:,}\")\n",
    "print(f\"Trainable Percentage trainable before LoRA:     {100 * trainable_params / all_params:.2f}%\")\n",
    "\n",
    "## Adding LoRA Adapter\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model.add_adapter(peft_config)\n",
    "\n",
    "trainable_params_lora = 0\n",
    "for name, param in model.named_parameters():\n",
    "    all_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params_lora += param.numel()\n",
    "print(f\"Total trainable parameters after LoRA: {trainable_params_lora:,}\")\n",
    "print(f\"Trainable Percentage trainable after LoRA:     {100 * trainable_params_lora / all_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we reduce the trainable parameters by 99.72% which is quite significant, and ensuring that this can run on my mac. \n",
    "Now lets get to the training. \n",
    "\n",
    "First we will need to decide on a loss function, to do that I decide to look at the [Sentence Transformer Loss Overview](https://sbert.net/docs/sentence_transformer/loss_overview.html), \n",
    "and found that multiple MultipleNegativesRankingLoss fits my data well.\n",
    "\n",
    "Second we need to set some hyper parameters, in our training arguments. There is no one solution when chosing hyper parameters. \n",
    "What I've read is that we should just expirement. Below is what I've chosen. I would like to say that the low batch size of 8\n",
    "is mostly to do with my memory limitations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61af61c3ffcc4d47a734742f4b91f01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1440' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1440/1440 43:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.080900</td>\n",
       "      <td>1.892573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.095000</td>\n",
       "      <td>0.913434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.863300</td>\n",
       "      <td>0.822999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.821200</td>\n",
       "      <td>0.773398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.698800</td>\n",
       "      <td>0.733710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.718600</td>\n",
       "      <td>0.717924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.824400</td>\n",
       "      <td>0.701685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.691880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.633800</td>\n",
       "      <td>0.687126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.731900</td>\n",
       "      <td>0.685912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.677925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.616400</td>\n",
       "      <td>0.675557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.775900</td>\n",
       "      <td>0.672484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.599900</td>\n",
       "      <td>0.672817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1440, training_loss=0.8778682687216335, metrics={'train_runtime': 2595.8262, 'train_samples_per_second': 4.436, 'train_steps_per_second': 0.555, 'total_flos': 0.0, 'train_loss': 0.8778682687216335, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"models/multilingual-e5-small-finetune-danish-subject\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    bf16=True,  \n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch according to the documentation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=25,\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model and before we test if we I would like to say a bit about what I look for in the logging. First I look to see if I have a steadily falling validation loss over the training.\n",
    "What can some times happen is that you have falling training loss, but that validation loss stops decreasing. This is usually due to overfitting the model. \n",
    "Second I see that even though my training loss is generally decreasing, it has some jumps. This is probably due too the small batch size. I cannot change this however,\n",
    "due to memory limitations. \n",
    "\n",
    "Now we can run our test again, to see if performance have improved. To do this we have to reembed our corpus with the finetuned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving embeddings: 100%|██████████| 1173/1173 [06:09<00:00,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(data[\"corpus\"].keys()), desc=\"Saving embeddings\") as pbar:\n",
    "    for id, context in data[\"corpus\"].items():\n",
    "        speech_name = data[\"related_speech\"][id]\n",
    "        embedding = model.encode(context).tolist()\n",
    "        write_to_db(speech_name, id, context, embedding, False)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Precision for Test Data:  0.9937499761581421\n",
      "Cosine Precision for Validation Data:  0.996666669845581\n",
      "Recall@10 Metric:  0.5052083333333334\n",
      "Recall@4 Metric:  0.378125\n",
      "Recall@10 Metric %:  0.4798400297619047\n",
      "Recall@4 Metric %:  0.3585069444444444\n"
     ]
    }
   ],
   "source": [
    "test_triplet_eval = dev_evaluator(model)\n",
    "print(\"Cosine Precision for Test Data: \", test_triplet_eval[\"dev_evaluator_cosine_accuracy\"])\n",
    "validation_evaluator = TripletEvaluator(\n",
    "    anchors=dataset[\"validation\"][\"anchor\"],\n",
    "    positives=dataset[\"validation\"][\"positive\"],\n",
    "    negatives=dataset[\"validation\"][\"negative\"],\n",
    "    name=\"validation_evaluator\",\n",
    ")\n",
    "val_triplet_eval = validation_evaluator(model)\n",
    "\n",
    "print(\"Cosine Precision for Validation Data: \", val_triplet_eval[\"validation_evaluator_cosine_accuracy\"])\n",
    "\n",
    "recall_10 = (\n",
    "    test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 10, data, False, False), axis=1\n",
    "    ).sum()\n",
    "    / test_triplet.shape[0]\n",
    ")\n",
    "print(\"Recall@10 Metric: \", recall_10)\n",
    "recall_4 = (\n",
    "    test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 4, data, False, False), axis=1\n",
    "    ).sum()\n",
    "    / test_triplet.shape[0]\n",
    ")\n",
    "print(\"Recall@4 Metric: \", recall_4)\n",
    "\n",
    "\n",
    "recall_10 = (\n",
    "    test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 10, data, False, True), axis=1\n",
    "    ).sum()\n",
    "    / test_triplet.shape[0]\n",
    ")\n",
    "print(\"Recall@10 Metric %: \", recall_10)\n",
    "recall_4 = (\n",
    "    test_triplet.apply(\n",
    "        lambda x: recall_k(x[\"anchor\"], model, 4, data, False, True), axis=1\n",
    "    ).sum()\n",
    "    / test_triplet.shape[0]\n",
    ")\n",
    "print(\"Recall@4 Metric %: \", recall_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GREAT SUCCES!!!!! It looks like we've improved our model on all metrics. We did not have to send our data anywhere :D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
